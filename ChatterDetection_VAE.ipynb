{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbiHqKP14UHd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import find_peaks , peak_prominences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hbd_9Ll9W6i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda, Input,BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.losses import mse\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dropout\n",
        "import petname\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flid_wYO-E80",
        "outputId": "2bfff865-d7bc-4c40-f2aa-7b1a49c33d96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting petname\n",
            "  Downloading petname-2.6.tar.gz (8.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: petname\n",
            "  Building wheel for petname (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for petname: filename=petname-2.6-py3-none-any.whl size=8120 sha256=05328359ef95e189bbe4e061ecc6fc563bbe759885735ac4bb2c0398de1d3f3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/ea/67/b3166a5d29acff7207c1a19b4bc315b8bc4d58b46a6540ac7c\n",
            "Successfully built petname\n",
            "Installing collected packages: petname\n",
            "Successfully installed petname-2.6\n"
          ]
        }
      ],
      "source": [
        "pip install petname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHh58xCl9Eu-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats\n",
        "\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, Lambda\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.losses import mse\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import petname\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgkPR3054Vgs"
      },
      "outputs": [],
      "source": [
        "import scipy.io\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eZiCptf4XJQ",
        "outputId": "df6b9fed-d36f-4884-a303-4204d5c10c7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r61BPVhv4aFm",
        "outputId": "22471100-17be-4b57-f327-fb038fd42913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ChatterDetection\n"
          ]
        }
      ],
      "source": [
        "cd drive/MyDrive/ChatterDetection/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = scipy.io.loadmat('1201-130-non_norm-shuffled.mat')\n",
        "#TrainingDataandLabels.mat\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFGnax30sWsH",
        "outputId": "e50a9452-e832-4152-9217-5c1596b7647f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'__header__': b'MATLAB 5.0 MAT-file, Platform: PCWIN64, Created on: Thu Apr 21 14:43:43 2022',\n",
              " '__version__': '1.0',\n",
              " '__globals__': [],\n",
              " 'shuffled': array([[-0.69854227,  1.96451214, -0.44032254, ...,  0.30546336,\n",
              "         -0.83412373,  0.35606482],\n",
              "        [-1.19331212,  1.96451214, -0.87998131, ..., -0.09144186,\n",
              "         -0.95143245, -0.09606861],\n",
              "        [-0.86750664,  0.54699163, -1.39032985, ...,  0.39454301,\n",
              "         -0.11457563,  0.15705583],\n",
              "        ...,\n",
              "        [-1.10604802, -0.55083754, -0.79511844, ..., -1.28075191,\n",
              "         -0.93743505, -0.88449093],\n",
              "        [-0.94387737, -1.36221792, -1.12765906, ...,  0.05626287,\n",
              "         -0.21396299, -0.88454949],\n",
              "        [-0.74041732, -1.90524657, -1.02698324, ..., -1.22417667,\n",
              "         -0.91541404, -0.37256109]]),\n",
              " 'shuffled_labels': array([[1, 2, 1, ..., 2, 1, 2]], dtype=uint8)}"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = data['shuffled']\n",
        "print(x.shape)\n",
        "y = data [\"shuffled_labels\"]\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESdZJRnBvrjQ",
        "outputId": "ad0e3183-f987-4d36-d325-e5978776c0d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1201, 1906)\n",
            "(1, 1906)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UE5-NYDj4auX"
      },
      "outputs": [],
      "source": [
        "#data = scipy.io.loadmat('1201-130-non_norm-shuffled.mat')\n",
        "#TrainingDataandLabels.mat\n",
        "x = data['flow']\n",
        "print(x.shape)\n",
        "y = data [\"labels\"]\n",
        "print(y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxVg7hcy4cjX"
      },
      "outputs": [],
      "source": [
        "x_transposed = np.transpose(x)\n",
        "y_transposed = np.transpose(y)\n",
        "df1 = pd.DataFrame(x_transposed)\n",
        "df2 = pd.DataFrame(y_transposed, columns = [\"Labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "QecsrgQz4ejB",
        "outputId": "e63522b6-c20d-4a99-acec-77c03be36bcb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "0    -0.698542 -1.193312 -0.867507 -0.886248 -0.955708 -0.759861 -0.944170   \n",
              "1     1.964512  1.964512  0.546992  1.700166  0.770071  0.189443  1.074559   \n",
              "2    -0.440323 -0.879981 -1.390330 -0.404773 -1.141422 -1.184586 -0.885721   \n",
              "3    -0.866218  1.066887  1.964512 -0.540120  1.284461  1.076257 -0.136011   \n",
              "4    -1.068038 -0.647355 -0.524248 -1.091406 -0.529109 -0.892456 -0.905633   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1901  0.117641  0.469391  0.342770 -0.341521  0.926854 -0.920919  0.050406   \n",
              "1902 -0.847535 -0.517982 -0.913306 -0.793420 -0.659947 -0.906863 -0.686126   \n",
              "1903  0.305463 -0.091442  0.394543 -0.283364 -0.233876 -0.323072 -0.199614   \n",
              "1904 -0.834124 -0.951432 -0.114576 -1.186108 -0.798164 -0.098001 -1.452235   \n",
              "1905  0.356065 -0.096069  0.157056 -0.613328 -1.411765 -0.252851 -0.967655   \n",
              "\n",
              "             7         8         9  ...      1192      1193      1194  \\\n",
              "0    -1.113720 -0.802498 -0.932223  ... -0.831020 -1.367196 -0.834241   \n",
              "1     0.872094  1.964512  1.964512  ...  0.449947  1.832175  0.281920   \n",
              "2    -0.714472 -1.309450 -0.559037  ... -0.869966 -1.000277 -1.215919   \n",
              "3     1.964512  1.964512  1.964512  ... -0.670020  0.624944 -0.382986   \n",
              "4    -0.737430 -1.092636 -0.799394  ...  0.390595 -0.453734 -0.828970   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "1901  0.326079 -0.284477  0.203558  ... -0.565713  0.475716 -0.250391   \n",
              "1902 -0.706273 -0.862938 -0.530222  ... -0.470836 -0.282134 -1.854352   \n",
              "1903  0.156763  0.165079  0.314600  ... -0.834007  0.245257  0.117582   \n",
              "1904 -0.555230 -0.615963 -1.042562  ... -0.927479 -0.737020 -0.558627   \n",
              "1905 -0.412562 -0.059933  0.129471  ...  0.364616 -0.641147 -0.008336   \n",
              "\n",
              "          1195      1196      1197      1198      1199      1200  Labels  \n",
              "0    -0.679332 -1.297560 -0.769876 -1.106048 -0.943877 -0.740417       1  \n",
              "1     0.485907  0.951745 -0.864637 -0.550838 -1.362218 -1.905247       2  \n",
              "2    -0.793244 -1.129650 -1.073602 -0.795118 -1.127659 -1.026983       1  \n",
              "3    -0.043652  0.696336  0.500548  0.325493 -0.118792 -0.206291       2  \n",
              "4    -1.509044 -0.539886 -1.258321 -1.009238 -0.808764 -1.051113       1  \n",
              "...        ...       ...       ...       ...       ...       ...     ...  \n",
              "1901 -0.066551 -0.122131 -0.881504 -0.463573 -0.793479 -1.038169       2  \n",
              "1902 -0.014954 -0.390248 -1.576981 -0.256424 -0.542697 -1.189798       1  \n",
              "1903 -0.620883  0.603391 -0.494262 -1.280752  0.056263 -1.224177       2  \n",
              "1904 -0.869908 -0.521437 -0.856320 -0.937435 -0.213963 -0.915414       1  \n",
              "1905 -0.523311 -0.879981 -0.664164 -0.884491 -0.884549 -0.372561       2  \n",
              "\n",
              "[1906 rows x 1202 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-33777d49-b0df-4609-96be-e829f40856bb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>1192</th>\n",
              "      <th>1193</th>\n",
              "      <th>1194</th>\n",
              "      <th>1195</th>\n",
              "      <th>1196</th>\n",
              "      <th>1197</th>\n",
              "      <th>1198</th>\n",
              "      <th>1199</th>\n",
              "      <th>1200</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.698542</td>\n",
              "      <td>-1.193312</td>\n",
              "      <td>-0.867507</td>\n",
              "      <td>-0.886248</td>\n",
              "      <td>-0.955708</td>\n",
              "      <td>-0.759861</td>\n",
              "      <td>-0.944170</td>\n",
              "      <td>-1.113720</td>\n",
              "      <td>-0.802498</td>\n",
              "      <td>-0.932223</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.831020</td>\n",
              "      <td>-1.367196</td>\n",
              "      <td>-0.834241</td>\n",
              "      <td>-0.679332</td>\n",
              "      <td>-1.297560</td>\n",
              "      <td>-0.769876</td>\n",
              "      <td>-1.106048</td>\n",
              "      <td>-0.943877</td>\n",
              "      <td>-0.740417</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.964512</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>0.546992</td>\n",
              "      <td>1.700166</td>\n",
              "      <td>0.770071</td>\n",
              "      <td>0.189443</td>\n",
              "      <td>1.074559</td>\n",
              "      <td>0.872094</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>...</td>\n",
              "      <td>0.449947</td>\n",
              "      <td>1.832175</td>\n",
              "      <td>0.281920</td>\n",
              "      <td>0.485907</td>\n",
              "      <td>0.951745</td>\n",
              "      <td>-0.864637</td>\n",
              "      <td>-0.550838</td>\n",
              "      <td>-1.362218</td>\n",
              "      <td>-1.905247</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.440323</td>\n",
              "      <td>-0.879981</td>\n",
              "      <td>-1.390330</td>\n",
              "      <td>-0.404773</td>\n",
              "      <td>-1.141422</td>\n",
              "      <td>-1.184586</td>\n",
              "      <td>-0.885721</td>\n",
              "      <td>-0.714472</td>\n",
              "      <td>-1.309450</td>\n",
              "      <td>-0.559037</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.869966</td>\n",
              "      <td>-1.000277</td>\n",
              "      <td>-1.215919</td>\n",
              "      <td>-0.793244</td>\n",
              "      <td>-1.129650</td>\n",
              "      <td>-1.073602</td>\n",
              "      <td>-0.795118</td>\n",
              "      <td>-1.127659</td>\n",
              "      <td>-1.026983</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.866218</td>\n",
              "      <td>1.066887</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>-0.540120</td>\n",
              "      <td>1.284461</td>\n",
              "      <td>1.076257</td>\n",
              "      <td>-0.136011</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.670020</td>\n",
              "      <td>0.624944</td>\n",
              "      <td>-0.382986</td>\n",
              "      <td>-0.043652</td>\n",
              "      <td>0.696336</td>\n",
              "      <td>0.500548</td>\n",
              "      <td>0.325493</td>\n",
              "      <td>-0.118792</td>\n",
              "      <td>-0.206291</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.068038</td>\n",
              "      <td>-0.647355</td>\n",
              "      <td>-0.524248</td>\n",
              "      <td>-1.091406</td>\n",
              "      <td>-0.529109</td>\n",
              "      <td>-0.892456</td>\n",
              "      <td>-0.905633</td>\n",
              "      <td>-0.737430</td>\n",
              "      <td>-1.092636</td>\n",
              "      <td>-0.799394</td>\n",
              "      <td>...</td>\n",
              "      <td>0.390595</td>\n",
              "      <td>-0.453734</td>\n",
              "      <td>-0.828970</td>\n",
              "      <td>-1.509044</td>\n",
              "      <td>-0.539886</td>\n",
              "      <td>-1.258321</td>\n",
              "      <td>-1.009238</td>\n",
              "      <td>-0.808764</td>\n",
              "      <td>-1.051113</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1901</th>\n",
              "      <td>0.117641</td>\n",
              "      <td>0.469391</td>\n",
              "      <td>0.342770</td>\n",
              "      <td>-0.341521</td>\n",
              "      <td>0.926854</td>\n",
              "      <td>-0.920919</td>\n",
              "      <td>0.050406</td>\n",
              "      <td>0.326079</td>\n",
              "      <td>-0.284477</td>\n",
              "      <td>0.203558</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.565713</td>\n",
              "      <td>0.475716</td>\n",
              "      <td>-0.250391</td>\n",
              "      <td>-0.066551</td>\n",
              "      <td>-0.122131</td>\n",
              "      <td>-0.881504</td>\n",
              "      <td>-0.463573</td>\n",
              "      <td>-0.793479</td>\n",
              "      <td>-1.038169</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1902</th>\n",
              "      <td>-0.847535</td>\n",
              "      <td>-0.517982</td>\n",
              "      <td>-0.913306</td>\n",
              "      <td>-0.793420</td>\n",
              "      <td>-0.659947</td>\n",
              "      <td>-0.906863</td>\n",
              "      <td>-0.686126</td>\n",
              "      <td>-0.706273</td>\n",
              "      <td>-0.862938</td>\n",
              "      <td>-0.530222</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.470836</td>\n",
              "      <td>-0.282134</td>\n",
              "      <td>-1.854352</td>\n",
              "      <td>-0.014954</td>\n",
              "      <td>-0.390248</td>\n",
              "      <td>-1.576981</td>\n",
              "      <td>-0.256424</td>\n",
              "      <td>-0.542697</td>\n",
              "      <td>-1.189798</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1903</th>\n",
              "      <td>0.305463</td>\n",
              "      <td>-0.091442</td>\n",
              "      <td>0.394543</td>\n",
              "      <td>-0.283364</td>\n",
              "      <td>-0.233876</td>\n",
              "      <td>-0.323072</td>\n",
              "      <td>-0.199614</td>\n",
              "      <td>0.156763</td>\n",
              "      <td>0.165079</td>\n",
              "      <td>0.314600</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.834007</td>\n",
              "      <td>0.245257</td>\n",
              "      <td>0.117582</td>\n",
              "      <td>-0.620883</td>\n",
              "      <td>0.603391</td>\n",
              "      <td>-0.494262</td>\n",
              "      <td>-1.280752</td>\n",
              "      <td>0.056263</td>\n",
              "      <td>-1.224177</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1904</th>\n",
              "      <td>-0.834124</td>\n",
              "      <td>-0.951432</td>\n",
              "      <td>-0.114576</td>\n",
              "      <td>-1.186108</td>\n",
              "      <td>-0.798164</td>\n",
              "      <td>-0.098001</td>\n",
              "      <td>-1.452235</td>\n",
              "      <td>-0.555230</td>\n",
              "      <td>-0.615963</td>\n",
              "      <td>-1.042562</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.927479</td>\n",
              "      <td>-0.737020</td>\n",
              "      <td>-0.558627</td>\n",
              "      <td>-0.869908</td>\n",
              "      <td>-0.521437</td>\n",
              "      <td>-0.856320</td>\n",
              "      <td>-0.937435</td>\n",
              "      <td>-0.213963</td>\n",
              "      <td>-0.915414</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1905</th>\n",
              "      <td>0.356065</td>\n",
              "      <td>-0.096069</td>\n",
              "      <td>0.157056</td>\n",
              "      <td>-0.613328</td>\n",
              "      <td>-1.411765</td>\n",
              "      <td>-0.252851</td>\n",
              "      <td>-0.967655</td>\n",
              "      <td>-0.412562</td>\n",
              "      <td>-0.059933</td>\n",
              "      <td>0.129471</td>\n",
              "      <td>...</td>\n",
              "      <td>0.364616</td>\n",
              "      <td>-0.641147</td>\n",
              "      <td>-0.008336</td>\n",
              "      <td>-0.523311</td>\n",
              "      <td>-0.879981</td>\n",
              "      <td>-0.664164</td>\n",
              "      <td>-0.884491</td>\n",
              "      <td>-0.884549</td>\n",
              "      <td>-0.372561</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1906 rows × 1202 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33777d49-b0df-4609-96be-e829f40856bb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-33777d49-b0df-4609-96be-e829f40856bb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-33777d49-b0df-4609-96be-e829f40856bb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-10efc589-e264-4817-827f-e008e683ec32\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-10efc589-e264-4817-827f-e008e683ec32')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-10efc589-e264-4817-827f-e008e683ec32 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ],
      "source": [
        "dff = pd.concat([df1, df2], axis=1)\n",
        "dff"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test = train_test_split(dff,  test_size=0.2)"
      ],
      "metadata": {
        "id": "dw5J5N781Kf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKFisDFu4zuV"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_stable = StandardScaler()\n",
        "scaler_chatter = StandardScaler()\n",
        "\n",
        "X_train_stable_sc = scaler_stable.fit_transform(X_train[X_train[\"Labels\"]==1].iloc[:,0:1201])\n",
        "X_train_chatter_sc = scaler_chatter.fit_transform(X_train[X_train[\"Labels\"]==2].iloc[:,0:1201])\n",
        "\n",
        "X_test_stable_sc = scaler_stable.transform(X_test[X_test[\"Labels\"]==1].iloc[:,0:1201])\n",
        "X_test_chatter_sc = scaler_chatter.transform(X_test[X_test[\"Labels\"]==2].iloc[:,0:1201])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_s = scaler_stable.fit_transform(X_train.iloc[:,0:1201])\n",
        "X_test_s = scaler_stable.transform(X_test.iloc[:,0:1201])"
      ],
      "metadata": {
        "id": "YNnqBHGkytii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XtrainS = pd.DataFrame(X_train_s)\n",
        "XtrainS[\"Labels\"] = X_train[\"Labels\"].to_list()\n",
        "\n",
        "XtestS = pd.DataFrame(X_test_s)\n",
        "XtestS[\"Labels\"] = X_test[\"Labels\"].to_list()"
      ],
      "metadata": {
        "id": "RJSLSI0t0LL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_chatter_o = XtestS[XtestS[\"Labels\"]==2].iloc[:,0:1201]\n",
        "X_test_stable_o = XtestS[XtestS[\"Labels\"]==1].iloc[:,0:1201]\n",
        "\n",
        "X_train_chatter_o = XtrainS[XtrainS[\"Labels\"]==2].iloc[:,0:1201]\n",
        "X_train_stable_o = XtrainS[XtrainS[\"Labels\"]==1].iloc[:,0:1201]"
      ],
      "metadata": {
        "id": "FgyPo_Z8mf0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_stable = X_train[X_train[\"Labels\"]==1].iloc[:,0:1201]\n",
        "X_train_chatter = X_train[X_train[\"Labels\"]==2].iloc[:,0:1201]\n",
        "\n",
        "X_test_stable = X_test[X_test[\"Labels\"]==1].iloc[:,0:1201]\n",
        "X_test_chatter = X_test[X_test[\"Labels\"]==2].iloc[:,0:1201]"
      ],
      "metadata": {
        "id": "SQw8wlGQ78Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_m = np.concatenate((X_train_stable_sc,X_train_chatter_sc),axis=0)\n",
        "test_m = np.concatenate((X_test_stable_sc,X_test_chatter_sc),axis=0)"
      ],
      "metadata": {
        "id": "Kctx55LzjJkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxMaQUQ48a-8"
      },
      "outputs": [],
      "source": [
        "# specify training parameters and callback functions\n",
        "\n",
        "# batch size for stochastic solver\n",
        "batch_size = 256\n",
        "\n",
        "# number of times entire dataset is considered in stochastic solver\n",
        "epochs = 100\n",
        "\n",
        "# unique name for the network for saving\n",
        "unique_name = petname.name()\n",
        "model_filename = 'a'+'.h5'\n",
        "\n",
        "# training history file name\n",
        "history_filename = 'results_'+unique_name+'.npz'\n",
        "\n",
        "# stop early after no improvement past epochs=patience and be verbose\n",
        "earlystopper = EarlyStopping(patience=100, verbose=1)\n",
        "\n",
        "# checkpoint and save model when improvement occurs\n",
        "checkpointer = ModelCheckpoint(model_filename, verbose=1, save_best_only=True)\n",
        "\n",
        "# consolidate callback functions for convenience\n",
        "callbacks = [earlystopper, checkpointer]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cikyt1Op8fsU"
      },
      "outputs": [],
      "source": [
        "# encoding dimension; i.e. dimensionality of the latent space\n",
        "encoding_dim=64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt-s2Iww8vuq"
      },
      "outputs": [],
      "source": [
        "def sampling(args):\n",
        "    z_mean, z_log_sigma = args\n",
        "    epsilon = K.random_normal(shape=(encoding_dim,))\n",
        "    return z_mean + K.exp(z_log_sigma) * epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3cNVQkWB2Aa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "\n",
        "disable_eager_execution()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzRqWcB08wVH"
      },
      "outputs": [],
      "source": [
        "# construct variational autoencoder network structure\n",
        "\n",
        "# input layer is full time series of length nt\n",
        "inputs = Input(1201,)\n",
        "\n",
        "# encoder hidden layers\n",
        "encoded = Dense(512)(inputs)\n",
        "encoded = LeakyReLU(alpha=0.2)(encoded)\n",
        "encoded = Dense(256)(encoded)\n",
        "encoded = LeakyReLU(alpha=0.2)(encoded)\n",
        "encoded = Dense(128)(encoded)\n",
        "encoded = LeakyReLU(alpha=0.2)(encoded)\n",
        "encoded = Dense(64)(encoded)\n",
        "encoded = LeakyReLU(alpha=0.2)(encoded)\n",
        "\n",
        "z_mean = Dense(encoding_dim)(encoded)\n",
        "z_log_sigma = Dense(encoding_dim)(encoded)\n",
        "z = Lambda(sampling,output_shape=(encoding_dim,))([z_mean,z_log_sigma])\n",
        "\n",
        "# decoder hidden layers\n",
        "# explicitly named so we can define the decoder model\n",
        "#decoder_a = Dense(32)\n",
        "#decoder_b = Dense(64)\n",
        "#outputter = Dense(nt,activation='tanh')\n",
        "\n",
        "\n",
        "decoded = Dense(64)(z)\n",
        "decoded = LeakyReLU(alpha=0.2)(decoded)\n",
        "decoded = Dense(128)(decoded)\n",
        "decoded = LeakyReLU(alpha=0.2)(decoded)\n",
        "decoded = Dense(256)(decoded)\n",
        "decoded = LeakyReLU(alpha=0.2)(decoded)\n",
        "decoded = Dense(512)(decoded)\n",
        "decoded = LeakyReLU(alpha=0.2)(decoded)\n",
        "# output layer is same length as input\n",
        "outputs = Dense(1201)(decoded)\n",
        "\n",
        "# consolidate to define autoencoder model inputs and outputs\n",
        "vae = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# specify encoder and decoder model for easy encoding and decoding later\n",
        "encoder = Model(inputs=inputs, outputs=[z_mean,z_log_sigma,z],\n",
        "                    name='encoder')\n",
        "# create a placeholder for an encoded input\n",
        "encoded_input = Input(shape=(encoding_dim,))\n",
        "# retrieve the last layers of the autoencoder model\n",
        "\n",
        "decoded_output = vae.layers[-9](encoded_input)\n",
        "decoded_output = vae.layers[-8](decoded_output)\n",
        "decoded_output = vae.layers[-7](decoded_output)\n",
        "decoded_output = vae.layers[-6](decoded_output)\n",
        "decoded_output = vae.layers[-5](decoded_output)\n",
        "decoded_output = vae.layers[-4](decoded_output)\n",
        "decoded_output = vae.layers[-3](decoded_output)\n",
        "decoded_output = vae.layers[-2](decoded_output)\n",
        "decoded_output = vae.layers[-1](decoded_output)\n",
        "# create the decoder model\n",
        "decoder = Model(inputs=encoded_input, outputs=decoded_output, name='decoder')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1hs5c_e80ZA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92e9af1f-bf85-479c-d907-8b92e487d373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full autoencoder\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_8 (InputLayer)        [(None, 1201)]               0         []                            \n",
            "                                                                                                  \n",
            " dense_34 (Dense)            (None, 512)                  615424    ['input_8[0][0]']             \n",
            "                                                                                                  \n",
            " leaky_re_lu_24 (LeakyReLU)  (None, 512)                  0         ['dense_34[0][0]']            \n",
            "                                                                                                  \n",
            " dense_35 (Dense)            (None, 256)                  131328    ['leaky_re_lu_24[0][0]']      \n",
            "                                                                                                  \n",
            " leaky_re_lu_25 (LeakyReLU)  (None, 256)                  0         ['dense_35[0][0]']            \n",
            "                                                                                                  \n",
            " dense_36 (Dense)            (None, 128)                  32896     ['leaky_re_lu_25[0][0]']      \n",
            "                                                                                                  \n",
            " leaky_re_lu_26 (LeakyReLU)  (None, 128)                  0         ['dense_36[0][0]']            \n",
            "                                                                                                  \n",
            " dense_37 (Dense)            (None, 64)                   8256      ['leaky_re_lu_26[0][0]']      \n",
            "                                                                                                  \n",
            " leaky_re_lu_27 (LeakyReLU)  (None, 64)                   0         ['dense_37[0][0]']            \n",
            "                                                                                                  \n",
            " dense_38 (Dense)            (None, 64)                   4160      ['leaky_re_lu_27[0][0]']      \n",
            "                                                                                                  \n",
            " dense_39 (Dense)            (None, 64)                   4160      ['leaky_re_lu_27[0][0]']      \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)           (None, 64)                   0         ['dense_38[0][0]',            \n",
            "                                                                     'dense_39[0][0]']            \n",
            "                                                                                                  \n",
            " dense_40 (Dense)            (None, 64)                   4160      ['lambda_3[0][0]']            \n",
            "                                                                                                  \n",
            " leaky_re_lu_28 (LeakyReLU)  (None, 64)                   0         ['dense_40[0][0]']            \n",
            "                                                                                                  \n",
            " dense_41 (Dense)            (None, 128)                  8320      ['leaky_re_lu_28[0][0]']      \n",
            "                                                                                                  \n",
            " leaky_re_lu_29 (LeakyReLU)  (None, 128)                  0         ['dense_41[0][0]']            \n",
            "                                                                                                  \n",
            " dense_42 (Dense)            (None, 256)                  33024     ['leaky_re_lu_29[0][0]']      \n",
            "                                                                                                  \n",
            " leaky_re_lu_30 (LeakyReLU)  (None, 256)                  0         ['dense_42[0][0]']            \n",
            "                                                                                                  \n",
            " dense_43 (Dense)            (None, 512)                  131584    ['leaky_re_lu_30[0][0]']      \n",
            "                                                                                                  \n",
            " leaky_re_lu_31 (LeakyReLU)  (None, 512)                  0         ['dense_43[0][0]']            \n",
            "                                                                                                  \n",
            " dense_44 (Dense)            (None, 1201)                 616113    ['leaky_re_lu_31[0][0]']      \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1589425 (6.06 MB)\n",
            "Trainable params: 1589425 (6.06 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print('Full autoencoder')\n",
        "print(vae.summary())\n",
        "#print('\\n Encoder portion of autoencoder')\n",
        "#print(vae_encoder.summary())\n",
        "#print('\\n Decoder portion of autoencoder')\n",
        "#print(vae_decoder.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcH1PnZ783Ib"
      },
      "outputs": [],
      "source": [
        "# specify loss\n",
        "# regularization balances signal reconstruction with\n",
        "# a Gaussian distribution in the latent space\n",
        "regularization = 2\n",
        "def vae_loss(input_img, output):\n",
        "    # compute the average MSE error, then scale it up, ie. simply sum on all axes\n",
        "    reconstruction_loss = K.sum(K.square(output-input_img))\n",
        "    kl_loss = - 0.5 * K.sum(1 + z_log_sigma - K.square(z_mean) - K.square(K.exp(z_log_sigma)), axis=-1)\n",
        "    # return the average loss over all images in batch\n",
        "    total_loss = K.mean(reconstruction_loss + regularization*kl_loss)\n",
        "    return total_loss\n",
        "\n",
        "vae.compile(optimizer='adam', loss=vae_loss, metrics=['mse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxNq6uul85qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce1b03b2-32dd-47f3-865d-15de71edb4e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 1524 samples, validate on 382 samples\n",
            "Epoch 1/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 55339790.1678 - mse: 179.4175\n",
            "Epoch 1: val_loss improved from inf to 195848.22534, saving model to a.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1524/1524 [==============================] - 2s 1ms/sample - loss: 55339790.1678 - mse: 179.4175 - val_loss: 195848.2253 - val_mse: 0.7484\n",
            "Epoch 2/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 332736.2805 - mse: 1.0890\n",
            "Epoch 2: val_loss did not improve from 195848.22534\n",
            "1524/1524 [==============================] - 0s 66us/sample - loss: 332736.2805 - mse: 1.0890 - val_loss: 218631.7959 - val_mse: 0.8513\n",
            "Epoch 3/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 539158.2531 - mse: 1.5831\n",
            "Epoch 3: val_loss did not improve from 195848.22534\n",
            "1524/1524 [==============================] - 0s 69us/sample - loss: 497783.2315 - mse: 1.4811 - val_loss: 209523.2390 - val_mse: 0.8362\n",
            "Epoch 4/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 261259.5202 - mse: 0.8537\n",
            "Epoch 4: val_loss did not improve from 195848.22534\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 261259.5202 - mse: 0.8537 - val_loss: 237885.0339 - val_mse: 0.9175\n",
            "Epoch 5/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 257039.0155 - mse: 0.8400\n",
            "Epoch 5: val_loss improved from 195848.22534 to 193452.15077, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 174us/sample - loss: 257039.0155 - mse: 0.8400 - val_loss: 193452.1508 - val_mse: 0.7486\n",
            "Epoch 6/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 239362.0132 - mse: 0.7825\n",
            "Epoch 6: val_loss improved from 193452.15077 to 187925.39983, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 157us/sample - loss: 239362.0132 - mse: 0.7825 - val_loss: 187925.3998 - val_mse: 0.7307\n",
            "Epoch 7/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 219661.1564 - mse: 0.7178\n",
            "Epoch 7: val_loss improved from 187925.39983 to 179691.69515, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 160us/sample - loss: 219661.1564 - mse: 0.7178 - val_loss: 179691.6951 - val_mse: 0.7162\n",
            "Epoch 8/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 213015.3904 - mse: 0.6964\n",
            "Epoch 8: val_loss improved from 179691.69515 to 168930.77507, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 156us/sample - loss: 213015.3904 - mse: 0.6964 - val_loss: 168930.7751 - val_mse: 0.6706\n",
            "Epoch 9/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 205944.2094 - mse: 0.6730\n",
            "Epoch 9: val_loss improved from 168930.77507 to 152172.36858, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 157us/sample - loss: 205944.2094 - mse: 0.6730 - val_loss: 152172.3686 - val_mse: 0.6021\n",
            "Epoch 10/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 202259.9219 - mse: 0.6565\n",
            "Epoch 10: val_loss did not improve from 152172.36858\n",
            "1524/1524 [==============================] - 0s 73us/sample - loss: 199789.2267 - mse: 0.6533 - val_loss: 159287.6747 - val_mse: 0.6266\n",
            "Epoch 11/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 190142.4813 - mse: 0.6172\n",
            "Epoch 11: val_loss did not improve from 152172.36858\n",
            "1524/1524 [==============================] - 0s 72us/sample - loss: 188444.1306 - mse: 0.6163 - val_loss: 171564.6332 - val_mse: 0.6524\n",
            "Epoch 12/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 177970.4875 - mse: 0.5819\n",
            "Epoch 12: val_loss improved from 152172.36858 to 143042.53039, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 165us/sample - loss: 177970.4875 - mse: 0.5819 - val_loss: 143042.5304 - val_mse: 0.5567\n",
            "Epoch 13/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 173533.9180 - mse: 0.5680\n",
            "Epoch 13: val_loss did not improve from 143042.53039\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 173533.9180 - mse: 0.5680 - val_loss: 143801.4321 - val_mse: 0.5574\n",
            "Epoch 14/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 165902.8969 - mse: 0.5434\n",
            "Epoch 14: val_loss improved from 143042.53039 to 137160.73200, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 149us/sample - loss: 165902.8969 - mse: 0.5434 - val_loss: 137160.7320 - val_mse: 0.5322\n",
            "Epoch 15/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 164399.5220 - mse: 0.5379\n",
            "Epoch 15: val_loss did not improve from 137160.73200\n",
            "1524/1524 [==============================] - 0s 67us/sample - loss: 164399.5220 - mse: 0.5379 - val_loss: 138033.0719 - val_mse: 0.5340\n",
            "Epoch 16/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 162539.4366 - mse: 0.5321\n",
            "Epoch 16: val_loss improved from 137160.73200 to 128568.01947, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 156us/sample - loss: 162539.4366 - mse: 0.5321 - val_loss: 128568.0195 - val_mse: 0.5142\n",
            "Epoch 17/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 161990.2000 - mse: 0.5264\n",
            "Epoch 17: val_loss improved from 128568.01947 to 126376.17445, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 185us/sample - loss: 160353.8750 - mse: 0.5249 - val_loss: 126376.1745 - val_mse: 0.5084\n",
            "Epoch 18/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 156610.9719 - mse: 0.5088\n",
            "Epoch 18: val_loss did not improve from 126376.17445\n",
            "1524/1524 [==============================] - 0s 71us/sample - loss: 156652.1659 - mse: 0.5130 - val_loss: 134946.8649 - val_mse: 0.5182\n",
            "Epoch 19/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 159982.0812 - mse: 0.5198\n",
            "Epoch 19: val_loss did not improve from 126376.17445\n",
            "1524/1524 [==============================] - 0s 69us/sample - loss: 156362.8325 - mse: 0.5116 - val_loss: 132983.6282 - val_mse: 0.5176\n",
            "Epoch 20/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 154393.0668 - mse: 0.5056\n",
            "Epoch 20: val_loss did not improve from 126376.17445\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 154393.0668 - mse: 0.5056 - val_loss: 129082.1893 - val_mse: 0.4970\n",
            "Epoch 21/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 154917.7031 - mse: 0.5034\n",
            "Epoch 21: val_loss improved from 126376.17445 to 124578.41439, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 173us/sample - loss: 154295.8929 - mse: 0.5052 - val_loss: 124578.4144 - val_mse: 0.4946\n",
            "Epoch 22/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 152721.0639 - mse: 0.4998\n",
            "Epoch 22: val_loss did not improve from 124578.41439\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 152721.0639 - mse: 0.4998 - val_loss: 126720.4707 - val_mse: 0.4995\n",
            "Epoch 23/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 153438.3971 - mse: 0.5026\n",
            "Epoch 23: val_loss did not improve from 124578.41439\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 153438.3971 - mse: 0.5026 - val_loss: 125794.6492 - val_mse: 0.4963\n",
            "Epoch 24/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 151801.9175 - mse: 0.4973\n",
            "Epoch 24: val_loss improved from 124578.41439 to 124132.05542, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 181us/sample - loss: 151801.9175 - mse: 0.4973 - val_loss: 124132.0554 - val_mse: 0.4895\n",
            "Epoch 25/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 155021.0562 - mse: 0.5037\n",
            "Epoch 25: val_loss improved from 124132.05542 to 121339.60794, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 171us/sample - loss: 153366.9951 - mse: 0.5020 - val_loss: 121339.6079 - val_mse: 0.4921\n",
            "Epoch 26/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 157493.1375 - mse: 0.5118\n",
            "Epoch 26: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 65us/sample - loss: 153519.9351 - mse: 0.5023 - val_loss: 125732.2207 - val_mse: 0.4914\n",
            "Epoch 27/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 151211.5178 - mse: 0.4950\n",
            "Epoch 27: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 151211.5178 - mse: 0.4950 - val_loss: 129219.6143 - val_mse: 0.4959\n",
            "Epoch 28/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 153770.6625 - mse: 0.4997\n",
            "Epoch 28: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 73us/sample - loss: 150150.8103 - mse: 0.4913 - val_loss: 123847.9411 - val_mse: 0.4855\n",
            "Epoch 29/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 150395.9906 - mse: 0.4887\n",
            "Epoch 29: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 66us/sample - loss: 150109.5543 - mse: 0.4916 - val_loss: 123228.7441 - val_mse: 0.4876\n",
            "Epoch 30/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 150721.4489 - mse: 0.4933\n",
            "Epoch 30: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 150721.4489 - mse: 0.4933 - val_loss: 123565.7646 - val_mse: 0.4862\n",
            "Epoch 31/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 151703.4937 - mse: 0.4930\n",
            "Epoch 31: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 67us/sample - loss: 150371.2386 - mse: 0.4923 - val_loss: 125796.6423 - val_mse: 0.4863\n",
            "Epoch 32/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 149430.1578 - mse: 0.4856\n",
            "Epoch 32: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 65us/sample - loss: 149524.4644 - mse: 0.4897 - val_loss: 123912.7655 - val_mse: 0.4860\n",
            "Epoch 33/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 150787.0261 - mse: 0.4936\n",
            "Epoch 33: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 66us/sample - loss: 150787.0261 - mse: 0.4936 - val_loss: 125128.2618 - val_mse: 0.4895\n",
            "Epoch 34/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 151794.7125 - mse: 0.4933\n",
            "Epoch 34: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 70us/sample - loss: 150430.3764 - mse: 0.4925 - val_loss: 124852.5155 - val_mse: 0.4840\n",
            "Epoch 35/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 152491.5156 - mse: 0.4955\n",
            "Epoch 35: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 49us/sample - loss: 150172.7748 - mse: 0.4915 - val_loss: 126887.5679 - val_mse: 0.4872\n",
            "Epoch 36/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 138943.8906 - mse: 0.4515\n",
            "Epoch 36: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 50us/sample - loss: 149386.4795 - mse: 0.4893 - val_loss: 123754.1612 - val_mse: 0.4862\n",
            "Epoch 37/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 149137.1669 - mse: 0.4883\n",
            "Epoch 37: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 54us/sample - loss: 149137.1669 - mse: 0.4883 - val_loss: 124105.8905 - val_mse: 0.4844\n",
            "Epoch 38/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 149865.3560 - mse: 0.4908\n",
            "Epoch 38: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 149865.3560 - mse: 0.4908 - val_loss: 121888.3116 - val_mse: 0.4840\n",
            "Epoch 39/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 141007.6875 - mse: 0.4582\n",
            "Epoch 39: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 52us/sample - loss: 149528.9110 - mse: 0.4892 - val_loss: 126509.7915 - val_mse: 0.4843\n",
            "Epoch 40/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 137519.3594 - mse: 0.4468\n",
            "Epoch 40: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 49us/sample - loss: 149213.0952 - mse: 0.4884 - val_loss: 122006.9579 - val_mse: 0.4829\n",
            "Epoch 41/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 148916.0481 - mse: 0.4877\n",
            "Epoch 41: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 56us/sample - loss: 148916.0481 - mse: 0.4877 - val_loss: 121788.0627 - val_mse: 0.4862\n",
            "Epoch 42/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 149146.0195 - mse: 0.4881\n",
            "Epoch 42: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 55us/sample - loss: 149146.0195 - mse: 0.4881 - val_loss: 125373.1506 - val_mse: 0.4838\n",
            "Epoch 43/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 149248.6509 - mse: 0.4886\n",
            "Epoch 43: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 54us/sample - loss: 149248.6509 - mse: 0.4886 - val_loss: 125482.0625 - val_mse: 0.4870\n",
            "Epoch 44/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 149253.5020 - mse: 0.4886\n",
            "Epoch 44: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 149253.5020 - mse: 0.4886 - val_loss: 123572.6539 - val_mse: 0.4855\n",
            "Epoch 45/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 148979.3744 - mse: 0.4879\n",
            "Epoch 45: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 148979.3744 - mse: 0.4879 - val_loss: 122791.0774 - val_mse: 0.4835\n",
            "Epoch 46/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 154457.7188 - mse: 0.5019\n",
            "Epoch 46: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 49us/sample - loss: 148706.3850 - mse: 0.4871 - val_loss: 127632.8499 - val_mse: 0.4829\n",
            "Epoch 47/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 141143.9531 - mse: 0.4586\n",
            "Epoch 47: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 49us/sample - loss: 148768.5927 - mse: 0.4869 - val_loss: 123240.4181 - val_mse: 0.4830\n",
            "Epoch 48/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 151147.8438 - mse: 0.4911\n",
            "Epoch 48: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 49us/sample - loss: 148694.4051 - mse: 0.4866 - val_loss: 122524.5298 - val_mse: 0.4841\n",
            "Epoch 49/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 158802.5625 - mse: 0.5161\n",
            "Epoch 49: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 49us/sample - loss: 148648.5836 - mse: 0.4868 - val_loss: 124929.3828 - val_mse: 0.4865\n",
            "Epoch 50/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 149140.2070 - mse: 0.4882\n",
            "Epoch 50: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 149140.2070 - mse: 0.4882 - val_loss: 124526.7907 - val_mse: 0.4826\n",
            "Epoch 51/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 146933.3594 - mse: 0.4774\n",
            "Epoch 51: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 52us/sample - loss: 148322.0609 - mse: 0.4857 - val_loss: 123764.1758 - val_mse: 0.4818\n",
            "Epoch 52/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 155121.6094 - mse: 0.5041\n",
            "Epoch 52: val_loss did not improve from 121339.60794\n",
            "1524/1524 [==============================] - 0s 48us/sample - loss: 148631.2562 - mse: 0.4867 - val_loss: 123473.4353 - val_mse: 0.4835\n",
            "Epoch 53/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 142394.2656 - mse: 0.4627\n",
            "Epoch 53: val_loss improved from 121339.60794 to 121043.80178, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 124us/sample - loss: 148726.8958 - mse: 0.4867 - val_loss: 121043.8018 - val_mse: 0.4833\n",
            "Epoch 54/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 141391.2188 - mse: 0.4594\n",
            "Epoch 54: val_loss did not improve from 121043.80178\n",
            "1524/1524 [==============================] - 0s 52us/sample - loss: 148165.3284 - mse: 0.4853 - val_loss: 126105.0082 - val_mse: 0.4852\n",
            "Epoch 55/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 133126.5938 - mse: 0.4325\n",
            "Epoch 55: val_loss did not improve from 121043.80178\n",
            "1524/1524 [==============================] - 0s 52us/sample - loss: 148229.1263 - mse: 0.4855 - val_loss: 122321.0191 - val_mse: 0.4821\n",
            "Epoch 56/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 148533.7044 - mse: 0.4863\n",
            "Epoch 56: val_loss improved from 121043.80178 to 120883.88024, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 128us/sample - loss: 148533.7044 - mse: 0.4863 - val_loss: 120883.8802 - val_mse: 0.4820\n",
            "Epoch 57/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 153623.9688 - mse: 0.4991\n",
            "Epoch 57: val_loss did not improve from 120883.88024\n",
            "1524/1524 [==============================] - 0s 52us/sample - loss: 148332.2940 - mse: 0.4857 - val_loss: 122338.6225 - val_mse: 0.4821\n",
            "Epoch 58/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 156163.7969 - mse: 0.5074\n",
            "Epoch 58: val_loss did not improve from 120883.88024\n",
            "1524/1524 [==============================] - 0s 48us/sample - loss: 148180.6370 - mse: 0.4852 - val_loss: 122473.5173 - val_mse: 0.4809\n",
            "Epoch 59/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 147857.3023 - mse: 0.4843\n",
            "Epoch 59: val_loss did not improve from 120883.88024\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 147857.3023 - mse: 0.4843 - val_loss: 127367.5197 - val_mse: 0.4832\n",
            "Epoch 60/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 150616.6562 - mse: 0.4893\n",
            "Epoch 60: val_loss did not improve from 120883.88024\n",
            "1524/1524 [==============================] - 0s 50us/sample - loss: 148210.7879 - mse: 0.4853 - val_loss: 124795.6951 - val_mse: 0.4833\n",
            "Epoch 61/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 147821.7263 - mse: 0.4838\n",
            "Epoch 61: val_loss did not improve from 120883.88024\n",
            "1524/1524 [==============================] - 0s 55us/sample - loss: 147821.7263 - mse: 0.4838 - val_loss: 124984.5125 - val_mse: 0.4817\n",
            "Epoch 62/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 152509.7188 - mse: 0.4955\n",
            "Epoch 62: val_loss did not improve from 120883.88024\n",
            "1524/1524 [==============================] - 0s 50us/sample - loss: 148431.5037 - mse: 0.4857 - val_loss: 122924.7669 - val_mse: 0.4826\n",
            "Epoch 63/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 150102.0312 - mse: 0.4877\n",
            "Epoch 63: val_loss improved from 120883.88024 to 120566.55334, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 126us/sample - loss: 148224.1032 - mse: 0.4851 - val_loss: 120566.5533 - val_mse: 0.4824\n",
            "Epoch 64/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 153349.0469 - mse: 0.4983\n",
            "Epoch 64: val_loss did not improve from 120566.55334\n",
            "1524/1524 [==============================] - 0s 51us/sample - loss: 148068.1854 - mse: 0.4846 - val_loss: 121900.1425 - val_mse: 0.4815\n",
            "Epoch 65/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 147900.9110 - mse: 0.4841\n",
            "Epoch 65: val_loss did not improve from 120566.55334\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 147900.9110 - mse: 0.4841 - val_loss: 121233.8894 - val_mse: 0.4810\n",
            "Epoch 66/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 147851.0586 - mse: 0.4842\n",
            "Epoch 66: val_loss did not improve from 120566.55334\n",
            "1524/1524 [==============================] - 0s 55us/sample - loss: 147851.0586 - mse: 0.4842 - val_loss: 126312.0701 - val_mse: 0.4825\n",
            "Epoch 67/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 149623.6406 - mse: 0.4862\n",
            "Epoch 67: val_loss did not improve from 120566.55334\n",
            "1524/1524 [==============================] - 0s 50us/sample - loss: 147913.0291 - mse: 0.4842 - val_loss: 127390.7489 - val_mse: 0.4818\n",
            "Epoch 68/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 152336.7812 - mse: 0.4950\n",
            "Epoch 68: val_loss did not improve from 120566.55334\n",
            "1524/1524 [==============================] - 0s 49us/sample - loss: 147811.2044 - mse: 0.4836 - val_loss: 123524.2517 - val_mse: 0.4816\n",
            "Epoch 69/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 161964.4375 - mse: 0.5263\n",
            "Epoch 69: val_loss improved from 120566.55334 to 120357.21450, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 127us/sample - loss: 147942.2503 - mse: 0.4838 - val_loss: 120357.2145 - val_mse: 0.4801\n",
            "Epoch 70/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 147628.8552 - mse: 0.4833\n",
            "Epoch 70: val_loss did not improve from 120357.21450\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 147628.8552 - mse: 0.4833 - val_loss: 124603.5754 - val_mse: 0.4815\n",
            "Epoch 71/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 147500.7334 - mse: 0.4832\n",
            "Epoch 71: val_loss did not improve from 120357.21450\n",
            "1524/1524 [==============================] - 0s 55us/sample - loss: 147500.7334 - mse: 0.4832 - val_loss: 122985.9207 - val_mse: 0.4822\n",
            "Epoch 72/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 151048.0469 - mse: 0.4908\n",
            "Epoch 72: val_loss did not improve from 120357.21450\n",
            "1524/1524 [==============================] - 0s 50us/sample - loss: 147755.6341 - mse: 0.4836 - val_loss: 121168.4762 - val_mse: 0.4822\n",
            "Epoch 73/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 147324.1719 - mse: 0.4787\n",
            "Epoch 73: val_loss did not improve from 120357.21450\n",
            "1524/1524 [==============================] - 0s 48us/sample - loss: 147669.1549 - mse: 0.4834 - val_loss: 123672.8314 - val_mse: 0.4821\n",
            "Epoch 74/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 150744.8438 - mse: 0.4898\n",
            "Epoch 74: val_loss did not improve from 120357.21450\n",
            "1524/1524 [==============================] - 0s 51us/sample - loss: 148126.0798 - mse: 0.4850 - val_loss: 122289.0353 - val_mse: 0.4798\n",
            "Epoch 75/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 148109.1295 - mse: 0.4844\n",
            "Epoch 75: val_loss did not improve from 120357.21450\n",
            "1524/1524 [==============================] - 0s 55us/sample - loss: 148109.1295 - mse: 0.4844 - val_loss: 124737.8998 - val_mse: 0.4828\n",
            "Epoch 76/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 147854.9014 - mse: 0.4841\n",
            "Epoch 76: val_loss did not improve from 120357.21450\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 147854.9014 - mse: 0.4841 - val_loss: 122830.5637 - val_mse: 0.4805\n",
            "Epoch 77/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 146927.0156 - mse: 0.4773\n",
            "Epoch 77: val_loss did not improve from 120357.21450\n",
            "1524/1524 [==============================] - 0s 51us/sample - loss: 147626.6805 - mse: 0.4834 - val_loss: 123860.3172 - val_mse: 0.4808\n",
            "Epoch 78/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 149944.5469 - mse: 0.4872\n",
            "Epoch 78: val_loss did not improve from 120357.21450\n",
            "1524/1524 [==============================] - 0s 48us/sample - loss: 147801.5610 - mse: 0.4838 - val_loss: 124348.6801 - val_mse: 0.4803\n",
            "Epoch 79/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 147432.3171 - mse: 0.4827\n",
            "Epoch 79: val_loss did not improve from 120357.21450\n",
            "1524/1524 [==============================] - 0s 53us/sample - loss: 147432.3171 - mse: 0.4827 - val_loss: 126901.9232 - val_mse: 0.4803\n",
            "Epoch 80/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 147672.2872 - mse: 0.4832\n",
            "Epoch 80: val_loss did not improve from 120357.21450\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 147672.2872 - mse: 0.4832 - val_loss: 121559.6741 - val_mse: 0.4801\n",
            "Epoch 81/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 138820.2344 - mse: 0.4510\n",
            "Epoch 81: val_loss did not improve from 120357.21450\n",
            "1524/1524 [==============================] - 0s 50us/sample - loss: 147559.9551 - mse: 0.4832 - val_loss: 121283.5482 - val_mse: 0.4806\n",
            "Epoch 82/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 147490.1341 - mse: 0.4828\n",
            "Epoch 82: val_loss did not improve from 120357.21450\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 147490.1341 - mse: 0.4828 - val_loss: 123078.5148 - val_mse: 0.4802\n",
            "Epoch 83/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 147549.3475 - mse: 0.4827\n",
            "Epoch 83: val_loss did not improve from 120357.21450\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 147549.3475 - mse: 0.4827 - val_loss: 123844.8757 - val_mse: 0.4814\n",
            "Epoch 84/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 147611.6561 - mse: 0.4834\n",
            "Epoch 84: val_loss did not improve from 120357.21450\n",
            "1524/1524 [==============================] - 0s 56us/sample - loss: 147611.6561 - mse: 0.4834 - val_loss: 123048.3875 - val_mse: 0.4801\n",
            "Epoch 85/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 150508.7500 - mse: 0.4890\n",
            "Epoch 85: val_loss improved from 120357.21450 to 119686.93484, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 128us/sample - loss: 147404.4319 - mse: 0.4823 - val_loss: 119686.9348 - val_mse: 0.4799\n",
            "Epoch 86/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 147362.9583 - mse: 0.4825\n",
            "Epoch 86: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 147362.9583 - mse: 0.4825 - val_loss: 124108.2181 - val_mse: 0.4811\n",
            "Epoch 87/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 147361.6996 - mse: 0.4826\n",
            "Epoch 87: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 147361.6996 - mse: 0.4826 - val_loss: 119894.8529 - val_mse: 0.4810\n",
            "Epoch 88/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 148048.1250 - mse: 0.4810\n",
            "Epoch 88: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 49us/sample - loss: 147412.8899 - mse: 0.4823 - val_loss: 123697.7212 - val_mse: 0.4792\n",
            "Epoch 89/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 163680.2500 - mse: 0.5318\n",
            "Epoch 89: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 49us/sample - loss: 147528.1368 - mse: 0.4830 - val_loss: 120969.0253 - val_mse: 0.4801\n",
            "Epoch 90/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 147256.4557 - mse: 0.4820\n",
            "Epoch 90: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 147256.4557 - mse: 0.4820 - val_loss: 122564.1832 - val_mse: 0.4795\n",
            "Epoch 91/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 151193.6250 - mse: 0.4912\n",
            "Epoch 91: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 47us/sample - loss: 147311.6635 - mse: 0.4821 - val_loss: 121794.7662 - val_mse: 0.4813\n",
            "Epoch 92/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 147241.8729 - mse: 0.4820\n",
            "Epoch 92: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 147241.8729 - mse: 0.4820 - val_loss: 119796.3617 - val_mse: 0.4805\n",
            "Epoch 93/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 147125.5895 - mse: 0.4815\n",
            "Epoch 93: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 147125.5895 - mse: 0.4815 - val_loss: 123070.5241 - val_mse: 0.4797\n",
            "Epoch 94/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 135607.6875 - mse: 0.4405\n",
            "Epoch 94: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 50us/sample - loss: 146925.1355 - mse: 0.4811 - val_loss: 124297.4104 - val_mse: 0.4794\n",
            "Epoch 95/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 146995.9873 - mse: 0.4812\n",
            "Epoch 95: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 55us/sample - loss: 146995.9873 - mse: 0.4812 - val_loss: 125037.7457 - val_mse: 0.4797\n",
            "Epoch 96/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 146924.8467 - mse: 0.4810\n",
            "Epoch 96: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 55us/sample - loss: 146924.8467 - mse: 0.4810 - val_loss: 123708.8321 - val_mse: 0.4794\n",
            "Epoch 97/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 147010.5562 - mse: 0.4814\n",
            "Epoch 97: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 55us/sample - loss: 147010.5562 - mse: 0.4814 - val_loss: 124849.8630 - val_mse: 0.4812\n",
            "Epoch 98/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 144604.3750 - mse: 0.4697\n",
            "Epoch 98: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 50us/sample - loss: 147031.4745 - mse: 0.4814 - val_loss: 122221.4820 - val_mse: 0.4796\n",
            "Epoch 99/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 146997.7858 - mse: 0.4811\n",
            "Epoch 99: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 146997.7858 - mse: 0.4811 - val_loss: 123093.1750 - val_mse: 0.4787\n",
            "Epoch 100/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 137779.1562 - mse: 0.4476\n",
            "Epoch 100: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 50us/sample - loss: 147111.6662 - mse: 0.4815 - val_loss: 122038.1710 - val_mse: 0.4793\n",
            "Epoch 101/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 146119.8594 - mse: 0.4747\n",
            "Epoch 101: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 147060.1624 - mse: 0.4814 - val_loss: 122899.1640 - val_mse: 0.4802\n",
            "Epoch 102/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 147276.0949 - mse: 0.4818\n",
            "Epoch 102: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 56us/sample - loss: 147276.0949 - mse: 0.4818 - val_loss: 123191.2750 - val_mse: 0.4785\n",
            "Epoch 103/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 139022.7656 - mse: 0.4516\n",
            "Epoch 103: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 46us/sample - loss: 146874.9699 - mse: 0.4808 - val_loss: 123440.5766 - val_mse: 0.4794\n",
            "Epoch 104/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 147466.5312 - mse: 0.4791\n",
            "Epoch 104: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 50us/sample - loss: 147074.0673 - mse: 0.4810 - val_loss: 121643.5077 - val_mse: 0.4792\n",
            "Epoch 105/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 147009.7288 - mse: 0.4813\n",
            "Epoch 105: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 54us/sample - loss: 147009.7288 - mse: 0.4813 - val_loss: 124124.7459 - val_mse: 0.4794\n",
            "Epoch 106/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 156969.4688 - mse: 0.5100\n",
            "Epoch 106: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 50us/sample - loss: 146859.8311 - mse: 0.4806 - val_loss: 122299.7666 - val_mse: 0.4787\n",
            "Epoch 107/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 146909.2279 - mse: 0.4808\n",
            "Epoch 107: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 146909.2279 - mse: 0.4808 - val_loss: 122701.3004 - val_mse: 0.4786\n",
            "Epoch 108/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 146901.2500 - mse: 0.4772\n",
            "Epoch 108: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 51us/sample - loss: 146904.9707 - mse: 0.4807 - val_loss: 125617.5188 - val_mse: 0.4807\n",
            "Epoch 109/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 148381.2031 - mse: 0.4821\n",
            "Epoch 109: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 49us/sample - loss: 146593.3986 - mse: 0.4799 - val_loss: 124360.0357 - val_mse: 0.4789\n",
            "Epoch 110/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 162286.3125 - mse: 0.5273\n",
            "Epoch 110: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 51us/sample - loss: 146936.6807 - mse: 0.4810 - val_loss: 123621.2784 - val_mse: 0.4783\n",
            "Epoch 111/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 146863.9916 - mse: 0.4804\n",
            "Epoch 111: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 146863.9916 - mse: 0.4804 - val_loss: 121882.4193 - val_mse: 0.4787\n",
            "Epoch 112/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 141769.7188 - mse: 0.4605\n",
            "Epoch 112: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 50us/sample - loss: 146926.1252 - mse: 0.4807 - val_loss: 125088.9813 - val_mse: 0.4783\n",
            "Epoch 113/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 146665.2915 - mse: 0.4799\n",
            "Epoch 113: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 146665.2915 - mse: 0.4799 - val_loss: 124336.3921 - val_mse: 0.4782\n",
            "Epoch 114/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 141476.7656 - mse: 0.4596\n",
            "Epoch 114: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 49us/sample - loss: 146538.9768 - mse: 0.4797 - val_loss: 124377.4069 - val_mse: 0.4795\n",
            "Epoch 115/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 156063.7031 - mse: 0.5070\n",
            "Epoch 115: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 49us/sample - loss: 146709.9582 - mse: 0.4803 - val_loss: 123208.7859 - val_mse: 0.4789\n",
            "Epoch 116/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 150489.8750 - mse: 0.4889\n",
            "Epoch 116: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 50us/sample - loss: 146790.2012 - mse: 0.4805 - val_loss: 120469.1045 - val_mse: 0.4776\n",
            "Epoch 117/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 147781.0625 - mse: 0.4801\n",
            "Epoch 117: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 52us/sample - loss: 146534.6936 - mse: 0.4798 - val_loss: 123080.0879 - val_mse: 0.4779\n",
            "Epoch 118/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 146388.5389 - mse: 0.4791\n",
            "Epoch 118: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 56us/sample - loss: 146388.5389 - mse: 0.4791 - val_loss: 121378.2098 - val_mse: 0.4787\n",
            "Epoch 119/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 146754.7353 - mse: 0.4802\n",
            "Epoch 119: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 55us/sample - loss: 146754.7353 - mse: 0.4802 - val_loss: 120112.3925 - val_mse: 0.4779\n",
            "Epoch 120/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 146578.5825 - mse: 0.4798\n",
            "Epoch 120: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 56us/sample - loss: 146578.5825 - mse: 0.4798 - val_loss: 121842.5377 - val_mse: 0.4777\n",
            "Epoch 121/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 146549.7908 - mse: 0.4799\n",
            "Epoch 121: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 146549.7908 - mse: 0.4799 - val_loss: 122323.2365 - val_mse: 0.4786\n",
            "Epoch 122/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 157247.2188 - mse: 0.5108\n",
            "Epoch 122: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 48us/sample - loss: 146515.6772 - mse: 0.4796 - val_loss: 123974.5800 - val_mse: 0.4786\n",
            "Epoch 123/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 146357.6985 - mse: 0.4792\n",
            "Epoch 123: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 55us/sample - loss: 146357.6985 - mse: 0.4792 - val_loss: 125055.8946 - val_mse: 0.4774\n",
            "Epoch 124/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 146443.2409 - mse: 0.4793\n",
            "Epoch 124: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 146443.2409 - mse: 0.4793 - val_loss: 121941.9105 - val_mse: 0.4778\n",
            "Epoch 125/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 145333.2031 - mse: 0.4721\n",
            "Epoch 125: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 53us/sample - loss: 146233.4763 - mse: 0.4790 - val_loss: 125929.8023 - val_mse: 0.4772\n",
            "Epoch 126/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 146358.0828 - mse: 0.4792\n",
            "Epoch 126: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 56us/sample - loss: 146358.0828 - mse: 0.4792 - val_loss: 119917.5221 - val_mse: 0.4780\n",
            "Epoch 127/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 146292.0482 - mse: 0.4790\n",
            "Epoch 127: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 146292.0482 - mse: 0.4790 - val_loss: 120794.5635 - val_mse: 0.4779\n",
            "Epoch 128/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 151392.2812 - mse: 0.4918\n",
            "Epoch 128: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 52us/sample - loss: 146381.8535 - mse: 0.4789 - val_loss: 121462.4809 - val_mse: 0.4772\n",
            "Epoch 129/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 146126.5067 - mse: 0.4785\n",
            "Epoch 129: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 146126.5067 - mse: 0.4785 - val_loss: 121876.7860 - val_mse: 0.4774\n",
            "Epoch 130/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 146087.2889 - mse: 0.4784\n",
            "Epoch 130: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 55us/sample - loss: 146087.2889 - mse: 0.4784 - val_loss: 123067.0725 - val_mse: 0.4772\n",
            "Epoch 131/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 146130.0974 - mse: 0.4783\n",
            "Epoch 131: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 56us/sample - loss: 146130.0974 - mse: 0.4783 - val_loss: 120411.7044 - val_mse: 0.4772\n",
            "Epoch 132/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 146036.8706 - mse: 0.4780\n",
            "Epoch 132: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 146036.8706 - mse: 0.4780 - val_loss: 123363.3896 - val_mse: 0.4765\n",
            "Epoch 133/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 145855.4605 - mse: 0.4778\n",
            "Epoch 133: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 145855.4605 - mse: 0.4778 - val_loss: 121778.4417 - val_mse: 0.4764\n",
            "Epoch 134/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 135883.3750 - mse: 0.4414\n",
            "Epoch 134: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 53us/sample - loss: 145796.5370 - mse: 0.4774 - val_loss: 123437.6954 - val_mse: 0.4762\n",
            "Epoch 135/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 145729.2909 - mse: 0.4771\n",
            "Epoch 135: val_loss did not improve from 119686.93484\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 145729.2909 - mse: 0.4771 - val_loss: 123327.0921 - val_mse: 0.4760\n",
            "Epoch 136/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 153849.4688 - mse: 0.4998\n",
            "Epoch 136: val_loss improved from 119686.93484 to 119044.88134, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 129us/sample - loss: 145941.8719 - mse: 0.4772 - val_loss: 119044.8813 - val_mse: 0.4784\n",
            "Epoch 137/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 149237.0187 - mse: 0.4848\n",
            "Epoch 137: val_loss did not improve from 119044.88134\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 146157.2037 - mse: 0.4781 - val_loss: 120323.0446 - val_mse: 0.4763\n",
            "Epoch 138/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 145949.2691 - mse: 0.4776\n",
            "Epoch 138: val_loss did not improve from 119044.88134\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 145949.2691 - mse: 0.4776 - val_loss: 119372.4447 - val_mse: 0.4757\n",
            "Epoch 139/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 145814.0221 - mse: 0.4771\n",
            "Epoch 139: val_loss did not improve from 119044.88134\n",
            "1524/1524 [==============================] - 0s 68us/sample - loss: 145814.0221 - mse: 0.4771 - val_loss: 119078.7825 - val_mse: 0.4753\n",
            "Epoch 140/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 147790.6562 - mse: 0.4801\n",
            "Epoch 140: val_loss did not improve from 119044.88134\n",
            "1524/1524 [==============================] - 0s 68us/sample - loss: 145706.5796 - mse: 0.4767 - val_loss: 123113.1202 - val_mse: 0.4750\n",
            "Epoch 141/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 144327.2000 - mse: 0.4688\n",
            "Epoch 141: val_loss did not improve from 119044.88134\n",
            "1524/1524 [==============================] - 0s 66us/sample - loss: 145385.9498 - mse: 0.4761 - val_loss: 121281.6750 - val_mse: 0.4747\n",
            "Epoch 142/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 145487.9656 - mse: 0.4726\n",
            "Epoch 142: val_loss did not improve from 119044.88134\n",
            "1524/1524 [==============================] - 0s 80us/sample - loss: 145494.2527 - mse: 0.4763 - val_loss: 120141.8073 - val_mse: 0.4754\n",
            "Epoch 143/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 145613.3188 - mse: 0.4730\n",
            "Epoch 143: val_loss did not improve from 119044.88134\n",
            "1524/1524 [==============================] - 0s 67us/sample - loss: 145489.9443 - mse: 0.4763 - val_loss: 123285.1408 - val_mse: 0.4747\n",
            "Epoch 144/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 145476.7000 - mse: 0.4726\n",
            "Epoch 144: val_loss did not improve from 119044.88134\n",
            "1524/1524 [==============================] - 0s 73us/sample - loss: 145226.4264 - mse: 0.4754 - val_loss: 120121.4872 - val_mse: 0.4741\n",
            "Epoch 145/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 144253.8875 - mse: 0.4686\n",
            "Epoch 145: val_loss improved from 119044.88134 to 117571.51984, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 178us/sample - loss: 145051.8312 - mse: 0.4750 - val_loss: 117571.5198 - val_mse: 0.4738\n",
            "Epoch 146/500\n",
            "1024/1524 [===================>..........] - ETA: 0s - loss: 142253.0156 - mse: 0.4621\n",
            "Epoch 146: val_loss did not improve from 117571.51984\n",
            "1524/1524 [==============================] - 0s 86us/sample - loss: 144956.5323 - mse: 0.4745 - val_loss: 122594.9836 - val_mse: 0.4763\n",
            "Epoch 147/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 147164.3062 - mse: 0.4780\n",
            "Epoch 147: val_loss did not improve from 117571.51984\n",
            "1524/1524 [==============================] - 0s 71us/sample - loss: 145014.4625 - mse: 0.4745 - val_loss: 120583.0847 - val_mse: 0.4726\n",
            "Epoch 148/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 147524.5719 - mse: 0.4792\n",
            "Epoch 148: val_loss did not improve from 117571.51984\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 145023.7705 - mse: 0.4744 - val_loss: 121675.8107 - val_mse: 0.4733\n",
            "Epoch 149/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 144874.8744 - mse: 0.4741\n",
            "Epoch 149: val_loss did not improve from 117571.51984\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 144874.8744 - mse: 0.4741 - val_loss: 123073.0016 - val_mse: 0.4726\n",
            "Epoch 150/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 144735.3964 - mse: 0.4737\n",
            "Epoch 150: val_loss did not improve from 117571.51984\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 144735.3964 - mse: 0.4737 - val_loss: 121116.3984 - val_mse: 0.4720\n",
            "Epoch 151/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 148060.5500 - mse: 0.4810\n",
            "Epoch 151: val_loss did not improve from 117571.51984\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 144697.2903 - mse: 0.4733 - val_loss: 121516.4383 - val_mse: 0.4726\n",
            "Epoch 152/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 147216.5375 - mse: 0.4782\n",
            "Epoch 152: val_loss did not improve from 117571.51984\n",
            "1524/1524 [==============================] - 0s 72us/sample - loss: 144628.0041 - mse: 0.4731 - val_loss: 121603.4525 - val_mse: 0.4709\n",
            "Epoch 153/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 147663.4156 - mse: 0.4797\n",
            "Epoch 153: val_loss did not improve from 117571.51984\n",
            "1524/1524 [==============================] - 0s 68us/sample - loss: 144333.6357 - mse: 0.4721 - val_loss: 122257.3293 - val_mse: 0.4710\n",
            "Epoch 154/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 146194.4562 - mse: 0.4749\n",
            "Epoch 154: val_loss did not improve from 117571.51984\n",
            "1524/1524 [==============================] - 0s 70us/sample - loss: 144157.2149 - mse: 0.4717 - val_loss: 119787.8125 - val_mse: 0.4711\n",
            "Epoch 155/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 144200.2155 - mse: 0.4715\n",
            "Epoch 155: val_loss did not improve from 117571.51984\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 144200.2155 - mse: 0.4715 - val_loss: 121234.6570 - val_mse: 0.4702\n",
            "Epoch 156/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 145417.0125 - mse: 0.4723\n",
            "Epoch 156: val_loss did not improve from 117571.51984\n",
            "1524/1524 [==============================] - 0s 69us/sample - loss: 143800.1027 - mse: 0.4705 - val_loss: 118295.3706 - val_mse: 0.4700\n",
            "Epoch 157/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 143427.8643 - mse: 0.4694\n",
            "Epoch 157: val_loss improved from 117571.51984 to 117570.74247, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 160us/sample - loss: 143427.8643 - mse: 0.4694 - val_loss: 117570.7425 - val_mse: 0.4680\n",
            "Epoch 158/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 145200.8000 - mse: 0.4716\n",
            "Epoch 158: val_loss did not improve from 117570.74247\n",
            "1524/1524 [==============================] - 0s 70us/sample - loss: 143000.6793 - mse: 0.4678 - val_loss: 121333.0914 - val_mse: 0.4663\n",
            "Epoch 159/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 144725.3500 - mse: 0.4701\n",
            "Epoch 159: val_loss did not improve from 117570.74247\n",
            "1524/1524 [==============================] - 0s 70us/sample - loss: 142714.6580 - mse: 0.4669 - val_loss: 118316.8245 - val_mse: 0.4664\n",
            "Epoch 160/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 142953.4250 - mse: 0.4643\n",
            "Epoch 160: val_loss did not improve from 117570.74247\n",
            "1524/1524 [==============================] - 0s 67us/sample - loss: 142526.8948 - mse: 0.4665 - val_loss: 119816.9575 - val_mse: 0.4646\n",
            "Epoch 161/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 145409.8453 - mse: 0.4723\n",
            "Epoch 161: val_loss improved from 117570.74247 to 116943.98683, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 160us/sample - loss: 142047.2076 - mse: 0.4645 - val_loss: 116943.9868 - val_mse: 0.4635\n",
            "Epoch 162/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 141730.7269 - mse: 0.4639\n",
            "Epoch 162: val_loss improved from 116943.98683 to 116867.21531, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 160us/sample - loss: 141730.7269 - mse: 0.4639 - val_loss: 116867.2153 - val_mse: 0.4609\n",
            "Epoch 163/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 144877.1063 - mse: 0.4706\n",
            "Epoch 163: val_loss did not improve from 116867.21531\n",
            "1524/1524 [==============================] - 0s 66us/sample - loss: 141318.5795 - mse: 0.4621 - val_loss: 120149.1817 - val_mse: 0.4612\n",
            "Epoch 164/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 140614.7473 - mse: 0.4598\n",
            "Epoch 164: val_loss improved from 116867.21531 to 116838.24521, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 159us/sample - loss: 140614.7473 - mse: 0.4598 - val_loss: 116838.2452 - val_mse: 0.4581\n",
            "Epoch 165/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 139918.2207 - mse: 0.4577\n",
            "Epoch 165: val_loss did not improve from 116838.24521\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 139918.2207 - mse: 0.4577 - val_loss: 119488.8191 - val_mse: 0.4561\n",
            "Epoch 166/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 139918.3219 - mse: 0.4544\n",
            "Epoch 166: val_loss did not improve from 116838.24521\n",
            "1524/1524 [==============================] - 0s 65us/sample - loss: 139158.3020 - mse: 0.4554 - val_loss: 116941.6549 - val_mse: 0.4540\n",
            "Epoch 167/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 139827.7422 - mse: 0.4541\n",
            "Epoch 167: val_loss improved from 116838.24521 to 113997.52998, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 169us/sample - loss: 138427.5778 - mse: 0.4529 - val_loss: 113997.5300 - val_mse: 0.4508\n",
            "Epoch 168/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 137615.1809 - mse: 0.4506\n",
            "Epoch 168: val_loss did not improve from 113997.52998\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 137615.1809 - mse: 0.4506 - val_loss: 116300.7594 - val_mse: 0.4491\n",
            "Epoch 169/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 136953.3375 - mse: 0.4448\n",
            "Epoch 169: val_loss did not improve from 113997.52998\n",
            "1524/1524 [==============================] - 0s 70us/sample - loss: 136618.7591 - mse: 0.4471 - val_loss: 115065.2208 - val_mse: 0.4438\n",
            "Epoch 170/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 136441.5125 - mse: 0.4431\n",
            "Epoch 170: val_loss did not improve from 113997.52998\n",
            "1524/1524 [==============================] - 0s 69us/sample - loss: 135482.0026 - mse: 0.4433 - val_loss: 114611.6689 - val_mse: 0.4433\n",
            "Epoch 171/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 136798.0047 - mse: 0.4442\n",
            "Epoch 171: val_loss improved from 113997.52998 to 113291.81033, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 170us/sample - loss: 135410.2652 - mse: 0.4430 - val_loss: 113291.8103 - val_mse: 0.4416\n",
            "Epoch 172/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 134589.4922 - mse: 0.4371\n",
            "Epoch 172: val_loss improved from 113291.81033 to 110492.98879, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 186us/sample - loss: 134050.3174 - mse: 0.4387 - val_loss: 110492.9888 - val_mse: 0.4368\n",
            "Epoch 173/500\n",
            "1024/1524 [===================>..........] - ETA: 0s - loss: 134372.7168 - mse: 0.4363\n",
            "Epoch 173: val_loss improved from 110492.98879 to 110221.97501, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 173us/sample - loss: 132417.1616 - mse: 0.4333 - val_loss: 110221.9750 - val_mse: 0.4340\n",
            "Epoch 174/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 133956.6578 - mse: 0.4350\n",
            "Epoch 174: val_loss improved from 110221.97501 to 109427.19331, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 166us/sample - loss: 131263.1288 - mse: 0.4292 - val_loss: 109427.1933 - val_mse: 0.4321\n",
            "Epoch 175/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 130799.8313 - mse: 0.4247\n",
            "Epoch 175: val_loss did not improve from 109427.19331\n",
            "1524/1524 [==============================] - 0s 66us/sample - loss: 130304.8712 - mse: 0.4264 - val_loss: 110329.6329 - val_mse: 0.4271\n",
            "Epoch 176/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 129612.9334 - mse: 0.4240\n",
            "Epoch 176: val_loss improved from 109427.19331 to 106901.89971, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 171us/sample - loss: 129612.9334 - mse: 0.4240 - val_loss: 106901.8997 - val_mse: 0.4274\n",
            "Epoch 177/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 130114.9016 - mse: 0.4225\n",
            "Epoch 177: val_loss improved from 106901.89971 to 106196.82923, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 167us/sample - loss: 128788.1636 - mse: 0.4213 - val_loss: 106196.8292 - val_mse: 0.4228\n",
            "Epoch 178/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 127760.7355 - mse: 0.4180\n",
            "Epoch 178: val_loss did not improve from 106196.82923\n",
            "1524/1524 [==============================] - 0s 65us/sample - loss: 127760.7355 - mse: 0.4180 - val_loss: 108242.5387 - val_mse: 0.4165\n",
            "Epoch 179/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 126826.1062 - mse: 0.4118\n",
            "Epoch 179: val_loss did not improve from 106196.82923\n",
            "1524/1524 [==============================] - 0s 73us/sample - loss: 126584.5421 - mse: 0.4142 - val_loss: 106969.6648 - val_mse: 0.4163\n",
            "Epoch 180/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 126433.0841 - mse: 0.4138\n",
            "Epoch 180: val_loss improved from 106196.82923 to 103963.02127, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 146us/sample - loss: 126433.0841 - mse: 0.4138 - val_loss: 103963.0213 - val_mse: 0.4135\n",
            "Epoch 181/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 125664.7974 - mse: 0.4109\n",
            "Epoch 181: val_loss did not improve from 103963.02127\n",
            "1524/1524 [==============================] - 0s 68us/sample - loss: 125664.7974 - mse: 0.4109 - val_loss: 105859.3764 - val_mse: 0.4126\n",
            "Epoch 182/500\n",
            "1024/1524 [===================>..........] - ETA: 0s - loss: 122612.9316 - mse: 0.3981\n",
            "Epoch 182: val_loss did not improve from 103963.02127\n",
            "1524/1524 [==============================] - 0s 79us/sample - loss: 124811.3546 - mse: 0.4082 - val_loss: 106946.8150 - val_mse: 0.4113\n",
            "Epoch 183/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 124338.6128 - mse: 0.4069\n",
            "Epoch 183: val_loss did not improve from 103963.02127\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 124338.6128 - mse: 0.4069 - val_loss: 107087.7689 - val_mse: 0.4115\n",
            "Epoch 184/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 125210.8828 - mse: 0.4065\n",
            "Epoch 184: val_loss did not improve from 103963.02127\n",
            "1524/1524 [==============================] - 0s 74us/sample - loss: 123850.0167 - mse: 0.4050 - val_loss: 106679.6923 - val_mse: 0.4070\n",
            "Epoch 185/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 124447.6844 - mse: 0.4040\n",
            "Epoch 185: val_loss improved from 103963.02127 to 102658.04291, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 166us/sample - loss: 123361.3588 - mse: 0.4035 - val_loss: 102658.0429 - val_mse: 0.4063\n",
            "Epoch 186/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 123484.0578 - mse: 0.4008\n",
            "Epoch 186: val_loss did not improve from 102658.04291\n",
            "1524/1524 [==============================] - 0s 71us/sample - loss: 123345.5690 - mse: 0.4035 - val_loss: 106617.2089 - val_mse: 0.4048\n",
            "Epoch 187/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 123194.7328 - mse: 0.3999\n",
            "Epoch 187: val_loss did not improve from 102658.04291\n",
            "1524/1524 [==============================] - 0s 72us/sample - loss: 122821.0968 - mse: 0.4018 - val_loss: 105240.4336 - val_mse: 0.4037\n",
            "Epoch 188/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 122165.0688 - mse: 0.3965\n",
            "Epoch 188: val_loss did not improve from 102658.04291\n",
            "1524/1524 [==============================] - 0s 71us/sample - loss: 122438.2256 - mse: 0.4006 - val_loss: 104690.0520 - val_mse: 0.4031\n",
            "Epoch 189/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 122299.9469 - mse: 0.3970\n",
            "Epoch 189: val_loss improved from 102658.04291 to 101522.50299, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 155us/sample - loss: 122043.9988 - mse: 0.3992 - val_loss: 101522.5030 - val_mse: 0.4036\n",
            "Epoch 190/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 121561.0205 - mse: 0.3975\n",
            "Epoch 190: val_loss did not improve from 101522.50299\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 121561.0205 - mse: 0.3975 - val_loss: 102722.5053 - val_mse: 0.3996\n",
            "Epoch 191/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 123237.9219 - mse: 0.4000\n",
            "Epoch 191: val_loss did not improve from 101522.50299\n",
            "1524/1524 [==============================] - 0s 52us/sample - loss: 121441.1351 - mse: 0.3967 - val_loss: 102859.0440 - val_mse: 0.3989\n",
            "Epoch 192/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 121062.1087 - mse: 0.3959\n",
            "Epoch 192: val_loss did not improve from 101522.50299\n",
            "1524/1524 [==============================] - 0s 64us/sample - loss: 121062.1087 - mse: 0.3959 - val_loss: 102138.7667 - val_mse: 0.3977\n",
            "Epoch 193/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 115107.5547 - mse: 0.3736\n",
            "Epoch 193: val_loss did not improve from 101522.50299\n",
            "1524/1524 [==============================] - 0s 55us/sample - loss: 120291.8534 - mse: 0.3932 - val_loss: 101786.9575 - val_mse: 0.3993\n",
            "Epoch 194/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 117935.7344 - mse: 0.3828\n",
            "Epoch 194: val_loss did not improve from 101522.50299\n",
            "1524/1524 [==============================] - 0s 50us/sample - loss: 120013.1393 - mse: 0.3924 - val_loss: 103154.1042 - val_mse: 0.3985\n",
            "Epoch 195/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 119842.2229 - mse: 0.3921\n",
            "Epoch 195: val_loss improved from 101522.50299 to 99917.72826, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 147us/sample - loss: 119842.2229 - mse: 0.3921 - val_loss: 99917.7283 - val_mse: 0.3944\n",
            "Epoch 196/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 119668.3763 - mse: 0.3915\n",
            "Epoch 196: val_loss did not improve from 99917.72826\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 119668.3763 - mse: 0.3915 - val_loss: 100972.1801 - val_mse: 0.3929\n",
            "Epoch 197/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 116811.2500 - mse: 0.3791\n",
            "Epoch 197: val_loss did not improve from 99917.72826\n",
            "1524/1524 [==============================] - 0s 53us/sample - loss: 119290.6427 - mse: 0.3903 - val_loss: 101468.0632 - val_mse: 0.3921\n",
            "Epoch 198/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 117459.3828 - mse: 0.3812\n",
            "Epoch 198: val_loss did not improve from 99917.72826\n",
            "1524/1524 [==============================] - 0s 51us/sample - loss: 118459.6495 - mse: 0.3875 - val_loss: 102905.5746 - val_mse: 0.3934\n",
            "Epoch 199/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 118167.7528 - mse: 0.3864\n",
            "Epoch 199: val_loss did not improve from 99917.72826\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 118167.7528 - mse: 0.3864 - val_loss: 101178.6534 - val_mse: 0.3905\n",
            "Epoch 200/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 117432.5149 - mse: 0.3843\n",
            "Epoch 200: val_loss did not improve from 99917.72826\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 117432.5149 - mse: 0.3843 - val_loss: 101068.1982 - val_mse: 0.3864\n",
            "Epoch 201/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 116956.9449 - mse: 0.3824\n",
            "Epoch 201: val_loss improved from 99917.72826 to 96124.29628, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 149us/sample - loss: 116956.9449 - mse: 0.3824 - val_loss: 96124.2963 - val_mse: 0.3834\n",
            "Epoch 202/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 122370.7422 - mse: 0.3972\n",
            "Epoch 202: val_loss did not improve from 96124.29628\n",
            "1524/1524 [==============================] - 0s 53us/sample - loss: 116715.5416 - mse: 0.3817 - val_loss: 97737.2021 - val_mse: 0.3822\n",
            "Epoch 203/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 116002.4404 - mse: 0.3792\n",
            "Epoch 203: val_loss did not improve from 96124.29628\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 116002.4404 - mse: 0.3792 - val_loss: 100044.4235 - val_mse: 0.3824\n",
            "Epoch 204/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 115505.6980 - mse: 0.3774\n",
            "Epoch 204: val_loss did not improve from 96124.29628\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 115505.6980 - mse: 0.3774 - val_loss: 99310.6854 - val_mse: 0.3796\n",
            "Epoch 205/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 114455.8455 - mse: 0.3742\n",
            "Epoch 205: val_loss improved from 96124.29628 to 95931.27540, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 155us/sample - loss: 114455.8455 - mse: 0.3742 - val_loss: 95931.2754 - val_mse: 0.3722\n",
            "Epoch 206/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 113725.7378 - mse: 0.3720\n",
            "Epoch 206: val_loss improved from 95931.27540 to 95917.03753, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 287us/sample - loss: 113725.7378 - mse: 0.3720 - val_loss: 95917.0375 - val_mse: 0.3703\n",
            "Epoch 207/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 112488.3870 - mse: 0.3678\n",
            "Epoch 207: val_loss did not improve from 95917.03753\n",
            "1524/1524 [==============================] - 0s 67us/sample - loss: 112488.3870 - mse: 0.3678 - val_loss: 96602.1957 - val_mse: 0.3698\n",
            "Epoch 208/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 111597.7431 - mse: 0.3648\n",
            "Epoch 208: val_loss improved from 95917.03753 to 94689.54203, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 133us/sample - loss: 111597.7431 - mse: 0.3648 - val_loss: 94689.5420 - val_mse: 0.3655\n",
            "Epoch 209/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 109088.0547 - mse: 0.3539\n",
            "Epoch 209: val_loss improved from 94689.54203 to 91108.82610, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 128us/sample - loss: 110616.7304 - mse: 0.3618 - val_loss: 91108.8261 - val_mse: 0.3617\n",
            "Epoch 210/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 109997.2127 - mse: 0.3595\n",
            "Epoch 210: val_loss improved from 91108.82610 to 90708.21932, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 141us/sample - loss: 109997.2127 - mse: 0.3595 - val_loss: 90708.2193 - val_mse: 0.3567\n",
            "Epoch 211/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 109021.3296 - mse: 0.3563\n",
            "Epoch 211: val_loss did not improve from 90708.21932\n",
            "1524/1524 [==============================] - 0s 66us/sample - loss: 109021.3296 - mse: 0.3563 - val_loss: 91647.1333 - val_mse: 0.3582\n",
            "Epoch 212/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 106202.4438 - mse: 0.3445\n",
            "Epoch 212: val_loss did not improve from 90708.21932\n",
            "1524/1524 [==============================] - 0s 64us/sample - loss: 107806.3420 - mse: 0.3527 - val_loss: 91137.6647 - val_mse: 0.3509\n",
            "Epoch 213/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 107389.2808 - mse: 0.3506\n",
            "Epoch 213: val_loss improved from 90708.21932 to 88816.96384, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 134us/sample - loss: 107389.2808 - mse: 0.3506 - val_loss: 88816.9638 - val_mse: 0.3503\n",
            "Epoch 214/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 106456.7700 - mse: 0.3474\n",
            "Epoch 214: val_loss did not improve from 88816.96384\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 106456.7700 - mse: 0.3474 - val_loss: 89803.8038 - val_mse: 0.3461\n",
            "Epoch 215/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 105704.6779 - mse: 0.3454\n",
            "Epoch 215: val_loss improved from 88816.96384 to 87248.14089, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 143us/sample - loss: 105704.6779 - mse: 0.3454 - val_loss: 87248.1409 - val_mse: 0.3430\n",
            "Epoch 216/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 104338.8693 - mse: 0.3409\n",
            "Epoch 216: val_loss did not improve from 87248.14089\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 104338.8693 - mse: 0.3409 - val_loss: 89321.0804 - val_mse: 0.3402\n",
            "Epoch 217/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 103445.1646 - mse: 0.3379\n",
            "Epoch 217: val_loss improved from 87248.14089 to 84290.39879, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 134us/sample - loss: 103445.1646 - mse: 0.3379 - val_loss: 84290.3988 - val_mse: 0.3374\n",
            "Epoch 218/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 102944.2977 - mse: 0.3363\n",
            "Epoch 218: val_loss improved from 84290.39879 to 83105.82033, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 134us/sample - loss: 102944.2977 - mse: 0.3363 - val_loss: 83105.8203 - val_mse: 0.3347\n",
            "Epoch 219/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 99591.8438 - mse: 0.3229\n",
            "Epoch 219: val_loss did not improve from 83105.82033\n",
            "1524/1524 [==============================] - 0s 52us/sample - loss: 102374.5955 - mse: 0.3343 - val_loss: 84312.8754 - val_mse: 0.3314\n",
            "Epoch 220/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 101909.8124 - mse: 0.3328\n",
            "Epoch 220: val_loss did not improve from 83105.82033\n",
            "1524/1524 [==============================] - 0s 60us/sample - loss: 101909.8124 - mse: 0.3328 - val_loss: 86778.4614 - val_mse: 0.3303\n",
            "Epoch 221/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 103014.8891 - mse: 0.3340\n",
            "Epoch 221: val_loss improved from 83105.82033 to 82990.99548, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 141us/sample - loss: 101787.9060 - mse: 0.3325 - val_loss: 82990.9955 - val_mse: 0.3293\n",
            "Epoch 222/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 100942.2092 - mse: 0.3298\n",
            "Epoch 222: val_loss did not improve from 82990.99548\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 100942.2092 - mse: 0.3298 - val_loss: 83585.1414 - val_mse: 0.3275\n",
            "Epoch 223/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 100715.7947 - mse: 0.3293\n",
            "Epoch 223: val_loss did not improve from 82990.99548\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 100715.7947 - mse: 0.3293 - val_loss: 83086.6421 - val_mse: 0.3252\n",
            "Epoch 224/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 102195.6328 - mse: 0.3314\n",
            "Epoch 224: val_loss did not improve from 82990.99548\n",
            "1524/1524 [==============================] - 0s 49us/sample - loss: 100238.5566 - mse: 0.3274 - val_loss: 85513.0780 - val_mse: 0.3266\n",
            "Epoch 225/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 99940.9953 - mse: 0.3266\n",
            "Epoch 225: val_loss did not improve from 82990.99548\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 99940.9953 - mse: 0.3266 - val_loss: 84904.9146 - val_mse: 0.3255\n",
            "Epoch 226/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 99923.8804 - mse: 0.3265 \n",
            "Epoch 226: val_loss did not improve from 82990.99548\n",
            "1524/1524 [==============================] - 0s 60us/sample - loss: 99923.8804 - mse: 0.3265 - val_loss: 84863.6906 - val_mse: 0.3238\n",
            "Epoch 227/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 101595.6609 - mse: 0.3294\n",
            "Epoch 227: val_loss improved from 82990.99548 to 80281.26380, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 138us/sample - loss: 99527.8192 - mse: 0.3249 - val_loss: 80281.2638 - val_mse: 0.3228\n",
            "Epoch 228/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 99275.4138 - mse: 0.3242\n",
            "Epoch 228: val_loss did not improve from 80281.26380\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 99275.4138 - mse: 0.3242 - val_loss: 82459.1718 - val_mse: 0.3226\n",
            "Epoch 229/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 99016.9557 - mse: 0.3233\n",
            "Epoch 229: val_loss did not improve from 80281.26380\n",
            "1524/1524 [==============================] - 0s 56us/sample - loss: 99016.9557 - mse: 0.3233 - val_loss: 81908.2352 - val_mse: 0.3202\n",
            "Epoch 230/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 98488.0613 - mse: 0.3218 \n",
            "Epoch 230: val_loss did not improve from 80281.26380\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 98488.0613 - mse: 0.3218 - val_loss: 82262.8476 - val_mse: 0.3197\n",
            "Epoch 231/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 98224.4482 - mse: 0.3207\n",
            "Epoch 231: val_loss improved from 80281.26380 to 80110.09708, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 132us/sample - loss: 98224.4482 - mse: 0.3207 - val_loss: 80110.0971 - val_mse: 0.3218\n",
            "Epoch 232/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 98038.5382 - mse: 0.3202\n",
            "Epoch 232: val_loss improved from 80110.09708 to 78646.73523, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 141us/sample - loss: 98038.5382 - mse: 0.3202 - val_loss: 78646.7352 - val_mse: 0.3185\n",
            "Epoch 233/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 99470.3375 - mse: 0.3224 \n",
            "Epoch 233: val_loss did not improve from 78646.73523\n",
            "1524/1524 [==============================] - 0s 65us/sample - loss: 97648.8766 - mse: 0.3188 - val_loss: 81671.9029 - val_mse: 0.3183\n",
            "Epoch 234/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 96686.4609 - mse: 0.3134\n",
            "Epoch 234: val_loss did not improve from 78646.73523\n",
            "1524/1524 [==============================] - 0s 64us/sample - loss: 97371.1086 - mse: 0.3182 - val_loss: 79710.5918 - val_mse: 0.3187\n",
            "Epoch 235/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 97061.6048 - mse: 0.3169 \n",
            "Epoch 235: val_loss did not improve from 78646.73523\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 97061.6048 - mse: 0.3169 - val_loss: 84066.1011 - val_mse: 0.3191\n",
            "Epoch 236/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 97137.1780 - mse: 0.3169\n",
            "Epoch 236: val_loss did not improve from 78646.73523\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 97137.1780 - mse: 0.3169 - val_loss: 81162.8098 - val_mse: 0.3184\n",
            "Epoch 237/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 98030.6406 - mse: 0.3177 \n",
            "Epoch 237: val_loss did not improve from 78646.73523\n",
            "1524/1524 [==============================] - 0s 64us/sample - loss: 97136.2466 - mse: 0.3172 - val_loss: 81936.0138 - val_mse: 0.3169\n",
            "Epoch 238/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 97154.9018 - mse: 0.3172\n",
            "Epoch 238: val_loss did not improve from 78646.73523\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 97154.9018 - mse: 0.3172 - val_loss: 81731.3541 - val_mse: 0.3151\n",
            "Epoch 239/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 98191.1516 - mse: 0.3183 \n",
            "Epoch 239: val_loss did not improve from 78646.73523\n",
            "1524/1524 [==============================] - 0s 71us/sample - loss: 96722.7194 - mse: 0.3157 - val_loss: 82783.7578 - val_mse: 0.3171\n",
            "Epoch 240/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 96417.6294 - mse: 0.3149\n",
            "Epoch 240: val_loss did not improve from 78646.73523\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 96417.6294 - mse: 0.3149 - val_loss: 80467.5158 - val_mse: 0.3150\n",
            "Epoch 241/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 96378.3221 - mse: 0.3146\n",
            "Epoch 241: val_loss did not improve from 78646.73523\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 96378.3221 - mse: 0.3146 - val_loss: 79959.6832 - val_mse: 0.3151\n",
            "Epoch 242/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 96065.5771 - mse: 0.3138 \n",
            "Epoch 242: val_loss did not improve from 78646.73523\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 96065.5771 - mse: 0.3138 - val_loss: 81445.6879 - val_mse: 0.3128\n",
            "Epoch 243/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 96135.2589 - mse: 0.3139\n",
            "Epoch 243: val_loss did not improve from 78646.73523\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 96135.2589 - mse: 0.3139 - val_loss: 81184.5852 - val_mse: 0.3139\n",
            "Epoch 244/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 95738.4010 - mse: 0.3126\n",
            "Epoch 244: val_loss did not improve from 78646.73523\n",
            "1524/1524 [==============================] - 0s 64us/sample - loss: 95738.4010 - mse: 0.3126 - val_loss: 81923.6538 - val_mse: 0.3129\n",
            "Epoch 245/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 110572.6562 - mse: 0.3585\n",
            "Epoch 245: val_loss improved from 78646.73523 to 77857.47636, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 125us/sample - loss: 95499.4141 - mse: 0.3120 - val_loss: 77857.4764 - val_mse: 0.3134\n",
            "Epoch 246/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 95295.7790 - mse: 0.3111\n",
            "Epoch 246: val_loss did not improve from 77857.47636\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 95295.7790 - mse: 0.3111 - val_loss: 80386.9471 - val_mse: 0.3129\n",
            "Epoch 247/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 95521.3896 - mse: 0.3117 \n",
            "Epoch 247: val_loss did not improve from 77857.47636\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 95521.3896 - mse: 0.3117 - val_loss: 79978.8695 - val_mse: 0.3123\n",
            "Epoch 248/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 88834.6562 - mse: 0.2878\n",
            "Epoch 248: val_loss did not improve from 77857.47636\n",
            "1524/1524 [==============================] - 0s 51us/sample - loss: 95268.8501 - mse: 0.3112 - val_loss: 80755.3293 - val_mse: 0.3122\n",
            "Epoch 249/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 95120.2632 - mse: 0.3106 \n",
            "Epoch 249: val_loss did not improve from 77857.47636\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 95120.2632 - mse: 0.3106 - val_loss: 79276.0731 - val_mse: 0.3098\n",
            "Epoch 250/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 94904.6638 - mse: 0.3099\n",
            "Epoch 250: val_loss did not improve from 77857.47636\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 94904.6638 - mse: 0.3099 - val_loss: 79211.9917 - val_mse: 0.3115\n",
            "Epoch 251/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 94648.9594 - mse: 0.3090\n",
            "Epoch 251: val_loss did not improve from 77857.47636\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 94648.9594 - mse: 0.3090 - val_loss: 79274.5290 - val_mse: 0.3104\n",
            "Epoch 252/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 94683.3135 - mse: 0.3091\n",
            "Epoch 252: val_loss did not improve from 77857.47636\n",
            "1524/1524 [==============================] - 0s 60us/sample - loss: 94683.3135 - mse: 0.3091 - val_loss: 78852.9460 - val_mse: 0.3099\n",
            "Epoch 253/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 94309.2390 - mse: 0.3079 \n",
            "Epoch 253: val_loss did not improve from 77857.47636\n",
            "1524/1524 [==============================] - 0s 65us/sample - loss: 94309.2390 - mse: 0.3079 - val_loss: 78960.4166 - val_mse: 0.3089\n",
            "Epoch 254/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 94500.5370 - mse: 0.3085\n",
            "Epoch 254: val_loss did not improve from 77857.47636\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 94500.5370 - mse: 0.3085 - val_loss: 78261.1933 - val_mse: 0.3091\n",
            "Epoch 255/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 94428.5693 - mse: 0.3081\n",
            "Epoch 255: val_loss did not improve from 77857.47636\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 94428.5693 - mse: 0.3081 - val_loss: 78847.4061 - val_mse: 0.3089\n",
            "Epoch 256/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 94063.8216 - mse: 0.3068 \n",
            "Epoch 256: val_loss did not improve from 77857.47636\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 94063.8216 - mse: 0.3068 - val_loss: 80450.1576 - val_mse: 0.3082\n",
            "Epoch 257/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 93741.2495 - mse: 0.3060 \n",
            "Epoch 257: val_loss did not improve from 77857.47636\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 93741.2495 - mse: 0.3060 - val_loss: 79181.9514 - val_mse: 0.3088\n",
            "Epoch 258/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 93955.1842 - mse: 0.3068 \n",
            "Epoch 258: val_loss did not improve from 77857.47636\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 93955.1842 - mse: 0.3068 - val_loss: 80069.9691 - val_mse: 0.3075\n",
            "Epoch 259/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 93891.1492 - mse: 0.3066\n",
            "Epoch 259: val_loss did not improve from 77857.47636\n",
            "1524/1524 [==============================] - 0s 60us/sample - loss: 93891.1492 - mse: 0.3066 - val_loss: 78107.5001 - val_mse: 0.3077\n",
            "Epoch 260/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 93520.7265 - mse: 0.3052\n",
            "Epoch 260: val_loss did not improve from 77857.47636\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 93520.7265 - mse: 0.3052 - val_loss: 79599.7406 - val_mse: 0.3072\n",
            "Epoch 261/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 93538.8175 - mse: 0.3053\n",
            "Epoch 261: val_loss improved from 77857.47636 to 77776.11725, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 132us/sample - loss: 93538.8175 - mse: 0.3053 - val_loss: 77776.1172 - val_mse: 0.3073\n",
            "Epoch 262/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 93190.7973 - mse: 0.3042\n",
            "Epoch 262: val_loss did not improve from 77776.11725\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 93190.7973 - mse: 0.3042 - val_loss: 79698.8883 - val_mse: 0.3054\n",
            "Epoch 263/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 93614.3766 - mse: 0.3033\n",
            "Epoch 263: val_loss did not improve from 77776.11725\n",
            "1524/1524 [==============================] - 0s 64us/sample - loss: 93071.6454 - mse: 0.3038 - val_loss: 79413.6519 - val_mse: 0.3041\n",
            "Epoch 264/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 93073.8798 - mse: 0.3037\n",
            "Epoch 264: val_loss did not improve from 77776.11725\n",
            "1524/1524 [==============================] - 0s 65us/sample - loss: 93073.8798 - mse: 0.3037 - val_loss: 79262.9366 - val_mse: 0.3048\n",
            "Epoch 265/500\n",
            " 256/1524 [====>.........................] - ETA: 0s - loss: 90271.6406 - mse: 0.2923\n",
            "Epoch 265: val_loss improved from 77776.11725 to 77680.33878, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 127us/sample - loss: 92967.4080 - mse: 0.3034 - val_loss: 77680.3388 - val_mse: 0.3054\n",
            "Epoch 266/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 94181.0234 - mse: 0.3051\n",
            "Epoch 266: val_loss did not improve from 77680.33878\n",
            "1524/1524 [==============================] - 0s 83us/sample - loss: 92814.9064 - mse: 0.3029 - val_loss: 78636.7402 - val_mse: 0.3046\n",
            "Epoch 267/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 94510.5469 - mse: 0.3062\n",
            "Epoch 267: val_loss improved from 77680.33878 to 76528.39011, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 172us/sample - loss: 92911.4733 - mse: 0.3031 - val_loss: 76528.3901 - val_mse: 0.3044\n",
            "Epoch 268/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 93341.7172 - mse: 0.3024\n",
            "Epoch 268: val_loss did not improve from 76528.39011\n",
            "1524/1524 [==============================] - 0s 72us/sample - loss: 92515.2280 - mse: 0.3019 - val_loss: 77384.7324 - val_mse: 0.3035\n",
            "Epoch 269/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 93693.4141 - mse: 0.3035\n",
            "Epoch 269: val_loss improved from 76528.39011 to 75996.88985, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 166us/sample - loss: 92441.7921 - mse: 0.3016 - val_loss: 75996.8898 - val_mse: 0.3036\n",
            "Epoch 270/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 92181.6201 - mse: 0.3009 \n",
            "Epoch 270: val_loss did not improve from 75996.88985\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 92181.6201 - mse: 0.3009 - val_loss: 78160.3324 - val_mse: 0.3037\n",
            "Epoch 271/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 92233.2141 - mse: 0.3009\n",
            "Epoch 271: val_loss improved from 75996.88985 to 75963.53125, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 161us/sample - loss: 92233.2141 - mse: 0.3009 - val_loss: 75963.5312 - val_mse: 0.3031\n",
            "Epoch 272/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 92387.7250 - mse: 0.2993 \n",
            "Epoch 272: val_loss did not improve from 75963.53125\n",
            "1524/1524 [==============================] - 0s 73us/sample - loss: 92389.7503 - mse: 0.3016 - val_loss: 77751.7560 - val_mse: 0.3016\n",
            "Epoch 273/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 94717.8188 - mse: 0.3069 \n",
            "Epoch 273: val_loss did not improve from 75963.53125\n",
            "1524/1524 [==============================] - 0s 67us/sample - loss: 92288.4660 - mse: 0.3010 - val_loss: 77821.2519 - val_mse: 0.3020\n",
            "Epoch 274/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 91918.0690 - mse: 0.3002\n",
            "Epoch 274: val_loss did not improve from 75963.53125\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 91918.0690 - mse: 0.3002 - val_loss: 77221.5904 - val_mse: 0.3008\n",
            "Epoch 275/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 91985.6160 - mse: 0.3002\n",
            "Epoch 275: val_loss did not improve from 75963.53125\n",
            "1524/1524 [==============================] - 0s 66us/sample - loss: 91985.6160 - mse: 0.3002 - val_loss: 78544.8268 - val_mse: 0.3013\n",
            "Epoch 276/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 91810.2048 - mse: 0.2997 \n",
            "Epoch 276: val_loss did not improve from 75963.53125\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 91810.2048 - mse: 0.2997 - val_loss: 76940.2783 - val_mse: 0.2999\n",
            "Epoch 277/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 92392.9812 - mse: 0.2993\n",
            "Epoch 277: val_loss did not improve from 75963.53125\n",
            "1524/1524 [==============================] - 0s 75us/sample - loss: 91856.2298 - mse: 0.2998 - val_loss: 78161.4793 - val_mse: 0.3030\n",
            "Epoch 278/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 91588.8444 - mse: 0.2990\n",
            "Epoch 278: val_loss did not improve from 75963.53125\n",
            "1524/1524 [==============================] - 0s 60us/sample - loss: 91588.8444 - mse: 0.2990 - val_loss: 76857.2097 - val_mse: 0.3012\n",
            "Epoch 279/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 91262.6432 - mse: 0.2977\n",
            "Epoch 279: val_loss did not improve from 75963.53125\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 91262.6432 - mse: 0.2977 - val_loss: 78053.7825 - val_mse: 0.3007\n",
            "Epoch 280/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 91401.1357 - mse: 0.2980\n",
            "Epoch 280: val_loss did not improve from 75963.53125\n",
            "1524/1524 [==============================] - 0s 60us/sample - loss: 91401.1357 - mse: 0.2980 - val_loss: 76463.0234 - val_mse: 0.2998\n",
            "Epoch 281/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 91299.6288 - mse: 0.2980\n",
            "Epoch 281: val_loss did not improve from 75963.53125\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 91299.6288 - mse: 0.2980 - val_loss: 78778.5460 - val_mse: 0.3005\n",
            "Epoch 282/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 91936.3734 - mse: 0.2978\n",
            "Epoch 282: val_loss did not improve from 75963.53125\n",
            "1524/1524 [==============================] - 0s 66us/sample - loss: 91493.0117 - mse: 0.2986 - val_loss: 77668.9734 - val_mse: 0.2989\n",
            "Epoch 283/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 91267.8069 - mse: 0.2980\n",
            "Epoch 283: val_loss improved from 75963.53125 to 75891.71423, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 150us/sample - loss: 91267.8069 - mse: 0.2980 - val_loss: 75891.7142 - val_mse: 0.3006\n",
            "Epoch 284/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 91324.5096 - mse: 0.2982\n",
            "Epoch 284: val_loss did not improve from 75891.71423\n",
            "1524/1524 [==============================] - 0s 73us/sample - loss: 91324.5096 - mse: 0.2982 - val_loss: 81522.8980 - val_mse: 0.3002\n",
            "Epoch 285/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 92544.1516 - mse: 0.2998\n",
            "Epoch 285: val_loss did not improve from 75891.71423\n",
            "1524/1524 [==============================] - 0s 65us/sample - loss: 91043.5108 - mse: 0.2970 - val_loss: 78123.6467 - val_mse: 0.2994\n",
            "Epoch 286/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 91113.6900 - mse: 0.2971\n",
            "Epoch 286: val_loss improved from 75891.71423 to 75745.27491, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 183us/sample - loss: 91113.6900 - mse: 0.2971 - val_loss: 75745.2749 - val_mse: 0.2988\n",
            "Epoch 287/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 90587.8736 - mse: 0.2961\n",
            "Epoch 287: val_loss did not improve from 75745.27491\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 90587.8736 - mse: 0.2961 - val_loss: 76386.3234 - val_mse: 0.2992\n",
            "Epoch 288/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 91514.8219 - mse: 0.2964\n",
            "Epoch 288: val_loss did not improve from 75745.27491\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 90571.9696 - mse: 0.2956 - val_loss: 77086.4739 - val_mse: 0.2973\n",
            "Epoch 289/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 90759.6119 - mse: 0.2962\n",
            "Epoch 289: val_loss did not improve from 75745.27491\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 90759.6119 - mse: 0.2962 - val_loss: 77823.6727 - val_mse: 0.2997\n",
            "Epoch 290/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 90770.8352 - mse: 0.2962\n",
            "Epoch 290: val_loss did not improve from 75745.27491\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 90770.8352 - mse: 0.2962 - val_loss: 77330.2070 - val_mse: 0.2976\n",
            "Epoch 291/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 90506.7328 - mse: 0.2957\n",
            "Epoch 291: val_loss did not improve from 75745.27491\n",
            "1524/1524 [==============================] - 0s 60us/sample - loss: 90506.7328 - mse: 0.2957 - val_loss: 77494.8057 - val_mse: 0.2987\n",
            "Epoch 292/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 90668.7780 - mse: 0.2960\n",
            "Epoch 292: val_loss did not improve from 75745.27491\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 90668.7780 - mse: 0.2960 - val_loss: 77356.2046 - val_mse: 0.2982\n",
            "Epoch 293/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 91423.2344 - mse: 0.2962\n",
            "Epoch 293: val_loss did not improve from 75745.27491\n",
            "1524/1524 [==============================] - 0s 77us/sample - loss: 90715.2701 - mse: 0.2961 - val_loss: 76264.8465 - val_mse: 0.2983\n",
            "Epoch 294/500\n",
            "1024/1524 [===================>..........] - ETA: 0s - loss: 89864.5469 - mse: 0.2911\n",
            "Epoch 294: val_loss did not improve from 75745.27491\n",
            "1524/1524 [==============================] - 0s 79us/sample - loss: 90209.3898 - mse: 0.2945 - val_loss: 77419.1760 - val_mse: 0.2968\n",
            "Epoch 295/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 89470.5266 - mse: 0.2898\n",
            "Epoch 295: val_loss did not improve from 75745.27491\n",
            "1524/1524 [==============================] - 0s 77us/sample - loss: 90211.6921 - mse: 0.2946 - val_loss: 76729.6465 - val_mse: 0.2974\n",
            "Epoch 296/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 90424.4281 - mse: 0.2929\n",
            "Epoch 296: val_loss did not improve from 75745.27491\n",
            "1524/1524 [==============================] - 0s 78us/sample - loss: 90127.3010 - mse: 0.2942 - val_loss: 78153.1289 - val_mse: 0.2977\n",
            "Epoch 297/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 90261.5760 - mse: 0.2946\n",
            "Epoch 297: val_loss did not improve from 75745.27491\n",
            "1524/1524 [==============================] - 0s 205us/sample - loss: 90261.5760 - mse: 0.2946 - val_loss: 76729.8190 - val_mse: 0.2962\n",
            "Epoch 298/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 90943.1625 - mse: 0.2946\n",
            "Epoch 298: val_loss did not improve from 75745.27491\n",
            "1524/1524 [==============================] - 0s 266us/sample - loss: 90134.9815 - mse: 0.2942 - val_loss: 76028.7414 - val_mse: 0.2980\n",
            "Epoch 299/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 91025.2500 - mse: 0.2949\n",
            "Epoch 299: val_loss did not improve from 75745.27491\n",
            "1524/1524 [==============================] - 0s 259us/sample - loss: 90198.3865 - mse: 0.2944 - val_loss: 75917.5159 - val_mse: 0.2974\n",
            "Epoch 300/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 91907.4484 - mse: 0.2978\n",
            "Epoch 300: val_loss did not improve from 75745.27491\n",
            "1524/1524 [==============================] - 0s 184us/sample - loss: 90092.6417 - mse: 0.2939 - val_loss: 76092.5870 - val_mse: 0.2971\n",
            "Epoch 301/500\n",
            "1024/1524 [===================>..........] - ETA: 0s - loss: 90764.6660 - mse: 0.2940\n",
            "Epoch 301: val_loss improved from 75745.27491 to 74888.68323, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 188us/sample - loss: 90225.9582 - mse: 0.2944 - val_loss: 74888.6832 - val_mse: 0.2975\n",
            "Epoch 302/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 89943.0969 - mse: 0.2934\n",
            "Epoch 302: val_loss did not improve from 74888.68323\n",
            "1524/1524 [==============================] - 0s 60us/sample - loss: 89943.0969 - mse: 0.2934 - val_loss: 75570.4266 - val_mse: 0.2955\n",
            "Epoch 303/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 89937.0278 - mse: 0.2936\n",
            "Epoch 303: val_loss did not improve from 74888.68323\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 89937.0278 - mse: 0.2936 - val_loss: 77045.5620 - val_mse: 0.2964\n",
            "Epoch 304/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 89787.2063 - mse: 0.2933\n",
            "Epoch 304: val_loss did not improve from 74888.68323\n",
            "1524/1524 [==============================] - 0s 67us/sample - loss: 89787.2063 - mse: 0.2933 - val_loss: 77186.3642 - val_mse: 0.2960\n",
            "Epoch 305/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 89203.4547 - mse: 0.2890\n",
            "Epoch 305: val_loss did not improve from 74888.68323\n",
            "1524/1524 [==============================] - 0s 80us/sample - loss: 89669.6601 - mse: 0.2928 - val_loss: 78587.5691 - val_mse: 0.2966\n",
            "Epoch 306/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 90383.7516 - mse: 0.2928\n",
            "Epoch 306: val_loss did not improve from 74888.68323\n",
            "1524/1524 [==============================] - 0s 66us/sample - loss: 89828.9825 - mse: 0.2932 - val_loss: 76343.0232 - val_mse: 0.2960\n",
            "Epoch 307/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 90430.8484 - mse: 0.2929\n",
            "Epoch 307: val_loss did not improve from 74888.68323\n",
            "1524/1524 [==============================] - 0s 65us/sample - loss: 89719.2235 - mse: 0.2928 - val_loss: 75789.6255 - val_mse: 0.2969\n",
            "Epoch 308/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 88178.7469 - mse: 0.2856\n",
            "Epoch 308: val_loss did not improve from 74888.68323\n",
            "1524/1524 [==============================] - 0s 68us/sample - loss: 89589.2711 - mse: 0.2927 - val_loss: 75972.0938 - val_mse: 0.2958\n",
            "Epoch 309/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 89466.2849 - mse: 0.2920\n",
            "Epoch 309: val_loss improved from 74888.68323 to 73014.23765, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 158us/sample - loss: 89466.2849 - mse: 0.2920 - val_loss: 73014.2376 - val_mse: 0.2956\n",
            "Epoch 310/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 89644.1677 - mse: 0.2925\n",
            "Epoch 310: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 89644.1677 - mse: 0.2925 - val_loss: 76392.2743 - val_mse: 0.2950\n",
            "Epoch 311/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 89677.2465 - mse: 0.2926\n",
            "Epoch 311: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 56us/sample - loss: 89677.2465 - mse: 0.2926 - val_loss: 73689.4629 - val_mse: 0.2958\n",
            "Epoch 312/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 89464.4860 - mse: 0.2921\n",
            "Epoch 312: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 89464.4860 - mse: 0.2921 - val_loss: 77279.6678 - val_mse: 0.2960\n",
            "Epoch 313/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 87580.2453 - mse: 0.2837\n",
            "Epoch 313: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 77us/sample - loss: 89221.9488 - mse: 0.2915 - val_loss: 76341.1741 - val_mse: 0.2957\n",
            "Epoch 314/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 90305.9422 - mse: 0.2925\n",
            "Epoch 314: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 68us/sample - loss: 89253.6739 - mse: 0.2913 - val_loss: 73256.3691 - val_mse: 0.2950\n",
            "Epoch 315/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 89240.3860 - mse: 0.2912\n",
            "Epoch 315: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 89240.3860 - mse: 0.2912 - val_loss: 75957.8952 - val_mse: 0.2956\n",
            "Epoch 316/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 89468.6691 - mse: 0.2917\n",
            "Epoch 316: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 60us/sample - loss: 89468.6691 - mse: 0.2917 - val_loss: 76331.9184 - val_mse: 0.2950\n",
            "Epoch 317/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 89327.4598 - mse: 0.2914\n",
            "Epoch 317: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 89327.4598 - mse: 0.2914 - val_loss: 75338.1818 - val_mse: 0.2955\n",
            "Epoch 318/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 89378.8113 - mse: 0.2916 \n",
            "Epoch 318: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 89378.8113 - mse: 0.2916 - val_loss: 75670.0125 - val_mse: 0.2961\n",
            "Epoch 319/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 89058.5789 - mse: 0.2909\n",
            "Epoch 319: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 89058.5789 - mse: 0.2909 - val_loss: 77944.5197 - val_mse: 0.2949\n",
            "Epoch 320/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 89237.4018 - mse: 0.2912\n",
            "Epoch 320: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 89237.4018 - mse: 0.2912 - val_loss: 75392.1858 - val_mse: 0.2951\n",
            "Epoch 321/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 89078.1464 - mse: 0.2910\n",
            "Epoch 321: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 89078.1464 - mse: 0.2910 - val_loss: 75777.8985 - val_mse: 0.2955\n",
            "Epoch 322/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 89031.9107 - mse: 0.2905\n",
            "Epoch 322: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 60us/sample - loss: 89031.9107 - mse: 0.2905 - val_loss: 75448.5755 - val_mse: 0.2951\n",
            "Epoch 323/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 89246.8641 - mse: 0.2891\n",
            "Epoch 323: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 80us/sample - loss: 89065.6758 - mse: 0.2908 - val_loss: 75601.4384 - val_mse: 0.2951\n",
            "Epoch 324/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 89060.9934 - mse: 0.2908\n",
            "Epoch 324: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 89060.9934 - mse: 0.2908 - val_loss: 77342.8798 - val_mse: 0.2952\n",
            "Epoch 325/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 89031.9264 - mse: 0.2906\n",
            "Epoch 325: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 89031.9264 - mse: 0.2906 - val_loss: 75048.0709 - val_mse: 0.2945\n",
            "Epoch 326/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88905.4291 - mse: 0.2902\n",
            "Epoch 326: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 88905.4291 - mse: 0.2902 - val_loss: 76183.4621 - val_mse: 0.2939\n",
            "Epoch 327/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88950.0229 - mse: 0.2903\n",
            "Epoch 327: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 88950.0229 - mse: 0.2903 - val_loss: 76413.3241 - val_mse: 0.2953\n",
            "Epoch 328/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 92206.3922 - mse: 0.2988 \n",
            "Epoch 328: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 65us/sample - loss: 89099.1482 - mse: 0.2905 - val_loss: 76334.3060 - val_mse: 0.2944\n",
            "Epoch 329/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88983.2645 - mse: 0.2905\n",
            "Epoch 329: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 88983.2645 - mse: 0.2905 - val_loss: 76311.9715 - val_mse: 0.2946\n",
            "Epoch 330/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88953.1334 - mse: 0.2903\n",
            "Epoch 330: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 88953.1334 - mse: 0.2903 - val_loss: 74225.6251 - val_mse: 0.2948\n",
            "Epoch 331/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88769.9982 - mse: 0.2899\n",
            "Epoch 331: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 88769.9982 - mse: 0.2899 - val_loss: 76621.2358 - val_mse: 0.2936\n",
            "Epoch 332/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88901.8568 - mse: 0.2901\n",
            "Epoch 332: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 88901.8568 - mse: 0.2901 - val_loss: 75489.7799 - val_mse: 0.2938\n",
            "Epoch 333/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 87545.6391 - mse: 0.2836\n",
            "Epoch 333: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 70us/sample - loss: 88700.7705 - mse: 0.2898 - val_loss: 76784.5307 - val_mse: 0.2939\n",
            "Epoch 334/500\n",
            "1024/1524 [===================>..........] - ETA: 0s - loss: 87190.7559 - mse: 0.2824\n",
            "Epoch 334: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 76us/sample - loss: 88606.3029 - mse: 0.2893 - val_loss: 74090.8861 - val_mse: 0.2941\n",
            "Epoch 335/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88813.3000 - mse: 0.2898\n",
            "Epoch 335: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 64us/sample - loss: 88813.3000 - mse: 0.2898 - val_loss: 77164.6647 - val_mse: 0.2951\n",
            "Epoch 336/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88922.4580 - mse: 0.2902\n",
            "Epoch 336: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 88922.4580 - mse: 0.2902 - val_loss: 76858.7460 - val_mse: 0.2943\n",
            "Epoch 337/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 89323.6016 - mse: 0.2894\n",
            "Epoch 337: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 69us/sample - loss: 88592.5147 - mse: 0.2892 - val_loss: 76100.4262 - val_mse: 0.2933\n",
            "Epoch 338/500\n",
            "1024/1524 [===================>..........] - ETA: 0s - loss: 87197.8867 - mse: 0.2825\n",
            "Epoch 338: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 80us/sample - loss: 88578.3066 - mse: 0.2894 - val_loss: 76055.7336 - val_mse: 0.2931\n",
            "Epoch 339/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 90452.6625 - mse: 0.2931\n",
            "Epoch 339: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 71us/sample - loss: 88698.1197 - mse: 0.2894 - val_loss: 75632.5935 - val_mse: 0.2938\n",
            "Epoch 340/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88604.2142 - mse: 0.2891\n",
            "Epoch 340: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 88604.2142 - mse: 0.2891 - val_loss: 75811.9605 - val_mse: 0.2933\n",
            "Epoch 341/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88465.9266 - mse: 0.2887\n",
            "Epoch 341: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 88465.9266 - mse: 0.2887 - val_loss: 74922.5554 - val_mse: 0.2933\n",
            "Epoch 342/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88570.9496 - mse: 0.2889\n",
            "Epoch 342: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 88570.9496 - mse: 0.2889 - val_loss: 75623.8833 - val_mse: 0.2929\n",
            "Epoch 343/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 88558.4875 - mse: 0.2869\n",
            "Epoch 343: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 65us/sample - loss: 88472.5907 - mse: 0.2889 - val_loss: 76199.6685 - val_mse: 0.2939\n",
            "Epoch 344/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 90217.0469 - mse: 0.2923\n",
            "Epoch 344: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 76us/sample - loss: 88444.0455 - mse: 0.2885 - val_loss: 74767.5577 - val_mse: 0.2929\n",
            "Epoch 345/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 90701.5562 - mse: 0.2939\n",
            "Epoch 345: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 75us/sample - loss: 88576.3862 - mse: 0.2889 - val_loss: 75478.7803 - val_mse: 0.2931\n",
            "Epoch 346/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88501.6872 - mse: 0.2888\n",
            "Epoch 346: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 65us/sample - loss: 88501.6872 - mse: 0.2888 - val_loss: 76825.9652 - val_mse: 0.2932\n",
            "Epoch 347/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88341.7655 - mse: 0.2883\n",
            "Epoch 347: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 88341.7655 - mse: 0.2883 - val_loss: 76339.1883 - val_mse: 0.2929\n",
            "Epoch 348/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88344.2130 - mse: 0.2883\n",
            "Epoch 348: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 88344.2130 - mse: 0.2883 - val_loss: 75441.7699 - val_mse: 0.2932\n",
            "Epoch 349/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88369.8397 - mse: 0.2885\n",
            "Epoch 349: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 88369.8397 - mse: 0.2885 - val_loss: 74702.8854 - val_mse: 0.2924\n",
            "Epoch 350/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88163.3233 - mse: 0.2880\n",
            "Epoch 350: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 88163.3233 - mse: 0.2880 - val_loss: 75531.8851 - val_mse: 0.2929\n",
            "Epoch 351/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88333.1221 - mse: 0.2883\n",
            "Epoch 351: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 88333.1221 - mse: 0.2883 - val_loss: 74016.6022 - val_mse: 0.2927\n",
            "Epoch 352/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88211.0109 - mse: 0.2879\n",
            "Epoch 352: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 88211.0109 - mse: 0.2879 - val_loss: 76691.5378 - val_mse: 0.2927\n",
            "Epoch 353/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88211.5212 - mse: 0.2879\n",
            "Epoch 353: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 88211.5212 - mse: 0.2879 - val_loss: 75687.8201 - val_mse: 0.2925\n",
            "Epoch 354/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88225.7690 - mse: 0.2880\n",
            "Epoch 354: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 88225.7690 - mse: 0.2880 - val_loss: 76538.7564 - val_mse: 0.2925\n",
            "Epoch 355/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88087.1953 - mse: 0.2877\n",
            "Epoch 355: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 88087.1953 - mse: 0.2877 - val_loss: 76905.1120 - val_mse: 0.2932\n",
            "Epoch 356/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 89251.6750 - mse: 0.2892\n",
            "Epoch 356: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 72us/sample - loss: 88205.6311 - mse: 0.2879 - val_loss: 74478.9394 - val_mse: 0.2920\n",
            "Epoch 357/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88277.8836 - mse: 0.2880\n",
            "Epoch 357: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 88277.8836 - mse: 0.2880 - val_loss: 75060.4399 - val_mse: 0.2935\n",
            "Epoch 358/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88166.0949 - mse: 0.2877\n",
            "Epoch 358: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 88166.0949 - mse: 0.2877 - val_loss: 75302.1513 - val_mse: 0.2924\n",
            "Epoch 359/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88086.3634 - mse: 0.2875\n",
            "Epoch 359: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 88086.3634 - mse: 0.2875 - val_loss: 76684.0550 - val_mse: 0.2928\n",
            "Epoch 360/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 90406.5766 - mse: 0.2929\n",
            "Epoch 360: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 88166.6380 - mse: 0.2876 - val_loss: 74480.2175 - val_mse: 0.2937\n",
            "Epoch 361/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88103.2630 - mse: 0.2877\n",
            "Epoch 361: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 65us/sample - loss: 88103.2630 - mse: 0.2877 - val_loss: 77126.3613 - val_mse: 0.2927\n",
            "Epoch 362/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 88383.4547 - mse: 0.2864\n",
            "Epoch 362: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 68us/sample - loss: 87961.5978 - mse: 0.2872 - val_loss: 76333.1919 - val_mse: 0.2924\n",
            "Epoch 363/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87890.9912 - mse: 0.2870\n",
            "Epoch 363: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 87890.9912 - mse: 0.2870 - val_loss: 75954.1940 - val_mse: 0.2917\n",
            "Epoch 364/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87896.5442 - mse: 0.2871\n",
            "Epoch 364: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 87896.5442 - mse: 0.2871 - val_loss: 74153.1223 - val_mse: 0.2928\n",
            "Epoch 365/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 86675.8672 - mse: 0.2808\n",
            "Epoch 365: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 72us/sample - loss: 87873.0810 - mse: 0.2871 - val_loss: 77175.9540 - val_mse: 0.2923\n",
            "Epoch 366/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 88078.2755 - mse: 0.2874\n",
            "Epoch 366: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 65us/sample - loss: 88078.2755 - mse: 0.2874 - val_loss: 75330.8054 - val_mse: 0.2921\n",
            "Epoch 367/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 89666.8766 - mse: 0.2905\n",
            "Epoch 367: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 67us/sample - loss: 87993.3527 - mse: 0.2871 - val_loss: 76213.4857 - val_mse: 0.2921\n",
            "Epoch 368/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87825.7187 - mse: 0.2869\n",
            "Epoch 368: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 87825.7187 - mse: 0.2869 - val_loss: 75594.2837 - val_mse: 0.2922\n",
            "Epoch 369/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87891.5674 - mse: 0.2867\n",
            "Epoch 369: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 87891.5674 - mse: 0.2867 - val_loss: 74920.4941 - val_mse: 0.2922\n",
            "Epoch 370/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87770.5409 - mse: 0.2866\n",
            "Epoch 370: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 87770.5409 - mse: 0.2866 - val_loss: 76512.2530 - val_mse: 0.2919\n",
            "Epoch 371/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87877.7749 - mse: 0.2869\n",
            "Epoch 371: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 87877.7749 - mse: 0.2869 - val_loss: 75343.1056 - val_mse: 0.2920\n",
            "Epoch 372/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87813.6440 - mse: 0.2865\n",
            "Epoch 372: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 87813.6440 - mse: 0.2865 - val_loss: 76507.2151 - val_mse: 0.2919\n",
            "Epoch 373/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87704.0280 - mse: 0.2865\n",
            "Epoch 373: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 87704.0280 - mse: 0.2865 - val_loss: 76530.1645 - val_mse: 0.2921\n",
            "Epoch 374/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87776.9281 - mse: 0.2866\n",
            "Epoch 374: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 87776.9281 - mse: 0.2866 - val_loss: 74331.6936 - val_mse: 0.2924\n",
            "Epoch 375/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87660.4798 - mse: 0.2865\n",
            "Epoch 375: val_loss did not improve from 73014.23765\n",
            "1524/1524 [==============================] - 0s 60us/sample - loss: 87660.4798 - mse: 0.2865 - val_loss: 73721.7313 - val_mse: 0.2926\n",
            "Epoch 376/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 87446.6656 - mse: 0.2833\n",
            "Epoch 376: val_loss improved from 73014.23765 to 71858.35686, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 147us/sample - loss: 87692.4741 - mse: 0.2864 - val_loss: 71858.3569 - val_mse: 0.2918\n",
            "Epoch 377/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87728.0075 - mse: 0.2865\n",
            "Epoch 377: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 69us/sample - loss: 87728.0075 - mse: 0.2865 - val_loss: 74473.1026 - val_mse: 0.2915\n",
            "Epoch 378/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87678.3978 - mse: 0.2863\n",
            "Epoch 378: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 87678.3978 - mse: 0.2863 - val_loss: 74765.1522 - val_mse: 0.2920\n",
            "Epoch 379/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87670.8731 - mse: 0.2863\n",
            "Epoch 379: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 87670.8731 - mse: 0.2863 - val_loss: 74158.8136 - val_mse: 0.2915\n",
            "Epoch 380/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87622.5884 - mse: 0.2864\n",
            "Epoch 380: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 87622.5884 - mse: 0.2864 - val_loss: 73348.0410 - val_mse: 0.2916\n",
            "Epoch 381/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87651.1688 - mse: 0.2861\n",
            "Epoch 381: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 87651.1688 - mse: 0.2861 - val_loss: 73862.6179 - val_mse: 0.2914\n",
            "Epoch 382/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 88040.8875 - mse: 0.2853\n",
            "Epoch 382: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 66us/sample - loss: 87541.6807 - mse: 0.2858 - val_loss: 74993.5025 - val_mse: 0.2920\n",
            "Epoch 383/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87513.3298 - mse: 0.2859\n",
            "Epoch 383: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 65us/sample - loss: 87513.3298 - mse: 0.2859 - val_loss: 75482.2966 - val_mse: 0.2920\n",
            "Epoch 384/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87534.0933 - mse: 0.2858\n",
            "Epoch 384: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 87534.0933 - mse: 0.2858 - val_loss: 72859.8850 - val_mse: 0.2915\n",
            "Epoch 385/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 89048.8750 - mse: 0.2886\n",
            "Epoch 385: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 68us/sample - loss: 87511.5932 - mse: 0.2856 - val_loss: 74379.9754 - val_mse: 0.2917\n",
            "Epoch 386/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87508.0919 - mse: 0.2858\n",
            "Epoch 386: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 87508.0919 - mse: 0.2858 - val_loss: 73062.8964 - val_mse: 0.2914\n",
            "Epoch 387/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 87917.5500 - mse: 0.2849\n",
            "Epoch 387: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 87516.0550 - mse: 0.2857 - val_loss: 75064.8228 - val_mse: 0.2913\n",
            "Epoch 388/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87508.7349 - mse: 0.2857\n",
            "Epoch 388: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 87508.7349 - mse: 0.2857 - val_loss: 74161.8933 - val_mse: 0.2917\n",
            "Epoch 389/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87544.1525 - mse: 0.2857\n",
            "Epoch 389: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 87544.1525 - mse: 0.2857 - val_loss: 75572.0447 - val_mse: 0.2911\n",
            "Epoch 390/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87554.6530 - mse: 0.2857\n",
            "Epoch 390: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 87554.6530 - mse: 0.2857 - val_loss: 73623.6716 - val_mse: 0.2918\n",
            "Epoch 391/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 88499.5594 - mse: 0.2868\n",
            "Epoch 391: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 66us/sample - loss: 87551.0854 - mse: 0.2858 - val_loss: 77858.9424 - val_mse: 0.2914\n",
            "Epoch 392/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87464.6676 - mse: 0.2854\n",
            "Epoch 392: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 87464.6676 - mse: 0.2854 - val_loss: 72747.2397 - val_mse: 0.2910\n",
            "Epoch 393/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87503.2682 - mse: 0.2855\n",
            "Epoch 393: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 67us/sample - loss: 87503.2682 - mse: 0.2855 - val_loss: 74116.6285 - val_mse: 0.2910\n",
            "Epoch 394/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 88655.8484 - mse: 0.2873\n",
            "Epoch 394: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 66us/sample - loss: 87466.8643 - mse: 0.2855 - val_loss: 75328.7053 - val_mse: 0.2910\n",
            "Epoch 395/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 88934.2078 - mse: 0.2882\n",
            "Epoch 395: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 72us/sample - loss: 87430.2158 - mse: 0.2853 - val_loss: 75440.3413 - val_mse: 0.2914\n",
            "Epoch 396/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87273.7649 - mse: 0.2851\n",
            "Epoch 396: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 87273.7649 - mse: 0.2851 - val_loss: 76207.9624 - val_mse: 0.2915\n",
            "Epoch 397/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87360.7695 - mse: 0.2850\n",
            "Epoch 397: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 87360.7695 - mse: 0.2850 - val_loss: 76150.6337 - val_mse: 0.2909\n",
            "Epoch 398/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87223.0140 - mse: 0.2850\n",
            "Epoch 398: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 87223.0140 - mse: 0.2850 - val_loss: 75759.9133 - val_mse: 0.2911\n",
            "Epoch 399/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 87748.2422 - mse: 0.2843\n",
            "Epoch 399: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 65us/sample - loss: 87286.8034 - mse: 0.2850 - val_loss: 76939.9706 - val_mse: 0.2910\n",
            "Epoch 400/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87260.2593 - mse: 0.2849\n",
            "Epoch 400: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 87260.2593 - mse: 0.2849 - val_loss: 75538.4517 - val_mse: 0.2908\n",
            "Epoch 401/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 87802.8500 - mse: 0.2845\n",
            "Epoch 401: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 87225.9895 - mse: 0.2848 - val_loss: 75677.4357 - val_mse: 0.2905\n",
            "Epoch 402/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87259.3157 - mse: 0.2849\n",
            "Epoch 402: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 87259.3157 - mse: 0.2849 - val_loss: 74261.1217 - val_mse: 0.2914\n",
            "Epoch 403/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87228.2615 - mse: 0.2849\n",
            "Epoch 403: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 87228.2615 - mse: 0.2849 - val_loss: 74344.8264 - val_mse: 0.2917\n",
            "Epoch 404/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87246.7639 - mse: 0.2848\n",
            "Epoch 404: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 87246.7639 - mse: 0.2848 - val_loss: 73434.0286 - val_mse: 0.2913\n",
            "Epoch 405/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 87003.2766 - mse: 0.2819\n",
            "Epoch 405: val_loss did not improve from 71858.35686\n",
            "1524/1524 [==============================] - 0s 72us/sample - loss: 87113.8135 - mse: 0.2845 - val_loss: 75277.1364 - val_mse: 0.2906\n",
            "Epoch 406/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87178.5119 - mse: 0.2847\n",
            "Epoch 406: val_loss improved from 71858.35686 to 71732.10457, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 144us/sample - loss: 87178.5119 - mse: 0.2847 - val_loss: 71732.1046 - val_mse: 0.2910\n",
            "Epoch 407/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87221.6248 - mse: 0.2846\n",
            "Epoch 407: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 66us/sample - loss: 87221.6248 - mse: 0.2846 - val_loss: 74585.9083 - val_mse: 0.2906\n",
            "Epoch 408/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87101.1157 - mse: 0.2846\n",
            "Epoch 408: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 64us/sample - loss: 87101.1157 - mse: 0.2846 - val_loss: 74282.0989 - val_mse: 0.2909\n",
            "Epoch 409/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87147.5719 - mse: 0.2844\n",
            "Epoch 409: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 65us/sample - loss: 87147.5719 - mse: 0.2844 - val_loss: 75509.7778 - val_mse: 0.2909\n",
            "Epoch 410/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87137.2241 - mse: 0.2843\n",
            "Epoch 410: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 87137.2241 - mse: 0.2843 - val_loss: 74521.1322 - val_mse: 0.2907\n",
            "Epoch 411/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87080.9109 - mse: 0.2844\n",
            "Epoch 411: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 87080.9109 - mse: 0.2844 - val_loss: 75453.4132 - val_mse: 0.2917\n",
            "Epoch 412/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87068.4826 - mse: 0.2843\n",
            "Epoch 412: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 56us/sample - loss: 87068.4826 - mse: 0.2843 - val_loss: 74992.2295 - val_mse: 0.2909\n",
            "Epoch 413/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87070.9060 - mse: 0.2844\n",
            "Epoch 413: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 66us/sample - loss: 87070.9060 - mse: 0.2844 - val_loss: 74435.7471 - val_mse: 0.2905\n",
            "Epoch 414/500\n",
            "1024/1524 [===================>..........] - ETA: 0s - loss: 85478.5430 - mse: 0.2770\n",
            "Epoch 414: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 72us/sample - loss: 87172.3055 - mse: 0.2847 - val_loss: 74816.8993 - val_mse: 0.2914\n",
            "Epoch 415/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87073.5720 - mse: 0.2843\n",
            "Epoch 415: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 87073.5720 - mse: 0.2843 - val_loss: 74736.2858 - val_mse: 0.2907\n",
            "Epoch 416/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86959.8545 - mse: 0.2840\n",
            "Epoch 416: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 86959.8545 - mse: 0.2840 - val_loss: 74349.5674 - val_mse: 0.2904\n",
            "Epoch 417/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86995.7561 - mse: 0.2839\n",
            "Epoch 417: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 60us/sample - loss: 86995.7561 - mse: 0.2839 - val_loss: 76707.8423 - val_mse: 0.2910\n",
            "Epoch 418/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 87056.0802 - mse: 0.2839\n",
            "Epoch 418: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 87056.0802 - mse: 0.2839 - val_loss: 77287.6778 - val_mse: 0.2904\n",
            "Epoch 419/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86995.6426 - mse: 0.2838\n",
            "Epoch 419: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 86995.6426 - mse: 0.2838 - val_loss: 75598.7958 - val_mse: 0.2902\n",
            "Epoch 420/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86930.9203 - mse: 0.2839\n",
            "Epoch 420: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 86930.9203 - mse: 0.2839 - val_loss: 73999.7266 - val_mse: 0.2907\n",
            "Epoch 421/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86925.8907 - mse: 0.2837\n",
            "Epoch 421: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 86925.8907 - mse: 0.2837 - val_loss: 72698.9759 - val_mse: 0.2905\n",
            "Epoch 422/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86881.2940 - mse: 0.2837\n",
            "Epoch 422: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 60us/sample - loss: 86881.2940 - mse: 0.2837 - val_loss: 74701.6097 - val_mse: 0.2912\n",
            "Epoch 423/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86842.4408 - mse: 0.2835\n",
            "Epoch 423: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 60us/sample - loss: 86842.4408 - mse: 0.2835 - val_loss: 75649.7309 - val_mse: 0.2905\n",
            "Epoch 424/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86866.8364 - mse: 0.2835\n",
            "Epoch 424: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 86866.8364 - mse: 0.2835 - val_loss: 74148.0425 - val_mse: 0.2905\n",
            "Epoch 425/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 86974.4781 - mse: 0.2818\n",
            "Epoch 425: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 71us/sample - loss: 86864.9598 - mse: 0.2837 - val_loss: 75001.3775 - val_mse: 0.2910\n",
            "Epoch 426/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86928.2427 - mse: 0.2837\n",
            "Epoch 426: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 86928.2427 - mse: 0.2837 - val_loss: 74504.3469 - val_mse: 0.2906\n",
            "Epoch 427/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86834.5895 - mse: 0.2835\n",
            "Epoch 427: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 56us/sample - loss: 86834.5895 - mse: 0.2835 - val_loss: 75144.3859 - val_mse: 0.2903\n",
            "Epoch 428/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86792.9282 - mse: 0.2834\n",
            "Epoch 428: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 86792.9282 - mse: 0.2834 - val_loss: 74595.0606 - val_mse: 0.2902\n",
            "Epoch 429/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 87112.6812 - mse: 0.2823\n",
            "Epoch 429: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 67us/sample - loss: 86803.2856 - mse: 0.2834 - val_loss: 75390.1896 - val_mse: 0.2903\n",
            "Epoch 430/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86747.6734 - mse: 0.2834\n",
            "Epoch 430: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 86747.6734 - mse: 0.2834 - val_loss: 73590.1116 - val_mse: 0.2901\n",
            "Epoch 431/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86819.5508 - mse: 0.2834\n",
            "Epoch 431: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 60us/sample - loss: 86819.5508 - mse: 0.2834 - val_loss: 74236.7105 - val_mse: 0.2905\n",
            "Epoch 432/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86842.3817 - mse: 0.2834\n",
            "Epoch 432: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 86842.3817 - mse: 0.2834 - val_loss: 72663.8769 - val_mse: 0.2900\n",
            "Epoch 433/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86804.0238 - mse: 0.2833\n",
            "Epoch 433: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 86804.0238 - mse: 0.2833 - val_loss: 73993.5787 - val_mse: 0.2903\n",
            "Epoch 434/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 86704.2125 - mse: 0.2810\n",
            "Epoch 434: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 69us/sample - loss: 86707.7871 - mse: 0.2832 - val_loss: 73647.5219 - val_mse: 0.2902\n",
            "Epoch 435/500\n",
            "1024/1524 [===================>..........] - ETA: 0s - loss: 89139.1250 - mse: 0.2889\n",
            "Epoch 435: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 70us/sample - loss: 86729.6153 - mse: 0.2831 - val_loss: 75544.9804 - val_mse: 0.2901\n",
            "Epoch 436/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86759.8342 - mse: 0.2831\n",
            "Epoch 436: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 59us/sample - loss: 86759.8342 - mse: 0.2831 - val_loss: 73250.9465 - val_mse: 0.2904\n",
            "Epoch 437/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86690.5521 - mse: 0.2830\n",
            "Epoch 437: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 86690.5521 - mse: 0.2830 - val_loss: 74692.7575 - val_mse: 0.2901\n",
            "Epoch 438/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86682.5617 - mse: 0.2830\n",
            "Epoch 438: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 58us/sample - loss: 86682.5617 - mse: 0.2830 - val_loss: 75442.3938 - val_mse: 0.2902\n",
            "Epoch 439/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86656.0509 - mse: 0.2829\n",
            "Epoch 439: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 64us/sample - loss: 86656.0509 - mse: 0.2829 - val_loss: 74339.5808 - val_mse: 0.2901\n",
            "Epoch 440/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86580.2981 - mse: 0.2828\n",
            "Epoch 440: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 65us/sample - loss: 86580.2981 - mse: 0.2828 - val_loss: 76285.8310 - val_mse: 0.2903\n",
            "Epoch 441/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86652.4218 - mse: 0.2828\n",
            "Epoch 441: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 63us/sample - loss: 86652.4218 - mse: 0.2828 - val_loss: 74777.7700 - val_mse: 0.2902\n",
            "Epoch 442/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86532.8594 - mse: 0.2828\n",
            "Epoch 442: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 56us/sample - loss: 86532.8594 - mse: 0.2828 - val_loss: 74785.7437 - val_mse: 0.2903\n",
            "Epoch 443/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 89272.2844 - mse: 0.2893\n",
            "Epoch 443: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 75us/sample - loss: 86690.8424 - mse: 0.2828 - val_loss: 73064.4138 - val_mse: 0.2902\n",
            "Epoch 444/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 87534.6484 - mse: 0.2837\n",
            "Epoch 444: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 74us/sample - loss: 86625.8575 - mse: 0.2828 - val_loss: 74548.8352 - val_mse: 0.2901\n",
            "Epoch 445/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 86379.9859 - mse: 0.2799\n",
            "Epoch 445: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 72us/sample - loss: 86513.6696 - mse: 0.2826 - val_loss: 74889.7535 - val_mse: 0.2900\n",
            "Epoch 446/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86547.7886 - mse: 0.2825\n",
            "Epoch 446: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 57us/sample - loss: 86547.7886 - mse: 0.2825 - val_loss: 75259.8930 - val_mse: 0.2901\n",
            "Epoch 447/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86582.0531 - mse: 0.2826\n",
            "Epoch 447: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 86582.0531 - mse: 0.2826 - val_loss: 73865.3983 - val_mse: 0.2899\n",
            "Epoch 448/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86604.3868 - mse: 0.2827\n",
            "Epoch 448: val_loss did not improve from 71732.10457\n",
            "1524/1524 [==============================] - 0s 64us/sample - loss: 86604.3868 - mse: 0.2827 - val_loss: 72797.5713 - val_mse: 0.2896\n",
            "Epoch 449/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 85995.3781 - mse: 0.2787\n",
            "Epoch 449: val_loss improved from 71732.10457 to 71289.40764, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 166us/sample - loss: 86455.6782 - mse: 0.2824 - val_loss: 71289.4076 - val_mse: 0.2903\n",
            "Epoch 450/500\n",
            "1024/1524 [===================>..........] - ETA: 0s - loss: 88702.8516 - mse: 0.2875\n",
            "Epoch 450: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 76us/sample - loss: 86451.1971 - mse: 0.2823 - val_loss: 74075.4152 - val_mse: 0.2900\n",
            "Epoch 451/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 86278.5328 - mse: 0.2796\n",
            "Epoch 451: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 65us/sample - loss: 86457.0446 - mse: 0.2824 - val_loss: 74247.9110 - val_mse: 0.2898\n",
            "Epoch 452/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 88206.3359 - mse: 0.2859\n",
            "Epoch 452: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 65us/sample - loss: 86513.0306 - mse: 0.2823 - val_loss: 75201.7204 - val_mse: 0.2896\n",
            "Epoch 453/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86402.4394 - mse: 0.2822\n",
            "Epoch 453: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 62us/sample - loss: 86402.4394 - mse: 0.2822 - val_loss: 75461.0849 - val_mse: 0.2899\n",
            "Epoch 454/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 86739.1062 - mse: 0.2811\n",
            "Epoch 454: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 76us/sample - loss: 86450.3762 - mse: 0.2823 - val_loss: 75099.5519 - val_mse: 0.2897\n",
            "Epoch 455/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 85819.2031 - mse: 0.2781\n",
            "Epoch 455: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 66us/sample - loss: 86419.9558 - mse: 0.2823 - val_loss: 73044.7536 - val_mse: 0.2901\n",
            "Epoch 456/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 87367.9094 - mse: 0.2831\n",
            "Epoch 456: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 81us/sample - loss: 86409.4942 - mse: 0.2821 - val_loss: 74832.7237 - val_mse: 0.2897\n",
            "Epoch 457/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86371.0247 - mse: 0.2822\n",
            "Epoch 457: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 66us/sample - loss: 86371.0247 - mse: 0.2822 - val_loss: 73777.2455 - val_mse: 0.2898\n",
            "Epoch 458/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 86400.5484 - mse: 0.2800\n",
            "Epoch 458: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 66us/sample - loss: 86314.0039 - mse: 0.2819 - val_loss: 75264.2954 - val_mse: 0.2896\n",
            "Epoch 459/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 87212.1703 - mse: 0.2826\n",
            "Epoch 459: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 66us/sample - loss: 86370.1997 - mse: 0.2820 - val_loss: 72543.4628 - val_mse: 0.2899\n",
            "Epoch 460/500\n",
            "1024/1524 [===================>..........] - ETA: 0s - loss: 89667.0957 - mse: 0.2906\n",
            "Epoch 460: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 80us/sample - loss: 86402.5390 - mse: 0.2820 - val_loss: 75150.4944 - val_mse: 0.2897\n",
            "Epoch 461/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 85841.5531 - mse: 0.2782\n",
            "Epoch 461: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 79us/sample - loss: 86231.2789 - mse: 0.2817 - val_loss: 74496.1896 - val_mse: 0.2894\n",
            "Epoch 462/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 86765.7172 - mse: 0.2812\n",
            "Epoch 462: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 78us/sample - loss: 86249.6916 - mse: 0.2816 - val_loss: 75352.2285 - val_mse: 0.2896\n",
            "Epoch 463/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 83899.0156 - mse: 0.2719\n",
            "Epoch 463: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 71us/sample - loss: 86163.7757 - mse: 0.2817 - val_loss: 72968.9181 - val_mse: 0.2898\n",
            "Epoch 464/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86366.1070 - mse: 0.2819\n",
            "Epoch 464: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 72us/sample - loss: 86366.1070 - mse: 0.2819 - val_loss: 74621.2223 - val_mse: 0.2897\n",
            "Epoch 465/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 85536.9031 - mse: 0.2772\n",
            "Epoch 465: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 72us/sample - loss: 86218.7845 - mse: 0.2817 - val_loss: 71316.5221 - val_mse: 0.2894\n",
            "Epoch 466/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 85607.7016 - mse: 0.2774\n",
            "Epoch 466: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 84us/sample - loss: 86248.9198 - mse: 0.2818 - val_loss: 76161.4242 - val_mse: 0.2893\n",
            "Epoch 467/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 86346.8047 - mse: 0.2798\n",
            "Epoch 467: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 74us/sample - loss: 86170.0450 - mse: 0.2814 - val_loss: 73145.3815 - val_mse: 0.2893\n",
            "Epoch 468/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 86876.9672 - mse: 0.2815\n",
            "Epoch 468: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 74us/sample - loss: 86264.2957 - mse: 0.2817 - val_loss: 73232.3693 - val_mse: 0.2896\n",
            "Epoch 469/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86278.6718 - mse: 0.2817\n",
            "Epoch 469: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 60us/sample - loss: 86278.6718 - mse: 0.2817 - val_loss: 73262.4288 - val_mse: 0.2898\n",
            "Epoch 470/500\n",
            "1024/1524 [===================>..........] - ETA: 0s - loss: 89730.8320 - mse: 0.2908\n",
            "Epoch 470: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 83us/sample - loss: 86211.2606 - mse: 0.2813 - val_loss: 74279.6943 - val_mse: 0.2893\n",
            "Epoch 471/500\n",
            "1024/1524 [===================>..........] - ETA: 0s - loss: 87642.6738 - mse: 0.2840\n",
            "Epoch 471: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 79us/sample - loss: 86216.2945 - mse: 0.2814 - val_loss: 73749.0101 - val_mse: 0.2894\n",
            "Epoch 472/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 85347.3906 - mse: 0.2765\n",
            "Epoch 472: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 68us/sample - loss: 86145.9268 - mse: 0.2815 - val_loss: 71298.1234 - val_mse: 0.2897\n",
            "Epoch 473/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 87327.1313 - mse: 0.2830\n",
            "Epoch 473: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 74us/sample - loss: 86179.6153 - mse: 0.2813 - val_loss: 75698.6148 - val_mse: 0.2893\n",
            "Epoch 474/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 88586.3094 - mse: 0.2871\n",
            "Epoch 474: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 78us/sample - loss: 86194.8042 - mse: 0.2812 - val_loss: 75827.2092 - val_mse: 0.2895\n",
            "Epoch 475/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 86012.0030 - mse: 0.2810\n",
            "Epoch 475: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 72us/sample - loss: 86012.0030 - mse: 0.2810 - val_loss: 74906.2138 - val_mse: 0.2891\n",
            "Epoch 476/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 87004.1672 - mse: 0.2819\n",
            "Epoch 476: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 71us/sample - loss: 86112.0742 - mse: 0.2811 - val_loss: 74522.8609 - val_mse: 0.2895\n",
            "Epoch 477/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 86763.9281 - mse: 0.2812\n",
            "Epoch 477: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 75us/sample - loss: 86013.0372 - mse: 0.2808 - val_loss: 74921.0805 - val_mse: 0.2896\n",
            "Epoch 478/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 87716.9344 - mse: 0.2843\n",
            "Epoch 478: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 74us/sample - loss: 86063.0591 - mse: 0.2809 - val_loss: 73917.2127 - val_mse: 0.2895\n",
            "Epoch 479/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 87570.0953 - mse: 0.2838\n",
            "Epoch 479: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 69us/sample - loss: 86047.0789 - mse: 0.2808 - val_loss: 72976.7832 - val_mse: 0.2891\n",
            "Epoch 480/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 87812.8016 - mse: 0.2846\n",
            "Epoch 480: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 84us/sample - loss: 86044.5976 - mse: 0.2808 - val_loss: 76088.3331 - val_mse: 0.2895\n",
            "Epoch 481/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 88009.2016 - mse: 0.2852\n",
            "Epoch 481: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 76us/sample - loss: 85997.8491 - mse: 0.2806 - val_loss: 73637.2325 - val_mse: 0.2896\n",
            "Epoch 482/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 86305.9234 - mse: 0.2797\n",
            "Epoch 482: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 74us/sample - loss: 85936.3571 - mse: 0.2806 - val_loss: 74473.4660 - val_mse: 0.2895\n",
            "Epoch 483/500\n",
            "1024/1524 [===================>..........] - ETA: 0s - loss: 86070.5117 - mse: 0.2789\n",
            "Epoch 483: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 92us/sample - loss: 85917.8601 - mse: 0.2806 - val_loss: 75622.2974 - val_mse: 0.2895\n",
            "Epoch 484/500\n",
            "1024/1524 [===================>..........] - ETA: 0s - loss: 87290.7949 - mse: 0.2829\n",
            "Epoch 484: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 82us/sample - loss: 85951.9150 - mse: 0.2807 - val_loss: 76101.9015 - val_mse: 0.2890\n",
            "Epoch 485/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 87112.9531 - mse: 0.2823\n",
            "Epoch 485: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 77us/sample - loss: 85956.8688 - mse: 0.2806 - val_loss: 75076.8489 - val_mse: 0.2891\n",
            "Epoch 486/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 86022.9422 - mse: 0.2788\n",
            "Epoch 486: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 75us/sample - loss: 85879.6287 - mse: 0.2805 - val_loss: 74802.3029 - val_mse: 0.2891\n",
            "Epoch 487/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 84890.2125 - mse: 0.2751\n",
            "Epoch 487: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 72us/sample - loss: 85822.4998 - mse: 0.2804 - val_loss: 76589.1239 - val_mse: 0.2893\n",
            "Epoch 488/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 85899.5667 - mse: 0.2803\n",
            "Epoch 488: val_loss did not improve from 71289.40764\n",
            "1524/1524 [==============================] - 0s 61us/sample - loss: 85899.5667 - mse: 0.2803 - val_loss: 72887.8347 - val_mse: 0.2892\n",
            "Epoch 489/500\n",
            "1524/1524 [==============================] - ETA: 0s - loss: 85896.2235 - mse: 0.2803\n",
            "Epoch 489: val_loss improved from 71289.40764 to 70509.31757, saving model to a.h5\n",
            "1524/1524 [==============================] - 0s 174us/sample - loss: 85896.2235 - mse: 0.2803 - val_loss: 70509.3176 - val_mse: 0.2892\n",
            "Epoch 490/500\n",
            "1024/1524 [===================>..........] - ETA: 0s - loss: 86372.5762 - mse: 0.2799\n",
            "Epoch 490: val_loss did not improve from 70509.31757\n",
            "1524/1524 [==============================] - 0s 96us/sample - loss: 85817.8164 - mse: 0.2803 - val_loss: 75118.9015 - val_mse: 0.2890\n",
            "Epoch 491/500\n",
            "1024/1524 [===================>..........] - ETA: 0s - loss: 85923.8398 - mse: 0.2784\n",
            "Epoch 491: val_loss did not improve from 70509.31757\n",
            "1524/1524 [==============================] - 0s 88us/sample - loss: 85747.9972 - mse: 0.2802 - val_loss: 75168.4576 - val_mse: 0.2890\n",
            "Epoch 492/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 86920.0172 - mse: 0.2817\n",
            "Epoch 492: val_loss did not improve from 70509.31757\n",
            "1524/1524 [==============================] - 0s 73us/sample - loss: 85831.3280 - mse: 0.2802 - val_loss: 71539.9618 - val_mse: 0.2890\n",
            "Epoch 493/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 86941.2250 - mse: 0.2817\n",
            "Epoch 493: val_loss did not improve from 70509.31757\n",
            "1524/1524 [==============================] - 0s 75us/sample - loss: 85783.1496 - mse: 0.2800 - val_loss: 74064.5111 - val_mse: 0.2889\n",
            "Epoch 494/500\n",
            "1024/1524 [===================>..........] - ETA: 0s - loss: 83981.8770 - mse: 0.2721\n",
            "Epoch 494: val_loss did not improve from 70509.31757\n",
            "1524/1524 [==============================] - 0s 86us/sample - loss: 85682.4095 - mse: 0.2799 - val_loss: 75767.1638 - val_mse: 0.2890\n",
            "Epoch 495/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 86419.1438 - mse: 0.2801\n",
            "Epoch 495: val_loss did not improve from 70509.31757\n",
            "1524/1524 [==============================] - 0s 83us/sample - loss: 85760.5564 - mse: 0.2800 - val_loss: 73851.1055 - val_mse: 0.2891\n",
            "Epoch 496/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 87280.2281 - mse: 0.2828\n",
            "Epoch 496: val_loss did not improve from 70509.31757\n",
            "1524/1524 [==============================] - 0s 75us/sample - loss: 85792.2797 - mse: 0.2800 - val_loss: 74463.2193 - val_mse: 0.2889\n",
            "Epoch 497/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 85302.7984 - mse: 0.2764\n",
            "Epoch 497: val_loss did not improve from 70509.31757\n",
            "1524/1524 [==============================] - 0s 79us/sample - loss: 85625.5723 - mse: 0.2797 - val_loss: 74481.6786 - val_mse: 0.2888\n",
            "Epoch 498/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 86222.8500 - mse: 0.2794\n",
            "Epoch 498: val_loss did not improve from 70509.31757\n",
            "1524/1524 [==============================] - 0s 76us/sample - loss: 85638.0005 - mse: 0.2796 - val_loss: 73971.7222 - val_mse: 0.2889\n",
            "Epoch 499/500\n",
            "1024/1524 [===================>..........] - ETA: 0s - loss: 83002.1211 - mse: 0.2689\n",
            "Epoch 499: val_loss did not improve from 70509.31757\n",
            "1524/1524 [==============================] - 0s 79us/sample - loss: 85636.2493 - mse: 0.2797 - val_loss: 74731.9787 - val_mse: 0.2890\n",
            "Epoch 500/500\n",
            "1280/1524 [========================>.....] - ETA: 0s - loss: 85520.1484 - mse: 0.2771\n",
            "Epoch 500: val_loss did not improve from 70509.31757\n",
            "1524/1524 [==============================] - 0s 79us/sample - loss: 85610.8479 - mse: 0.2796 - val_loss: 74768.8162 - val_mse: 0.2888\n"
          ]
        }
      ],
      "source": [
        "# train variational autoencoder\n",
        "results = vae.fit(X_train.iloc[:,0:1201],X_train.iloc[:,0:1201],shuffle=True,batch_size = batch_size, epochs = 500,validation_data = (X_test.iloc[:,0:1201],X_test.iloc[:,0:1201]),callbacks = callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU_0Xo4p87_R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "50fa0cd2-8c77-4905-d586-9ba91ee37e7c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# QC training and validation curves (should follow eachother)\n",
        "plt.figure(figsize=(8,2))\n",
        "plt.plot(results.history['val_loss'], label='val')\n",
        "plt.plot(results.history['loss'], label='train')\n",
        "plt.xlabel('epoch index')\n",
        "plt.ylabel('loss value (MSE)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm26P9hp8_1_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a34903e6-4419-4b1d-f7fb-c88c5a30023e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.93610394,  1.3399373 ,  1.1695819 , ..., -0.14844003,\n",
              "         0.07946431, -0.5688245 ],\n",
              "       [ 0.04553369,  0.45528147,  0.23200966, ..., -0.45757097,\n",
              "        -0.34184048, -0.81085616],\n",
              "       [ 0.2630911 ,  0.25553185, -0.7987524 , ..., -0.71497476,\n",
              "        -0.55691695, -0.5798065 ],\n",
              "       ...,\n",
              "       [-0.26683795, -0.48341045, -1.1167164 , ..., -0.2747348 ,\n",
              "        -0.4691929 ,  0.04962463],\n",
              "       [-0.10152639, -0.03181163, -0.6409054 , ...,  0.02336227,\n",
              "         0.06058809,  0.22071745],\n",
              "       [-0.53841263,  0.23503083, -0.15776455, ...,  0.48539123,\n",
              "         0.35471508, -0.27087802]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ],
      "source": [
        "#encoded_test = np.array(encoder.predict(np.array(svm_test2)))\n",
        "vae_test = vae.predict(np.array(X_test_chatter))\n",
        "vae_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_chatter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "b6c1jQ4Qelle",
        "outputId": "08640f13-88fb-436b-ab77-054f41d630d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2         3         4         5         6     \\\n",
              "275   1.922543  1.682538  1.964512  1.964512  1.964512  1.964512  1.964512   \n",
              "1343  0.104639  1.534189 -0.042187 -0.333439  0.841464 -0.394933 -0.676873   \n",
              "1119  0.377032  0.306928 -0.763493 -1.146166 -0.886482 -1.770661 -0.661235   \n",
              "1787 -0.235515  0.296971  0.659029  0.805621  0.230732  0.047419 -0.309192   \n",
              "1365  0.600053 -0.315752  0.504004 -0.044706 -0.704633 -0.421991 -1.656807   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "503  -0.018351  0.195651  0.785884  0.154010 -0.047224  0.254393 -0.524541   \n",
              "169  -1.961236 -0.092145 -0.743170  0.209883  1.804824  1.235324  0.783190   \n",
              "1779 -0.611571 -0.278386 -1.138904 -0.174489 -0.593591 -0.615319 -0.083008   \n",
              "715   0.090993 -0.086991 -1.280049 -1.679004 -0.781355 -0.825690  0.537035   \n",
              "1403 -0.188252 -0.333966 -0.448346  0.273018  0.291642  0.059191  0.307103   \n",
              "\n",
              "          7         8         9     ...      1191      1192      1193  \\\n",
              "275   1.947551  1.151867  1.964512  ... -1.485208 -0.084590  1.019096   \n",
              "1343  0.480929 -0.829673  0.031782  ... -0.051734 -0.096127  0.632206   \n",
              "1119 -0.234461  0.138549  0.531881  ... -0.201898  0.434134  1.674514   \n",
              "1787 -0.778778 -1.133281 -0.628965  ...  0.432436 -0.197213  0.480870   \n",
              "1365 -1.886447 -0.196393 -0.803376  ...  1.964512  1.964512  1.964512   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "503  -0.212323 -0.059113 -0.288401  ... -0.495668 -0.167695 -0.186554   \n",
              "169   0.107567 -1.605386 -1.114716  ... -0.652040 -0.987099 -1.888614   \n",
              "1779  0.269913 -0.626271  0.750042  ...  0.164552 -0.586329 -0.430601   \n",
              "715   0.680055  0.738036  1.423908  ...  0.479582  0.503535  1.425314   \n",
              "1403  0.040216 -0.361492 -0.466677  ...  0.031372  0.455569  1.719610   \n",
              "\n",
              "          1194      1195      1196      1197      1198      1199      1200  \n",
              "275   1.964512  1.964512  1.964512  1.082641  0.595602 -1.112139 -1.438881  \n",
              "1343  0.095502  0.359813  0.544005 -0.060577 -0.335664  0.177730 -0.594470  \n",
              "1119 -0.364537 -0.059992 -0.045936 -1.682401 -1.712094 -0.716639 -1.224762  \n",
              "1787  0.081446  0.567197  0.246897  1.057340 -0.124005 -0.028131 -1.039751  \n",
              "1365  1.964512  1.964512  1.964512  1.947493  0.820966  0.483213 -0.187022  \n",
              "...        ...       ...       ...       ...       ...       ...       ...  \n",
              "503   0.403269 -0.087225  0.439171  0.250704 -0.429019 -0.300583 -0.556870  \n",
              "169   1.964512 -1.292465 -1.273021 -0.047224  1.746199  1.612141  1.260156  \n",
              "1779 -1.423420 -1.178553 -0.858195 -0.747562 -0.402840  0.073540 -0.264330  \n",
              "715  -0.805895  0.517650 -0.458947 -0.953189 -0.092379 -0.180112  0.070495  \n",
              "1403  0.387691  1.836450  1.436968  1.198485  1.856773  1.327215 -0.133844  \n",
              "\n",
              "[191 rows x 1201 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cc1cb624-26a6-43f3-be89-06e29e48e5fe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>1191</th>\n",
              "      <th>1192</th>\n",
              "      <th>1193</th>\n",
              "      <th>1194</th>\n",
              "      <th>1195</th>\n",
              "      <th>1196</th>\n",
              "      <th>1197</th>\n",
              "      <th>1198</th>\n",
              "      <th>1199</th>\n",
              "      <th>1200</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>1.922543</td>\n",
              "      <td>1.682538</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>1.947551</td>\n",
              "      <td>1.151867</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.485208</td>\n",
              "      <td>-0.084590</td>\n",
              "      <td>1.019096</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>1.082641</td>\n",
              "      <td>0.595602</td>\n",
              "      <td>-1.112139</td>\n",
              "      <td>-1.438881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1343</th>\n",
              "      <td>0.104639</td>\n",
              "      <td>1.534189</td>\n",
              "      <td>-0.042187</td>\n",
              "      <td>-0.333439</td>\n",
              "      <td>0.841464</td>\n",
              "      <td>-0.394933</td>\n",
              "      <td>-0.676873</td>\n",
              "      <td>0.480929</td>\n",
              "      <td>-0.829673</td>\n",
              "      <td>0.031782</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.051734</td>\n",
              "      <td>-0.096127</td>\n",
              "      <td>0.632206</td>\n",
              "      <td>0.095502</td>\n",
              "      <td>0.359813</td>\n",
              "      <td>0.544005</td>\n",
              "      <td>-0.060577</td>\n",
              "      <td>-0.335664</td>\n",
              "      <td>0.177730</td>\n",
              "      <td>-0.594470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1119</th>\n",
              "      <td>0.377032</td>\n",
              "      <td>0.306928</td>\n",
              "      <td>-0.763493</td>\n",
              "      <td>-1.146166</td>\n",
              "      <td>-0.886482</td>\n",
              "      <td>-1.770661</td>\n",
              "      <td>-0.661235</td>\n",
              "      <td>-0.234461</td>\n",
              "      <td>0.138549</td>\n",
              "      <td>0.531881</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.201898</td>\n",
              "      <td>0.434134</td>\n",
              "      <td>1.674514</td>\n",
              "      <td>-0.364537</td>\n",
              "      <td>-0.059992</td>\n",
              "      <td>-0.045936</td>\n",
              "      <td>-1.682401</td>\n",
              "      <td>-1.712094</td>\n",
              "      <td>-0.716639</td>\n",
              "      <td>-1.224762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1787</th>\n",
              "      <td>-0.235515</td>\n",
              "      <td>0.296971</td>\n",
              "      <td>0.659029</td>\n",
              "      <td>0.805621</td>\n",
              "      <td>0.230732</td>\n",
              "      <td>0.047419</td>\n",
              "      <td>-0.309192</td>\n",
              "      <td>-0.778778</td>\n",
              "      <td>-1.133281</td>\n",
              "      <td>-0.628965</td>\n",
              "      <td>...</td>\n",
              "      <td>0.432436</td>\n",
              "      <td>-0.197213</td>\n",
              "      <td>0.480870</td>\n",
              "      <td>0.081446</td>\n",
              "      <td>0.567197</td>\n",
              "      <td>0.246897</td>\n",
              "      <td>1.057340</td>\n",
              "      <td>-0.124005</td>\n",
              "      <td>-0.028131</td>\n",
              "      <td>-1.039751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1365</th>\n",
              "      <td>0.600053</td>\n",
              "      <td>-0.315752</td>\n",
              "      <td>0.504004</td>\n",
              "      <td>-0.044706</td>\n",
              "      <td>-0.704633</td>\n",
              "      <td>-0.421991</td>\n",
              "      <td>-1.656807</td>\n",
              "      <td>-1.886447</td>\n",
              "      <td>-0.196393</td>\n",
              "      <td>-0.803376</td>\n",
              "      <td>...</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>1.947493</td>\n",
              "      <td>0.820966</td>\n",
              "      <td>0.483213</td>\n",
              "      <td>-0.187022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>-0.018351</td>\n",
              "      <td>0.195651</td>\n",
              "      <td>0.785884</td>\n",
              "      <td>0.154010</td>\n",
              "      <td>-0.047224</td>\n",
              "      <td>0.254393</td>\n",
              "      <td>-0.524541</td>\n",
              "      <td>-0.212323</td>\n",
              "      <td>-0.059113</td>\n",
              "      <td>-0.288401</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.495668</td>\n",
              "      <td>-0.167695</td>\n",
              "      <td>-0.186554</td>\n",
              "      <td>0.403269</td>\n",
              "      <td>-0.087225</td>\n",
              "      <td>0.439171</td>\n",
              "      <td>0.250704</td>\n",
              "      <td>-0.429019</td>\n",
              "      <td>-0.300583</td>\n",
              "      <td>-0.556870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>-1.961236</td>\n",
              "      <td>-0.092145</td>\n",
              "      <td>-0.743170</td>\n",
              "      <td>0.209883</td>\n",
              "      <td>1.804824</td>\n",
              "      <td>1.235324</td>\n",
              "      <td>0.783190</td>\n",
              "      <td>0.107567</td>\n",
              "      <td>-1.605386</td>\n",
              "      <td>-1.114716</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.652040</td>\n",
              "      <td>-0.987099</td>\n",
              "      <td>-1.888614</td>\n",
              "      <td>1.964512</td>\n",
              "      <td>-1.292465</td>\n",
              "      <td>-1.273021</td>\n",
              "      <td>-0.047224</td>\n",
              "      <td>1.746199</td>\n",
              "      <td>1.612141</td>\n",
              "      <td>1.260156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1779</th>\n",
              "      <td>-0.611571</td>\n",
              "      <td>-0.278386</td>\n",
              "      <td>-1.138904</td>\n",
              "      <td>-0.174489</td>\n",
              "      <td>-0.593591</td>\n",
              "      <td>-0.615319</td>\n",
              "      <td>-0.083008</td>\n",
              "      <td>0.269913</td>\n",
              "      <td>-0.626271</td>\n",
              "      <td>0.750042</td>\n",
              "      <td>...</td>\n",
              "      <td>0.164552</td>\n",
              "      <td>-0.586329</td>\n",
              "      <td>-0.430601</td>\n",
              "      <td>-1.423420</td>\n",
              "      <td>-1.178553</td>\n",
              "      <td>-0.858195</td>\n",
              "      <td>-0.747562</td>\n",
              "      <td>-0.402840</td>\n",
              "      <td>0.073540</td>\n",
              "      <td>-0.264330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>715</th>\n",
              "      <td>0.090993</td>\n",
              "      <td>-0.086991</td>\n",
              "      <td>-1.280049</td>\n",
              "      <td>-1.679004</td>\n",
              "      <td>-0.781355</td>\n",
              "      <td>-0.825690</td>\n",
              "      <td>0.537035</td>\n",
              "      <td>0.680055</td>\n",
              "      <td>0.738036</td>\n",
              "      <td>1.423908</td>\n",
              "      <td>...</td>\n",
              "      <td>0.479582</td>\n",
              "      <td>0.503535</td>\n",
              "      <td>1.425314</td>\n",
              "      <td>-0.805895</td>\n",
              "      <td>0.517650</td>\n",
              "      <td>-0.458947</td>\n",
              "      <td>-0.953189</td>\n",
              "      <td>-0.092379</td>\n",
              "      <td>-0.180112</td>\n",
              "      <td>0.070495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1403</th>\n",
              "      <td>-0.188252</td>\n",
              "      <td>-0.333966</td>\n",
              "      <td>-0.448346</td>\n",
              "      <td>0.273018</td>\n",
              "      <td>0.291642</td>\n",
              "      <td>0.059191</td>\n",
              "      <td>0.307103</td>\n",
              "      <td>0.040216</td>\n",
              "      <td>-0.361492</td>\n",
              "      <td>-0.466677</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031372</td>\n",
              "      <td>0.455569</td>\n",
              "      <td>1.719610</td>\n",
              "      <td>0.387691</td>\n",
              "      <td>1.836450</td>\n",
              "      <td>1.436968</td>\n",
              "      <td>1.198485</td>\n",
              "      <td>1.856773</td>\n",
              "      <td>1.327215</td>\n",
              "      <td>-0.133844</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>191 rows × 1201 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc1cb624-26a6-43f3-be89-06e29e48e5fe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cc1cb624-26a6-43f3-be89-06e29e48e5fe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cc1cb624-26a6-43f3-be89-06e29e48e5fe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a2da4fd1-6850-435e-beda-1aa57ea45184\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a2da4fd1-6850-435e-beda-1aa57ea45184')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a2da4fd1-6850-435e-beda-1aa57ea45184 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ApfOEoTHvyX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "ab038e1a-649e-4f98-c252-3fcc35534fb6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#plt.plot(np.array(X_stable)[2],label='true signal')\n",
        "plt.plot(np.array(vae_test)[15],label='true signal')\n",
        "plt.plot(np.array(X_test_chatter)[15],label='y signal')\n",
        "#plt.plot(vae_test[0],label='encoded-decoded signal')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_stable = np.concatenate((X_train_stable,X_test_stable),axis=0)\n",
        "X_chatter = np.concatenate((X_train_chatter,X_test_chatter),axis=0)"
      ],
      "metadata": {
        "id": "v17wlAVThO5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAVzt2t4gmir",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "ea436ba1-4bdf-44b5-c48d-28e2727aad24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0         1         2         3         4         5         6     \\\n",
              "0   -1.247119 -0.658239 -1.002723 -1.201698 -0.626322 -1.177194 -1.079080   \n",
              "1   -1.034921 -0.662463 -1.287324 -0.986255 -0.746049 -1.341429 -0.806382   \n",
              "2   -1.014696 -0.614843 -1.251989 -0.942685 -0.678279 -1.296831 -0.808196   \n",
              "3   -1.193477 -0.764640 -0.752227 -1.207627 -0.695848 -0.879910 -1.218636   \n",
              "4   -0.810190 -1.345667 -0.772311 -0.914815 -1.313296 -0.688558 -1.100073   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "948 -1.199405 -0.603859 -0.975666 -1.168803 -0.588212 -1.142226 -1.032542   \n",
              "949 -0.681696 -0.597369 -1.176980 -0.617172 -0.760493 -1.203288 -0.533086   \n",
              "950 -1.080897 -1.204333 -0.618143 -1.153816 -1.097029 -0.633976 -1.320615   \n",
              "951 -1.093330 -0.561707 -1.147600 -1.042158 -0.587064 -1.251155 -0.897575   \n",
              "952 -0.725749 -0.484846 -1.106246 -0.649545 -0.625568 -1.152942 -0.526841   \n",
              "\n",
              "         7         8         9     ...      1191      1192      1193  \\\n",
              "0   -0.660064 -1.223737 -0.978103  ... -1.050704 -1.125739 -0.571560   \n",
              "1   -0.849832 -1.280061 -0.684462  ... -1.302523 -0.868230 -0.677421   \n",
              "2   -0.770398 -1.281240 -0.697602  ... -1.239281 -0.905472 -0.687081   \n",
              "3   -0.616375 -0.944635 -1.112388  ... -0.768159 -1.139987 -0.626775   \n",
              "4   -1.197142 -0.561146 -1.124901  ... -0.712727 -0.923697 -1.246542   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "948 -0.608088 -1.162683 -0.921014  ... -0.997260 -1.065512 -0.515260   \n",
              "949 -0.895164 -1.047799 -0.397452  ... -1.202046 -0.535123 -0.669273   \n",
              "950 -1.010325 -0.632131 -1.327054  ... -0.623405 -1.161940 -1.037467   \n",
              "951 -0.646208 -1.253838 -0.755356  ... -1.122711 -0.969429 -0.591398   \n",
              "952 -0.727309 -1.056082 -0.410907  ... -1.119038 -0.573587 -0.585000   \n",
              "\n",
              "         1194      1195      1196      1197      1198      1199      1200  \n",
              "0   -1.097375 -1.057637 -0.684252 -1.230707 -0.927904 -0.683271 -1.246336  \n",
              "1   -1.310598 -0.772808 -0.908920 -1.335207 -0.691635 -0.977430 -1.249920  \n",
              "2   -1.255985 -0.782589 -0.858866 -1.302649 -0.765889 -0.965221 -1.258315  \n",
              "3   -0.813158 -1.147579 -0.633516 -0.986299 -1.103421 -0.582606 -1.065968  \n",
              "4   -0.666775 -1.068555 -1.241235 -0.669744 -1.143107 -1.121638 -0.650921  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "948 -1.063552 -1.003945 -0.620328 -1.202586 -0.925829 -0.630563 -1.206069  \n",
              "949 -1.141198 -0.446937 -0.946682 -1.066153 -0.478908 -1.046918 -0.964586  \n",
              "950 -0.627989 -1.260358 -0.994453 -0.721050 -1.241086 -0.814595 -0.807265  \n",
              "951 -1.183944 -0.892129 -0.728412 -1.265786 -0.855031 -0.818443 -1.255143  \n",
              "952 -1.101611 -0.489114 -0.804806 -1.072195 -0.500851 -0.898460 -0.975763  \n",
              "\n",
              "[953 rows x 1201 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b05ad9ee-1089-4663-87df-eaa93821702d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>1191</th>\n",
              "      <th>1192</th>\n",
              "      <th>1193</th>\n",
              "      <th>1194</th>\n",
              "      <th>1195</th>\n",
              "      <th>1196</th>\n",
              "      <th>1197</th>\n",
              "      <th>1198</th>\n",
              "      <th>1199</th>\n",
              "      <th>1200</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.247119</td>\n",
              "      <td>-0.658239</td>\n",
              "      <td>-1.002723</td>\n",
              "      <td>-1.201698</td>\n",
              "      <td>-0.626322</td>\n",
              "      <td>-1.177194</td>\n",
              "      <td>-1.079080</td>\n",
              "      <td>-0.660064</td>\n",
              "      <td>-1.223737</td>\n",
              "      <td>-0.978103</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.050704</td>\n",
              "      <td>-1.125739</td>\n",
              "      <td>-0.571560</td>\n",
              "      <td>-1.097375</td>\n",
              "      <td>-1.057637</td>\n",
              "      <td>-0.684252</td>\n",
              "      <td>-1.230707</td>\n",
              "      <td>-0.927904</td>\n",
              "      <td>-0.683271</td>\n",
              "      <td>-1.246336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.034921</td>\n",
              "      <td>-0.662463</td>\n",
              "      <td>-1.287324</td>\n",
              "      <td>-0.986255</td>\n",
              "      <td>-0.746049</td>\n",
              "      <td>-1.341429</td>\n",
              "      <td>-0.806382</td>\n",
              "      <td>-0.849832</td>\n",
              "      <td>-1.280061</td>\n",
              "      <td>-0.684462</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.302523</td>\n",
              "      <td>-0.868230</td>\n",
              "      <td>-0.677421</td>\n",
              "      <td>-1.310598</td>\n",
              "      <td>-0.772808</td>\n",
              "      <td>-0.908920</td>\n",
              "      <td>-1.335207</td>\n",
              "      <td>-0.691635</td>\n",
              "      <td>-0.977430</td>\n",
              "      <td>-1.249920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.014696</td>\n",
              "      <td>-0.614843</td>\n",
              "      <td>-1.251989</td>\n",
              "      <td>-0.942685</td>\n",
              "      <td>-0.678279</td>\n",
              "      <td>-1.296831</td>\n",
              "      <td>-0.808196</td>\n",
              "      <td>-0.770398</td>\n",
              "      <td>-1.281240</td>\n",
              "      <td>-0.697602</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.239281</td>\n",
              "      <td>-0.905472</td>\n",
              "      <td>-0.687081</td>\n",
              "      <td>-1.255985</td>\n",
              "      <td>-0.782589</td>\n",
              "      <td>-0.858866</td>\n",
              "      <td>-1.302649</td>\n",
              "      <td>-0.765889</td>\n",
              "      <td>-0.965221</td>\n",
              "      <td>-1.258315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.193477</td>\n",
              "      <td>-0.764640</td>\n",
              "      <td>-0.752227</td>\n",
              "      <td>-1.207627</td>\n",
              "      <td>-0.695848</td>\n",
              "      <td>-0.879910</td>\n",
              "      <td>-1.218636</td>\n",
              "      <td>-0.616375</td>\n",
              "      <td>-0.944635</td>\n",
              "      <td>-1.112388</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.768159</td>\n",
              "      <td>-1.139987</td>\n",
              "      <td>-0.626775</td>\n",
              "      <td>-0.813158</td>\n",
              "      <td>-1.147579</td>\n",
              "      <td>-0.633516</td>\n",
              "      <td>-0.986299</td>\n",
              "      <td>-1.103421</td>\n",
              "      <td>-0.582606</td>\n",
              "      <td>-1.065968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.810190</td>\n",
              "      <td>-1.345667</td>\n",
              "      <td>-0.772311</td>\n",
              "      <td>-0.914815</td>\n",
              "      <td>-1.313296</td>\n",
              "      <td>-0.688558</td>\n",
              "      <td>-1.100073</td>\n",
              "      <td>-1.197142</td>\n",
              "      <td>-0.561146</td>\n",
              "      <td>-1.124901</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.712727</td>\n",
              "      <td>-0.923697</td>\n",
              "      <td>-1.246542</td>\n",
              "      <td>-0.666775</td>\n",
              "      <td>-1.068555</td>\n",
              "      <td>-1.241235</td>\n",
              "      <td>-0.669744</td>\n",
              "      <td>-1.143107</td>\n",
              "      <td>-1.121638</td>\n",
              "      <td>-0.650921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>948</th>\n",
              "      <td>-1.199405</td>\n",
              "      <td>-0.603859</td>\n",
              "      <td>-0.975666</td>\n",
              "      <td>-1.168803</td>\n",
              "      <td>-0.588212</td>\n",
              "      <td>-1.142226</td>\n",
              "      <td>-1.032542</td>\n",
              "      <td>-0.608088</td>\n",
              "      <td>-1.162683</td>\n",
              "      <td>-0.921014</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.997260</td>\n",
              "      <td>-1.065512</td>\n",
              "      <td>-0.515260</td>\n",
              "      <td>-1.063552</td>\n",
              "      <td>-1.003945</td>\n",
              "      <td>-0.620328</td>\n",
              "      <td>-1.202586</td>\n",
              "      <td>-0.925829</td>\n",
              "      <td>-0.630563</td>\n",
              "      <td>-1.206069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>949</th>\n",
              "      <td>-0.681696</td>\n",
              "      <td>-0.597369</td>\n",
              "      <td>-1.176980</td>\n",
              "      <td>-0.617172</td>\n",
              "      <td>-0.760493</td>\n",
              "      <td>-1.203288</td>\n",
              "      <td>-0.533086</td>\n",
              "      <td>-0.895164</td>\n",
              "      <td>-1.047799</td>\n",
              "      <td>-0.397452</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.202046</td>\n",
              "      <td>-0.535123</td>\n",
              "      <td>-0.669273</td>\n",
              "      <td>-1.141198</td>\n",
              "      <td>-0.446937</td>\n",
              "      <td>-0.946682</td>\n",
              "      <td>-1.066153</td>\n",
              "      <td>-0.478908</td>\n",
              "      <td>-1.046918</td>\n",
              "      <td>-0.964586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>950</th>\n",
              "      <td>-1.080897</td>\n",
              "      <td>-1.204333</td>\n",
              "      <td>-0.618143</td>\n",
              "      <td>-1.153816</td>\n",
              "      <td>-1.097029</td>\n",
              "      <td>-0.633976</td>\n",
              "      <td>-1.320615</td>\n",
              "      <td>-1.010325</td>\n",
              "      <td>-0.632131</td>\n",
              "      <td>-1.327054</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.623405</td>\n",
              "      <td>-1.161940</td>\n",
              "      <td>-1.037467</td>\n",
              "      <td>-0.627989</td>\n",
              "      <td>-1.260358</td>\n",
              "      <td>-0.994453</td>\n",
              "      <td>-0.721050</td>\n",
              "      <td>-1.241086</td>\n",
              "      <td>-0.814595</td>\n",
              "      <td>-0.807265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>951</th>\n",
              "      <td>-1.093330</td>\n",
              "      <td>-0.561707</td>\n",
              "      <td>-1.147600</td>\n",
              "      <td>-1.042158</td>\n",
              "      <td>-0.587064</td>\n",
              "      <td>-1.251155</td>\n",
              "      <td>-0.897575</td>\n",
              "      <td>-0.646208</td>\n",
              "      <td>-1.253838</td>\n",
              "      <td>-0.755356</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.122711</td>\n",
              "      <td>-0.969429</td>\n",
              "      <td>-0.591398</td>\n",
              "      <td>-1.183944</td>\n",
              "      <td>-0.892129</td>\n",
              "      <td>-0.728412</td>\n",
              "      <td>-1.265786</td>\n",
              "      <td>-0.855031</td>\n",
              "      <td>-0.818443</td>\n",
              "      <td>-1.255143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>952</th>\n",
              "      <td>-0.725749</td>\n",
              "      <td>-0.484846</td>\n",
              "      <td>-1.106246</td>\n",
              "      <td>-0.649545</td>\n",
              "      <td>-0.625568</td>\n",
              "      <td>-1.152942</td>\n",
              "      <td>-0.526841</td>\n",
              "      <td>-0.727309</td>\n",
              "      <td>-1.056082</td>\n",
              "      <td>-0.410907</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.119038</td>\n",
              "      <td>-0.573587</td>\n",
              "      <td>-0.585000</td>\n",
              "      <td>-1.101611</td>\n",
              "      <td>-0.489114</td>\n",
              "      <td>-0.804806</td>\n",
              "      <td>-1.072195</td>\n",
              "      <td>-0.500851</td>\n",
              "      <td>-0.898460</td>\n",
              "      <td>-0.975763</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>953 rows × 1201 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b05ad9ee-1089-4663-87df-eaa93821702d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b05ad9ee-1089-4663-87df-eaa93821702d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b05ad9ee-1089-4663-87df-eaa93821702d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a60e38d2-ff20-4e56-b765-bb126b24c599\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a60e38d2-ff20-4e56-b765-bb126b24c599')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a60e38d2-ff20-4e56-b765-bb126b24c599 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ],
      "source": [
        "vae_generated_stable = vae.predict(X_stable)\n",
        "#vae_generated_stable = scaler_chatter.inverse_transform(vae_generated_stable)\n",
        "vae_generated_stable = pd.DataFrame(vae_generated_stable)\n",
        "vae_generated_stable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "iiO9CpPW8udX",
        "outputId": "2adbae39-9833-4622-bbd1-2bb53376ad1b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0         1         2         3         4         5         6     \\\n",
              "0    0.503809  0.668056  0.185039  0.330226 -0.042543 -0.665721 -0.254632   \n",
              "1    1.053986  0.888298 -0.018872  0.058817 -0.553694 -1.113668 -0.793875   \n",
              "2   -0.770222 -1.247188 -0.457495 -0.375940 -0.056821  0.947299  1.250625   \n",
              "3    1.113125  0.660642 -0.000367 -0.040872 -0.594140 -1.026506 -0.745788   \n",
              "4    0.799404  0.088068  0.153921 -0.061041 -0.548115 -0.233107 -0.005679   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "948 -0.118947 -0.240552  0.336720 -0.309960 -0.495066 -0.276007 -0.798900   \n",
              "949 -0.448530 -0.199130 -0.540590  0.439760  0.671819  0.185687  0.774908   \n",
              "950 -0.293851 -0.473391 -1.100358 -0.265876 -0.620810 -0.628168  0.197359   \n",
              "951 -0.082612 -0.053637 -0.634518 -0.433149  0.026871 -0.341630  0.744611   \n",
              "952 -0.590680  0.144435 -0.095596  0.149732  0.810318 -0.046846  0.020341   \n",
              "\n",
              "         7         8         9     ...      1191      1192      1193  \\\n",
              "0   -0.430536 -0.531415  0.222507  ... -0.430825  0.234815  0.297519   \n",
              "1   -0.655930 -0.586679  0.719684  ... -0.552607  0.828006  1.092998   \n",
              "2    0.763853  0.770108  0.113301  ...  0.757292  0.097910 -0.649027   \n",
              "3   -0.624696 -0.450099  0.620807  ... -0.456832  0.750122  0.993291   \n",
              "4   -0.213023  0.406193  0.452800  ...  0.418285  0.558966  0.294640   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "948 -0.575996 -0.133440 -0.461488  ... -0.069356 -0.459398 -0.182916   \n",
              "949  0.258626 -0.498812 -0.228002  ... -0.718723 -0.403309 -0.468547   \n",
              "950 -0.094700 -0.154754  0.480223  ... -0.274375  0.433020 -0.226533   \n",
              "951  0.908290  0.459272  0.967162  ...  0.480696  0.806414  0.765780   \n",
              "952  0.031961 -0.712143 -0.583554  ... -0.637503 -0.831332 -0.308371   \n",
              "\n",
              "         1194      1195      1196      1197      1198      1199      1200  \n",
              "0    0.214371  0.871276  0.344799 -0.039269  0.215087 -0.358992 -0.464546  \n",
              "1    0.734656  1.333362  0.284504 -0.623581 -0.094510 -0.866790 -0.835775  \n",
              "2   -1.050137 -0.998205 -0.703596 -0.088969  0.373958  0.908917  1.361063  \n",
              "3    0.728667  1.228396  0.109250 -0.572858 -0.143555 -0.852503 -0.691564  \n",
              "4    0.660639  0.391865 -0.341706 -0.026764  0.022756 -0.378540  0.100205  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "948  0.242108 -0.221699 -0.013867 -0.071164 -0.676864 -0.403845 -0.401174  \n",
              "949 -1.070159 -0.109657 -0.300353 -0.212197  0.824482  0.218042  0.197422  \n",
              "950 -0.839944 -0.119775 -0.765365 -0.709446 -0.289550 -0.471074  0.023602  \n",
              "951 -0.546784 -0.253951 -0.072553 -0.793294  0.058317  0.059350  0.263585  \n",
              "952 -0.771263 -0.231355  0.189599 -0.082374  0.458161  0.398034 -0.235247  \n",
              "\n",
              "[953 rows x 1201 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9ba3ec8c-0eb4-4195-bead-2b797e294b40\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>1191</th>\n",
              "      <th>1192</th>\n",
              "      <th>1193</th>\n",
              "      <th>1194</th>\n",
              "      <th>1195</th>\n",
              "      <th>1196</th>\n",
              "      <th>1197</th>\n",
              "      <th>1198</th>\n",
              "      <th>1199</th>\n",
              "      <th>1200</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.503809</td>\n",
              "      <td>0.668056</td>\n",
              "      <td>0.185039</td>\n",
              "      <td>0.330226</td>\n",
              "      <td>-0.042543</td>\n",
              "      <td>-0.665721</td>\n",
              "      <td>-0.254632</td>\n",
              "      <td>-0.430536</td>\n",
              "      <td>-0.531415</td>\n",
              "      <td>0.222507</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.430825</td>\n",
              "      <td>0.234815</td>\n",
              "      <td>0.297519</td>\n",
              "      <td>0.214371</td>\n",
              "      <td>0.871276</td>\n",
              "      <td>0.344799</td>\n",
              "      <td>-0.039269</td>\n",
              "      <td>0.215087</td>\n",
              "      <td>-0.358992</td>\n",
              "      <td>-0.464546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.053986</td>\n",
              "      <td>0.888298</td>\n",
              "      <td>-0.018872</td>\n",
              "      <td>0.058817</td>\n",
              "      <td>-0.553694</td>\n",
              "      <td>-1.113668</td>\n",
              "      <td>-0.793875</td>\n",
              "      <td>-0.655930</td>\n",
              "      <td>-0.586679</td>\n",
              "      <td>0.719684</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.552607</td>\n",
              "      <td>0.828006</td>\n",
              "      <td>1.092998</td>\n",
              "      <td>0.734656</td>\n",
              "      <td>1.333362</td>\n",
              "      <td>0.284504</td>\n",
              "      <td>-0.623581</td>\n",
              "      <td>-0.094510</td>\n",
              "      <td>-0.866790</td>\n",
              "      <td>-0.835775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.770222</td>\n",
              "      <td>-1.247188</td>\n",
              "      <td>-0.457495</td>\n",
              "      <td>-0.375940</td>\n",
              "      <td>-0.056821</td>\n",
              "      <td>0.947299</td>\n",
              "      <td>1.250625</td>\n",
              "      <td>0.763853</td>\n",
              "      <td>0.770108</td>\n",
              "      <td>0.113301</td>\n",
              "      <td>...</td>\n",
              "      <td>0.757292</td>\n",
              "      <td>0.097910</td>\n",
              "      <td>-0.649027</td>\n",
              "      <td>-1.050137</td>\n",
              "      <td>-0.998205</td>\n",
              "      <td>-0.703596</td>\n",
              "      <td>-0.088969</td>\n",
              "      <td>0.373958</td>\n",
              "      <td>0.908917</td>\n",
              "      <td>1.361063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.113125</td>\n",
              "      <td>0.660642</td>\n",
              "      <td>-0.000367</td>\n",
              "      <td>-0.040872</td>\n",
              "      <td>-0.594140</td>\n",
              "      <td>-1.026506</td>\n",
              "      <td>-0.745788</td>\n",
              "      <td>-0.624696</td>\n",
              "      <td>-0.450099</td>\n",
              "      <td>0.620807</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.456832</td>\n",
              "      <td>0.750122</td>\n",
              "      <td>0.993291</td>\n",
              "      <td>0.728667</td>\n",
              "      <td>1.228396</td>\n",
              "      <td>0.109250</td>\n",
              "      <td>-0.572858</td>\n",
              "      <td>-0.143555</td>\n",
              "      <td>-0.852503</td>\n",
              "      <td>-0.691564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.799404</td>\n",
              "      <td>0.088068</td>\n",
              "      <td>0.153921</td>\n",
              "      <td>-0.061041</td>\n",
              "      <td>-0.548115</td>\n",
              "      <td>-0.233107</td>\n",
              "      <td>-0.005679</td>\n",
              "      <td>-0.213023</td>\n",
              "      <td>0.406193</td>\n",
              "      <td>0.452800</td>\n",
              "      <td>...</td>\n",
              "      <td>0.418285</td>\n",
              "      <td>0.558966</td>\n",
              "      <td>0.294640</td>\n",
              "      <td>0.660639</td>\n",
              "      <td>0.391865</td>\n",
              "      <td>-0.341706</td>\n",
              "      <td>-0.026764</td>\n",
              "      <td>0.022756</td>\n",
              "      <td>-0.378540</td>\n",
              "      <td>0.100205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>948</th>\n",
              "      <td>-0.118947</td>\n",
              "      <td>-0.240552</td>\n",
              "      <td>0.336720</td>\n",
              "      <td>-0.309960</td>\n",
              "      <td>-0.495066</td>\n",
              "      <td>-0.276007</td>\n",
              "      <td>-0.798900</td>\n",
              "      <td>-0.575996</td>\n",
              "      <td>-0.133440</td>\n",
              "      <td>-0.461488</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.069356</td>\n",
              "      <td>-0.459398</td>\n",
              "      <td>-0.182916</td>\n",
              "      <td>0.242108</td>\n",
              "      <td>-0.221699</td>\n",
              "      <td>-0.013867</td>\n",
              "      <td>-0.071164</td>\n",
              "      <td>-0.676864</td>\n",
              "      <td>-0.403845</td>\n",
              "      <td>-0.401174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>949</th>\n",
              "      <td>-0.448530</td>\n",
              "      <td>-0.199130</td>\n",
              "      <td>-0.540590</td>\n",
              "      <td>0.439760</td>\n",
              "      <td>0.671819</td>\n",
              "      <td>0.185687</td>\n",
              "      <td>0.774908</td>\n",
              "      <td>0.258626</td>\n",
              "      <td>-0.498812</td>\n",
              "      <td>-0.228002</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.718723</td>\n",
              "      <td>-0.403309</td>\n",
              "      <td>-0.468547</td>\n",
              "      <td>-1.070159</td>\n",
              "      <td>-0.109657</td>\n",
              "      <td>-0.300353</td>\n",
              "      <td>-0.212197</td>\n",
              "      <td>0.824482</td>\n",
              "      <td>0.218042</td>\n",
              "      <td>0.197422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>950</th>\n",
              "      <td>-0.293851</td>\n",
              "      <td>-0.473391</td>\n",
              "      <td>-1.100358</td>\n",
              "      <td>-0.265876</td>\n",
              "      <td>-0.620810</td>\n",
              "      <td>-0.628168</td>\n",
              "      <td>0.197359</td>\n",
              "      <td>-0.094700</td>\n",
              "      <td>-0.154754</td>\n",
              "      <td>0.480223</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.274375</td>\n",
              "      <td>0.433020</td>\n",
              "      <td>-0.226533</td>\n",
              "      <td>-0.839944</td>\n",
              "      <td>-0.119775</td>\n",
              "      <td>-0.765365</td>\n",
              "      <td>-0.709446</td>\n",
              "      <td>-0.289550</td>\n",
              "      <td>-0.471074</td>\n",
              "      <td>0.023602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>951</th>\n",
              "      <td>-0.082612</td>\n",
              "      <td>-0.053637</td>\n",
              "      <td>-0.634518</td>\n",
              "      <td>-0.433149</td>\n",
              "      <td>0.026871</td>\n",
              "      <td>-0.341630</td>\n",
              "      <td>0.744611</td>\n",
              "      <td>0.908290</td>\n",
              "      <td>0.459272</td>\n",
              "      <td>0.967162</td>\n",
              "      <td>...</td>\n",
              "      <td>0.480696</td>\n",
              "      <td>0.806414</td>\n",
              "      <td>0.765780</td>\n",
              "      <td>-0.546784</td>\n",
              "      <td>-0.253951</td>\n",
              "      <td>-0.072553</td>\n",
              "      <td>-0.793294</td>\n",
              "      <td>0.058317</td>\n",
              "      <td>0.059350</td>\n",
              "      <td>0.263585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>952</th>\n",
              "      <td>-0.590680</td>\n",
              "      <td>0.144435</td>\n",
              "      <td>-0.095596</td>\n",
              "      <td>0.149732</td>\n",
              "      <td>0.810318</td>\n",
              "      <td>-0.046846</td>\n",
              "      <td>0.020341</td>\n",
              "      <td>0.031961</td>\n",
              "      <td>-0.712143</td>\n",
              "      <td>-0.583554</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.637503</td>\n",
              "      <td>-0.831332</td>\n",
              "      <td>-0.308371</td>\n",
              "      <td>-0.771263</td>\n",
              "      <td>-0.231355</td>\n",
              "      <td>0.189599</td>\n",
              "      <td>-0.082374</td>\n",
              "      <td>0.458161</td>\n",
              "      <td>0.398034</td>\n",
              "      <td>-0.235247</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>953 rows × 1201 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ba3ec8c-0eb4-4195-bead-2b797e294b40')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9ba3ec8c-0eb4-4195-bead-2b797e294b40 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9ba3ec8c-0eb4-4195-bead-2b797e294b40');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-668cd719-085f-49c3-a903-cd6a53a84de2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-668cd719-085f-49c3-a903-cd6a53a84de2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-668cd719-085f-49c3-a903-cd6a53a84de2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ],
      "source": [
        "vae_generated_chatter = vae.predict(X_chatter)\n",
        "#vae_generated_chatter = scaler_chatter.inverse_transform(vae_generated_chatter)\n",
        "vae_generated_chatter = pd.DataFrame((vae_generated_chatter))\n",
        "vae_generated_chatter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_stable2 = pd.DataFrame(X_stable)\n",
        "X_chatter2 = pd.DataFrame(X_chatter)"
      ],
      "metadata": {
        "id": "vFf-g08hl7GA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLryeqU-fZWl"
      },
      "outputs": [],
      "source": [
        "chatter_svm_ch2 = vae_generated_chatter.iloc[8:80]\n",
        "\n",
        "stable_svm_all = pd.concat([X_stable2], axis=0)\n",
        "stable_svm_all = stable_svm_all.reset_index(drop=True)\n",
        "\n",
        "stable_svm_train = stable_svm_all.iloc[0:800]\n",
        "stable_svm_test = stable_svm_all.iloc[753:953]\n",
        "\n",
        "chatter_svm_train_o = X_chatter2.iloc[0:8]\n",
        "chatter_svm_test = X_chatter2.iloc[753:953]\n",
        "\n",
        "stable_svm_train_label = np.full((800, 1), 1)\n",
        "chatter_svm_train_o_label = np.full((8, 1), 2)\n",
        "\n",
        "stable_svm_test_label = np.full((200, 1), 1)\n",
        "chatter_svm_test_o_label = np.full((200, 1), 2)\n",
        "\n",
        "svm_train = pd.concat([stable_svm_train, chatter_svm_train_o], axis=0)\n",
        "svm_train = svm_train.reset_index(drop=True)\n",
        "\n",
        "svm_test = pd.concat([stable_svm_test, chatter_svm_test], axis=0)\n",
        "svm_test = svm_test.reset_index(drop=True)\n",
        "\n",
        "svm_train_label = np.concatenate((stable_svm_train_label, chatter_svm_train_o_label), axis=0)\n",
        "svm_test_label = np.concatenate((stable_svm_test_label, chatter_svm_test_o_label), axis=0)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNp6Xstw4OZl"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_svm = scaler.fit_transform(svm_train)\n",
        "X_test_svm = scaler.transform(svm_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters: C=100, kernel='poly,'degree=1, gamma='auto', Accuracy=0.57\n",
        "Parameters: C=100, kernel='poly,'degree=1, gamma='scale', Accuracy=0.57\n",
        "Parameters: C=1000, kernel='poly,'degree=1, gamma='auto', Accuracy=0.57\n",
        "Parameters: C=1000, kernel='poly,'degree=1, gamma='scale', Accuracy=0.57"
      ],
      "metadata": {
        "id": "QGKD0Wj6NaYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXJxcVgrAoJR",
        "outputId": "a6c7d478-73fb-4d0d-bb6f-8d727eaac78a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1524, 1202)"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOnDoaorlbPo",
        "outputId": "9f33936c-4bc7-4135-cdd5-a407b36afdce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.945\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.90      1.00      0.95       200\n",
            "           2       1.00      0.89      0.94       200\n",
            "\n",
            "    accuracy                           0.94       400\n",
            "   macro avg       0.95      0.95      0.94       400\n",
            "weighted avg       0.95      0.94      0.94       400\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "\n",
        "clf = svm.SVC(kernel = \"poly\",C=1,degree=2,gamma = 'auto')\n",
        "\n",
        "# Fit the classifier on the training data\n",
        "clf.fit(X_train_svm, svm_train_label)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "predictions = clf.predict(X_test_svm)\n",
        "\n",
        "# Calculate the accuracy of the SVM classifier\n",
        "accuracy = accuracy_score(svm_test_label, predictions)\n",
        "print(accuracy)\n",
        "print(classification_report(svm_test_label,predictions))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = scipy.io.loadmat('TrainingDataandLabels.mat')\n",
        "#1201-130-non_norm-shuffled.mat\n",
        "x2= data2['flow']\n",
        "print(x2.shape)\n",
        "y2 = data2 [\"labels\"]\n",
        "print(y2.shape)"
      ],
      "metadata": {
        "id": "pQ0kJNVpidSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f1f4059-0d2a-46db-a86c-27d11db70aaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1201, 1906)\n",
            "(1, 1906)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BR9HAY6kBXi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb54683f-c573-4e36-a075-d03fa4d56b3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1201, 1906)\n",
            "(1, 1906)\n"
          ]
        }
      ],
      "source": [
        "data2 = scipy.io.loadmat('1201-130-non_norm-shuffled.mat')\n",
        "x2 = data2['shuffled']\n",
        "print(x.shape)\n",
        "y2 = data2 [\"shuffled_labels\"]\n",
        "print(y2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNB2tJ1xkBVd"
      },
      "outputs": [],
      "source": [
        "x_transposed2 = np.transpose(x2)\n",
        "y_transposed2 = np.transpose(y2)\n",
        "df12 = pd.DataFrame(x_transposed2)\n",
        "df22 = pd.DataFrame(y_transposed2, columns = [\"Labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ubfp6osAkBRw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "4bc4a585-5cac-4184-b146-6788d654e0ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "0    -0.033332 -0.085857 -0.073185 -0.011171  0.056423  0.077605  0.061338   \n",
              "1    -0.014934 -0.088185 -0.089303  0.003536  0.094426  0.097737  0.001465   \n",
              "2    -0.012245 -0.046522 -0.058433 -0.033209  0.028661  0.063720  0.029545   \n",
              "3     0.030949  0.039864 -0.009889 -0.033550 -0.002050  0.039342  0.017993   \n",
              "4     0.021610 -0.019919 -0.047801 -0.043151  0.000843  0.047561  0.064020   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1901 -0.214760 -0.137189  0.063628  0.216979  0.117362 -0.129971 -0.232769   \n",
              "1902  0.003860 -0.026346 -0.026933  0.052409  0.137214  0.110519  0.018412   \n",
              "1903 -0.162564 -0.255623 -0.029582  0.252611  0.207382 -0.117210 -0.253825   \n",
              "1904  0.240359  0.231626  0.115252 -0.129998 -0.359195 -0.402580 -0.270478   \n",
              "1905  0.490254  0.476353  0.061108 -0.383877 -0.558357 -0.351572 -0.133216   \n",
              "\n",
              "             7         8         9  ...      1192      1193      1194  \\\n",
              "0     0.019678 -0.033606 -0.063481  ...  0.033123  0.015099  0.000589   \n",
              "1    -0.089163 -0.067931  0.037376  ... -0.003062 -0.013477 -0.004117   \n",
              "2    -0.035015 -0.066762 -0.048157  ...  0.240375  0.118871 -0.086310   \n",
              "3    -0.020063 -0.015736  0.009792  ...  0.057509  0.030321 -0.017843   \n",
              "4     0.024550 -0.017988 -0.020041  ... -0.023563 -0.028223 -0.001192   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "1901 -0.011466  0.174400 -0.002492  ... -0.118097 -0.043695  0.006463   \n",
              "1902 -0.066092 -0.096759 -0.095694  ... -0.117109  0.051782  0.184129   \n",
              "1903 -0.149713  0.076848  0.185937  ...  0.098490  0.375954  0.404241   \n",
              "1904 -0.083559  0.061900  0.162378  ...  0.102809  0.273305  0.374406   \n",
              "1905 -0.033826  0.070212  0.213363  ...  0.340287  0.257496  0.114228   \n",
              "\n",
              "          1195      1196      1197      1198      1199      1200  Labels  \n",
              "0    -0.018546 -0.040237 -0.015751  0.042935  0.056025  0.038771       1  \n",
              "1     0.016143  0.013687 -0.005675 -0.019226 -0.013902 -0.001791       1  \n",
              "2    -0.169833 -0.094856 -0.019909 -0.043971 -0.083824 -0.059989       1  \n",
              "3    -0.052160 -0.061638 -0.054184 -0.034421 -0.013585 -0.003385       1  \n",
              "4     0.021194  0.020908  0.007241 -0.014940 -0.029013 -0.012556       1  \n",
              "...        ...       ...       ...       ...       ...       ...     ...  \n",
              "1901  0.140691  0.270375  0.176057 -0.097308 -0.215748 -0.107130       2  \n",
              "1902  0.167420 -0.024256 -0.158296 -0.118841 -0.028036 -0.038221       2  \n",
              "1903  0.189033 -0.067160 -0.242110 -0.321250 -0.366230 -0.392366       2  \n",
              "1904  0.391519  0.281850  0.026974 -0.250276 -0.403238 -0.356164       2  \n",
              "1905  0.016397 -0.103353 -0.257294 -0.313553 -0.204888 -0.005498       2  \n",
              "\n",
              "[1906 rows x 1202 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d895ba7c-0b0d-4907-ae58-f427fdea1aed\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>1192</th>\n",
              "      <th>1193</th>\n",
              "      <th>1194</th>\n",
              "      <th>1195</th>\n",
              "      <th>1196</th>\n",
              "      <th>1197</th>\n",
              "      <th>1198</th>\n",
              "      <th>1199</th>\n",
              "      <th>1200</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.033332</td>\n",
              "      <td>-0.085857</td>\n",
              "      <td>-0.073185</td>\n",
              "      <td>-0.011171</td>\n",
              "      <td>0.056423</td>\n",
              "      <td>0.077605</td>\n",
              "      <td>0.061338</td>\n",
              "      <td>0.019678</td>\n",
              "      <td>-0.033606</td>\n",
              "      <td>-0.063481</td>\n",
              "      <td>...</td>\n",
              "      <td>0.033123</td>\n",
              "      <td>0.015099</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>-0.018546</td>\n",
              "      <td>-0.040237</td>\n",
              "      <td>-0.015751</td>\n",
              "      <td>0.042935</td>\n",
              "      <td>0.056025</td>\n",
              "      <td>0.038771</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.014934</td>\n",
              "      <td>-0.088185</td>\n",
              "      <td>-0.089303</td>\n",
              "      <td>0.003536</td>\n",
              "      <td>0.094426</td>\n",
              "      <td>0.097737</td>\n",
              "      <td>0.001465</td>\n",
              "      <td>-0.089163</td>\n",
              "      <td>-0.067931</td>\n",
              "      <td>0.037376</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.003062</td>\n",
              "      <td>-0.013477</td>\n",
              "      <td>-0.004117</td>\n",
              "      <td>0.016143</td>\n",
              "      <td>0.013687</td>\n",
              "      <td>-0.005675</td>\n",
              "      <td>-0.019226</td>\n",
              "      <td>-0.013902</td>\n",
              "      <td>-0.001791</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.012245</td>\n",
              "      <td>-0.046522</td>\n",
              "      <td>-0.058433</td>\n",
              "      <td>-0.033209</td>\n",
              "      <td>0.028661</td>\n",
              "      <td>0.063720</td>\n",
              "      <td>0.029545</td>\n",
              "      <td>-0.035015</td>\n",
              "      <td>-0.066762</td>\n",
              "      <td>-0.048157</td>\n",
              "      <td>...</td>\n",
              "      <td>0.240375</td>\n",
              "      <td>0.118871</td>\n",
              "      <td>-0.086310</td>\n",
              "      <td>-0.169833</td>\n",
              "      <td>-0.094856</td>\n",
              "      <td>-0.019909</td>\n",
              "      <td>-0.043971</td>\n",
              "      <td>-0.083824</td>\n",
              "      <td>-0.059989</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.030949</td>\n",
              "      <td>0.039864</td>\n",
              "      <td>-0.009889</td>\n",
              "      <td>-0.033550</td>\n",
              "      <td>-0.002050</td>\n",
              "      <td>0.039342</td>\n",
              "      <td>0.017993</td>\n",
              "      <td>-0.020063</td>\n",
              "      <td>-0.015736</td>\n",
              "      <td>0.009792</td>\n",
              "      <td>...</td>\n",
              "      <td>0.057509</td>\n",
              "      <td>0.030321</td>\n",
              "      <td>-0.017843</td>\n",
              "      <td>-0.052160</td>\n",
              "      <td>-0.061638</td>\n",
              "      <td>-0.054184</td>\n",
              "      <td>-0.034421</td>\n",
              "      <td>-0.013585</td>\n",
              "      <td>-0.003385</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.021610</td>\n",
              "      <td>-0.019919</td>\n",
              "      <td>-0.047801</td>\n",
              "      <td>-0.043151</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>0.047561</td>\n",
              "      <td>0.064020</td>\n",
              "      <td>0.024550</td>\n",
              "      <td>-0.017988</td>\n",
              "      <td>-0.020041</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.023563</td>\n",
              "      <td>-0.028223</td>\n",
              "      <td>-0.001192</td>\n",
              "      <td>0.021194</td>\n",
              "      <td>0.020908</td>\n",
              "      <td>0.007241</td>\n",
              "      <td>-0.014940</td>\n",
              "      <td>-0.029013</td>\n",
              "      <td>-0.012556</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1901</th>\n",
              "      <td>-0.214760</td>\n",
              "      <td>-0.137189</td>\n",
              "      <td>0.063628</td>\n",
              "      <td>0.216979</td>\n",
              "      <td>0.117362</td>\n",
              "      <td>-0.129971</td>\n",
              "      <td>-0.232769</td>\n",
              "      <td>-0.011466</td>\n",
              "      <td>0.174400</td>\n",
              "      <td>-0.002492</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.118097</td>\n",
              "      <td>-0.043695</td>\n",
              "      <td>0.006463</td>\n",
              "      <td>0.140691</td>\n",
              "      <td>0.270375</td>\n",
              "      <td>0.176057</td>\n",
              "      <td>-0.097308</td>\n",
              "      <td>-0.215748</td>\n",
              "      <td>-0.107130</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1902</th>\n",
              "      <td>0.003860</td>\n",
              "      <td>-0.026346</td>\n",
              "      <td>-0.026933</td>\n",
              "      <td>0.052409</td>\n",
              "      <td>0.137214</td>\n",
              "      <td>0.110519</td>\n",
              "      <td>0.018412</td>\n",
              "      <td>-0.066092</td>\n",
              "      <td>-0.096759</td>\n",
              "      <td>-0.095694</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.117109</td>\n",
              "      <td>0.051782</td>\n",
              "      <td>0.184129</td>\n",
              "      <td>0.167420</td>\n",
              "      <td>-0.024256</td>\n",
              "      <td>-0.158296</td>\n",
              "      <td>-0.118841</td>\n",
              "      <td>-0.028036</td>\n",
              "      <td>-0.038221</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1903</th>\n",
              "      <td>-0.162564</td>\n",
              "      <td>-0.255623</td>\n",
              "      <td>-0.029582</td>\n",
              "      <td>0.252611</td>\n",
              "      <td>0.207382</td>\n",
              "      <td>-0.117210</td>\n",
              "      <td>-0.253825</td>\n",
              "      <td>-0.149713</td>\n",
              "      <td>0.076848</td>\n",
              "      <td>0.185937</td>\n",
              "      <td>...</td>\n",
              "      <td>0.098490</td>\n",
              "      <td>0.375954</td>\n",
              "      <td>0.404241</td>\n",
              "      <td>0.189033</td>\n",
              "      <td>-0.067160</td>\n",
              "      <td>-0.242110</td>\n",
              "      <td>-0.321250</td>\n",
              "      <td>-0.366230</td>\n",
              "      <td>-0.392366</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1904</th>\n",
              "      <td>0.240359</td>\n",
              "      <td>0.231626</td>\n",
              "      <td>0.115252</td>\n",
              "      <td>-0.129998</td>\n",
              "      <td>-0.359195</td>\n",
              "      <td>-0.402580</td>\n",
              "      <td>-0.270478</td>\n",
              "      <td>-0.083559</td>\n",
              "      <td>0.061900</td>\n",
              "      <td>0.162378</td>\n",
              "      <td>...</td>\n",
              "      <td>0.102809</td>\n",
              "      <td>0.273305</td>\n",
              "      <td>0.374406</td>\n",
              "      <td>0.391519</td>\n",
              "      <td>0.281850</td>\n",
              "      <td>0.026974</td>\n",
              "      <td>-0.250276</td>\n",
              "      <td>-0.403238</td>\n",
              "      <td>-0.356164</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1905</th>\n",
              "      <td>0.490254</td>\n",
              "      <td>0.476353</td>\n",
              "      <td>0.061108</td>\n",
              "      <td>-0.383877</td>\n",
              "      <td>-0.558357</td>\n",
              "      <td>-0.351572</td>\n",
              "      <td>-0.133216</td>\n",
              "      <td>-0.033826</td>\n",
              "      <td>0.070212</td>\n",
              "      <td>0.213363</td>\n",
              "      <td>...</td>\n",
              "      <td>0.340287</td>\n",
              "      <td>0.257496</td>\n",
              "      <td>0.114228</td>\n",
              "      <td>0.016397</td>\n",
              "      <td>-0.103353</td>\n",
              "      <td>-0.257294</td>\n",
              "      <td>-0.313553</td>\n",
              "      <td>-0.204888</td>\n",
              "      <td>-0.005498</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1906 rows × 1202 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d895ba7c-0b0d-4907-ae58-f427fdea1aed')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d895ba7c-0b0d-4907-ae58-f427fdea1aed button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d895ba7c-0b0d-4907-ae58-f427fdea1aed');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1b7e677a-6e39-4463-8421-e4cdd6f8d109\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1b7e677a-6e39-4463-8421-e4cdd6f8d109')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1b7e677a-6e39-4463-8421-e4cdd6f8d109 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ],
      "source": [
        "dff2 = pd.concat([df12, df22], axis=1)\n",
        "dff2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vae_generated_chatter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "zqwfzwjsA0az",
        "outputId": "0c8d8d9e-3aae-4c67-a7a1-ff5d8642e933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0         1         2         3         4         5         6     \\\n",
              "0    0.503809  0.668056  0.185039  0.330226 -0.042543 -0.665721 -0.254632   \n",
              "1    1.053986  0.888298 -0.018872  0.058817 -0.553694 -1.113668 -0.793875   \n",
              "2   -0.770222 -1.247188 -0.457495 -0.375940 -0.056821  0.947299  1.250625   \n",
              "3    1.113125  0.660642 -0.000367 -0.040872 -0.594140 -1.026506 -0.745788   \n",
              "4    0.799404  0.088068  0.153921 -0.061041 -0.548115 -0.233107 -0.005679   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "948 -0.118947 -0.240552  0.336720 -0.309960 -0.495066 -0.276007 -0.798900   \n",
              "949 -0.448530 -0.199130 -0.540590  0.439760  0.671819  0.185687  0.774908   \n",
              "950 -0.293851 -0.473391 -1.100358 -0.265876 -0.620810 -0.628168  0.197359   \n",
              "951 -0.082612 -0.053637 -0.634518 -0.433149  0.026871 -0.341630  0.744611   \n",
              "952 -0.590680  0.144435 -0.095596  0.149732  0.810318 -0.046846  0.020341   \n",
              "\n",
              "         7         8         9     ...      1191      1192      1193  \\\n",
              "0   -0.430536 -0.531415  0.222507  ... -0.430825  0.234815  0.297519   \n",
              "1   -0.655930 -0.586679  0.719684  ... -0.552607  0.828006  1.092998   \n",
              "2    0.763853  0.770108  0.113301  ...  0.757292  0.097910 -0.649027   \n",
              "3   -0.624696 -0.450099  0.620807  ... -0.456832  0.750122  0.993291   \n",
              "4   -0.213023  0.406193  0.452800  ...  0.418285  0.558966  0.294640   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "948 -0.575996 -0.133440 -0.461488  ... -0.069356 -0.459398 -0.182916   \n",
              "949  0.258626 -0.498812 -0.228002  ... -0.718723 -0.403309 -0.468547   \n",
              "950 -0.094700 -0.154754  0.480223  ... -0.274375  0.433020 -0.226533   \n",
              "951  0.908290  0.459272  0.967162  ...  0.480696  0.806414  0.765780   \n",
              "952  0.031961 -0.712143 -0.583554  ... -0.637503 -0.831332 -0.308371   \n",
              "\n",
              "         1194      1195      1196      1197      1198      1199      1200  \n",
              "0    0.214371  0.871276  0.344799 -0.039269  0.215087 -0.358992 -0.464546  \n",
              "1    0.734656  1.333362  0.284504 -0.623581 -0.094510 -0.866790 -0.835775  \n",
              "2   -1.050137 -0.998205 -0.703596 -0.088969  0.373958  0.908917  1.361063  \n",
              "3    0.728667  1.228396  0.109250 -0.572858 -0.143555 -0.852503 -0.691564  \n",
              "4    0.660639  0.391865 -0.341706 -0.026764  0.022756 -0.378540  0.100205  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "948  0.242108 -0.221699 -0.013867 -0.071164 -0.676864 -0.403845 -0.401174  \n",
              "949 -1.070159 -0.109657 -0.300353 -0.212197  0.824482  0.218042  0.197422  \n",
              "950 -0.839944 -0.119775 -0.765365 -0.709446 -0.289550 -0.471074  0.023602  \n",
              "951 -0.546784 -0.253951 -0.072553 -0.793294  0.058317  0.059350  0.263585  \n",
              "952 -0.771263 -0.231355  0.189599 -0.082374  0.458161  0.398034 -0.235247  \n",
              "\n",
              "[953 rows x 1201 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-00576652-7936-4ff2-8df3-42e4bb46ba9d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>1191</th>\n",
              "      <th>1192</th>\n",
              "      <th>1193</th>\n",
              "      <th>1194</th>\n",
              "      <th>1195</th>\n",
              "      <th>1196</th>\n",
              "      <th>1197</th>\n",
              "      <th>1198</th>\n",
              "      <th>1199</th>\n",
              "      <th>1200</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.503809</td>\n",
              "      <td>0.668056</td>\n",
              "      <td>0.185039</td>\n",
              "      <td>0.330226</td>\n",
              "      <td>-0.042543</td>\n",
              "      <td>-0.665721</td>\n",
              "      <td>-0.254632</td>\n",
              "      <td>-0.430536</td>\n",
              "      <td>-0.531415</td>\n",
              "      <td>0.222507</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.430825</td>\n",
              "      <td>0.234815</td>\n",
              "      <td>0.297519</td>\n",
              "      <td>0.214371</td>\n",
              "      <td>0.871276</td>\n",
              "      <td>0.344799</td>\n",
              "      <td>-0.039269</td>\n",
              "      <td>0.215087</td>\n",
              "      <td>-0.358992</td>\n",
              "      <td>-0.464546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.053986</td>\n",
              "      <td>0.888298</td>\n",
              "      <td>-0.018872</td>\n",
              "      <td>0.058817</td>\n",
              "      <td>-0.553694</td>\n",
              "      <td>-1.113668</td>\n",
              "      <td>-0.793875</td>\n",
              "      <td>-0.655930</td>\n",
              "      <td>-0.586679</td>\n",
              "      <td>0.719684</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.552607</td>\n",
              "      <td>0.828006</td>\n",
              "      <td>1.092998</td>\n",
              "      <td>0.734656</td>\n",
              "      <td>1.333362</td>\n",
              "      <td>0.284504</td>\n",
              "      <td>-0.623581</td>\n",
              "      <td>-0.094510</td>\n",
              "      <td>-0.866790</td>\n",
              "      <td>-0.835775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.770222</td>\n",
              "      <td>-1.247188</td>\n",
              "      <td>-0.457495</td>\n",
              "      <td>-0.375940</td>\n",
              "      <td>-0.056821</td>\n",
              "      <td>0.947299</td>\n",
              "      <td>1.250625</td>\n",
              "      <td>0.763853</td>\n",
              "      <td>0.770108</td>\n",
              "      <td>0.113301</td>\n",
              "      <td>...</td>\n",
              "      <td>0.757292</td>\n",
              "      <td>0.097910</td>\n",
              "      <td>-0.649027</td>\n",
              "      <td>-1.050137</td>\n",
              "      <td>-0.998205</td>\n",
              "      <td>-0.703596</td>\n",
              "      <td>-0.088969</td>\n",
              "      <td>0.373958</td>\n",
              "      <td>0.908917</td>\n",
              "      <td>1.361063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.113125</td>\n",
              "      <td>0.660642</td>\n",
              "      <td>-0.000367</td>\n",
              "      <td>-0.040872</td>\n",
              "      <td>-0.594140</td>\n",
              "      <td>-1.026506</td>\n",
              "      <td>-0.745788</td>\n",
              "      <td>-0.624696</td>\n",
              "      <td>-0.450099</td>\n",
              "      <td>0.620807</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.456832</td>\n",
              "      <td>0.750122</td>\n",
              "      <td>0.993291</td>\n",
              "      <td>0.728667</td>\n",
              "      <td>1.228396</td>\n",
              "      <td>0.109250</td>\n",
              "      <td>-0.572858</td>\n",
              "      <td>-0.143555</td>\n",
              "      <td>-0.852503</td>\n",
              "      <td>-0.691564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.799404</td>\n",
              "      <td>0.088068</td>\n",
              "      <td>0.153921</td>\n",
              "      <td>-0.061041</td>\n",
              "      <td>-0.548115</td>\n",
              "      <td>-0.233107</td>\n",
              "      <td>-0.005679</td>\n",
              "      <td>-0.213023</td>\n",
              "      <td>0.406193</td>\n",
              "      <td>0.452800</td>\n",
              "      <td>...</td>\n",
              "      <td>0.418285</td>\n",
              "      <td>0.558966</td>\n",
              "      <td>0.294640</td>\n",
              "      <td>0.660639</td>\n",
              "      <td>0.391865</td>\n",
              "      <td>-0.341706</td>\n",
              "      <td>-0.026764</td>\n",
              "      <td>0.022756</td>\n",
              "      <td>-0.378540</td>\n",
              "      <td>0.100205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>948</th>\n",
              "      <td>-0.118947</td>\n",
              "      <td>-0.240552</td>\n",
              "      <td>0.336720</td>\n",
              "      <td>-0.309960</td>\n",
              "      <td>-0.495066</td>\n",
              "      <td>-0.276007</td>\n",
              "      <td>-0.798900</td>\n",
              "      <td>-0.575996</td>\n",
              "      <td>-0.133440</td>\n",
              "      <td>-0.461488</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.069356</td>\n",
              "      <td>-0.459398</td>\n",
              "      <td>-0.182916</td>\n",
              "      <td>0.242108</td>\n",
              "      <td>-0.221699</td>\n",
              "      <td>-0.013867</td>\n",
              "      <td>-0.071164</td>\n",
              "      <td>-0.676864</td>\n",
              "      <td>-0.403845</td>\n",
              "      <td>-0.401174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>949</th>\n",
              "      <td>-0.448530</td>\n",
              "      <td>-0.199130</td>\n",
              "      <td>-0.540590</td>\n",
              "      <td>0.439760</td>\n",
              "      <td>0.671819</td>\n",
              "      <td>0.185687</td>\n",
              "      <td>0.774908</td>\n",
              "      <td>0.258626</td>\n",
              "      <td>-0.498812</td>\n",
              "      <td>-0.228002</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.718723</td>\n",
              "      <td>-0.403309</td>\n",
              "      <td>-0.468547</td>\n",
              "      <td>-1.070159</td>\n",
              "      <td>-0.109657</td>\n",
              "      <td>-0.300353</td>\n",
              "      <td>-0.212197</td>\n",
              "      <td>0.824482</td>\n",
              "      <td>0.218042</td>\n",
              "      <td>0.197422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>950</th>\n",
              "      <td>-0.293851</td>\n",
              "      <td>-0.473391</td>\n",
              "      <td>-1.100358</td>\n",
              "      <td>-0.265876</td>\n",
              "      <td>-0.620810</td>\n",
              "      <td>-0.628168</td>\n",
              "      <td>0.197359</td>\n",
              "      <td>-0.094700</td>\n",
              "      <td>-0.154754</td>\n",
              "      <td>0.480223</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.274375</td>\n",
              "      <td>0.433020</td>\n",
              "      <td>-0.226533</td>\n",
              "      <td>-0.839944</td>\n",
              "      <td>-0.119775</td>\n",
              "      <td>-0.765365</td>\n",
              "      <td>-0.709446</td>\n",
              "      <td>-0.289550</td>\n",
              "      <td>-0.471074</td>\n",
              "      <td>0.023602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>951</th>\n",
              "      <td>-0.082612</td>\n",
              "      <td>-0.053637</td>\n",
              "      <td>-0.634518</td>\n",
              "      <td>-0.433149</td>\n",
              "      <td>0.026871</td>\n",
              "      <td>-0.341630</td>\n",
              "      <td>0.744611</td>\n",
              "      <td>0.908290</td>\n",
              "      <td>0.459272</td>\n",
              "      <td>0.967162</td>\n",
              "      <td>...</td>\n",
              "      <td>0.480696</td>\n",
              "      <td>0.806414</td>\n",
              "      <td>0.765780</td>\n",
              "      <td>-0.546784</td>\n",
              "      <td>-0.253951</td>\n",
              "      <td>-0.072553</td>\n",
              "      <td>-0.793294</td>\n",
              "      <td>0.058317</td>\n",
              "      <td>0.059350</td>\n",
              "      <td>0.263585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>952</th>\n",
              "      <td>-0.590680</td>\n",
              "      <td>0.144435</td>\n",
              "      <td>-0.095596</td>\n",
              "      <td>0.149732</td>\n",
              "      <td>0.810318</td>\n",
              "      <td>-0.046846</td>\n",
              "      <td>0.020341</td>\n",
              "      <td>0.031961</td>\n",
              "      <td>-0.712143</td>\n",
              "      <td>-0.583554</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.637503</td>\n",
              "      <td>-0.831332</td>\n",
              "      <td>-0.308371</td>\n",
              "      <td>-0.771263</td>\n",
              "      <td>-0.231355</td>\n",
              "      <td>0.189599</td>\n",
              "      <td>-0.082374</td>\n",
              "      <td>0.458161</td>\n",
              "      <td>0.398034</td>\n",
              "      <td>-0.235247</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>953 rows × 1201 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00576652-7936-4ff2-8df3-42e4bb46ba9d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-00576652-7936-4ff2-8df3-42e4bb46ba9d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-00576652-7936-4ff2-8df3-42e4bb46ba9d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-045d1ea5-8f5a-4220-aa8a-ad6132b21892\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-045d1ea5-8f5a-4220-aa8a-ad6132b21892')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-045d1ea5-8f5a-4220-aa8a-ad6132b21892 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8EOeurNkBPL"
      },
      "outputs": [],
      "source": [
        "Xchatter = dff2[dff2[\"Labels\"]==2].iloc[:,0:1201]\n",
        "Xstable = dff2[dff2[\"Labels\"]==1].iloc[:,0:1201]\n",
        "\n",
        "stable_svm_all = pd.concat([Xstable,vae_generated_stable], axis=0)\n",
        "stable_svm_all = stable_svm_all.reset_index(drop=True)\n",
        "\n",
        "chatter_svm_ch3 = vae_generated_chatter[8:160] #generated 392\n",
        "\n",
        "svm3_stable_train = stable_svm_all[0:800] #stable train 800\n",
        "svm3_stable_test = stable_svm_all[753:953] #stable test 200\n",
        "\n",
        "svm3_chatter_train = Xchatter[0:8] #original chatter train 8\n",
        "\n",
        "svm3_chatter_test = Xchatter[753:953] #chatter test 200\n",
        "\n",
        "stable_svm3_train_label = np.full((800, 1), 1) # stable train labeling\n",
        "chatter_svm3_train_o_label = np.full((160, 1), 2) #chatter train labeling\n",
        "\n",
        "stable_svm3_test_label = np.full((200, 1), 1) # stable test labeling\n",
        "chatter_svm3_test_o_label = np.full((200, 1), 2) #chatter test labeling\n",
        "\n",
        "svm3_train = pd.concat([svm3_stable_train, svm3_chatter_train,chatter_svm_ch3], axis=0) #concatting all train datas\n",
        "svm3_train = svm3_train.reset_index(drop=True)\n",
        "\n",
        "svm3_test = pd.concat([svm3_stable_test, svm3_chatter_test], axis=0) #concatting all test datas\n",
        "svm3_test = svm3_test.reset_index(drop=True)\n",
        "\n",
        "svm3_train_label = np.concatenate((stable_svm3_train_label, chatter_svm3_train_o_label), axis=0)\n",
        "svm3_test_label = np.concatenate((stable_svm3_test_label, chatter_svm3_test_o_label), axis=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler3 = StandardScaler()\n",
        "X_train_svm3 = scaler3.fit_transform(svm3_train)\n",
        "X_test_svm3 = scaler3.transform(svm3_test)"
      ],
      "metadata": {
        "id": "0-fXZx5G_e1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_svm3.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWyU4I4JAoDW",
        "outputId": "d6e86c3a-5ac3-4f5b-f6a2-9f22def9d995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(960, 1201)"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "clf = svm.SVC(kernel = \"poly\",C=1,degree=2,gamma = 'auto')\n",
        "\n",
        "# Fit the classifier on the training data\n",
        "clf.fit(X_train_svm3, svm3_train_label)\n",
        "\n",
        "# Predict the labels for the test data\n",
        "predictions = clf.predict(X_test_svm3)\n",
        "\n",
        "# Calculate the accuracy of the SVM classifier\n",
        "accuracy = accuracy_score(svm3_test_label, predictions)\n",
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFuzkg9r_p09",
        "outputId": "b791deb5-fd72-4601-cde2-28241252c474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.83"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XUideb9G_py2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eIwNxlXD_pw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yWw1uj-d_pvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WEP3Bz43_psL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A_ZDTrWI_ppt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Lsiifv5pK_u"
      },
      "outputs": [],
      "source": [
        " X_train_wl = np.array(X_train_wl)\n",
        " X_train_wl=X_train_wl.reshape((X_train_wl.shape[0], X_train_wl.shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1BgYyrXrukz",
        "outputId": "3e2542af-d802-4bd4-db19-f0d7e3d147aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(382, 1201, 1)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test_wl = np.array(X_test_wl)\n",
        "X_test_wl=X_test_wl.reshape((X_test_wl.shape[0], X_test_wl.shape[1], 1))\n",
        "X_test_wl.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PqGJIidfLE6"
      },
      "outputs": [],
      "source": [
        "encoded_test = np.array(encoder.predict(np.array(X_test_wl)))\n",
        "vae_test = vae.predict(np.array(X_test_wl))\n",
        "vae_test"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}